# 딥 러닝을 이용한 자연어 처리 입문     



워드 임베딩(Word Embedding)은 단어를 벡터로 표현하는 방법으로, 단어를 밀집 표현으로 변환한다     

<br/>

## 1. 희소 표현 (Sparse Representation)     

__벡터 또는 행렬(matrix)의 값이 대부분이 0으로 표현되는 방법__     

원-핫 벡터는 희소 벡터이다     

원-핫 벡터는 단어의 의미를 담지 못한다는 단점이 있다      

​     

희소 벡터의 문제점     

-> __단어의 개수가 늘어나면 벡터의 차원이 한없이 커진다는 점__

희소 표현의 일종인 DTM과 같은 경우에도 특정 문서에 여러 단어가 다수 등장하였으나, 다른 많은 문서에서는 해당 특정 문서에 등장했던 단어들이 전부 등장하지 않는다면 역시나 행렬의 많은 값이 0이 되면서 공간적 낭비 발생     

<br/>

<br/>

## 2. 밀집 표현 (Dense Representation)    

__사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춘다__     

__0과 1만 가진 값이 아니라 실수값을 가진다__     

벡터의 차원이 조밀해졌다고 하여 밀집 벡터(dense vector)

<br/>

<br/>

## 3. 워드 임베딩 (Word Embedding)     

__단어를 밀집 벡터(dense vector)의 형태로 표현하는 방법__     

임베딩 과정을 통해 나온 결과를 __임베딩 벡터 (embedding vector)__ 라고 한다     



워드 임베딩 방법론으로는 __LSA, Word2Vec, FastText, Glove__ 등이 있다     

케라스에서 제공하는 __Embedding()__ 는 앞서 언급한 방법들을 사용하지는 않지만, 단어를 랜덤한 값을 가지는 밀집 벡터로 변환한 뒤에, 인공 신경망의 가중치를 학습하는 것과 같은 방식으로 단어 벡터를 학습하는 방법을 사용      

