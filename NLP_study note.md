# ë”¥ ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸

- [ìì—°ì–´ ì²˜ë¦¬ë€?](#ìì—°ì–´-ì²˜ë¦¬ë€)
- [í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (Text preprocessing)](#í…ìŠ¤íŠ¸-ì „ì²˜ë¦¬-text-preprocessing)
  * [í† í°í™” Tokenization](#í† í°í™”-tokenization)
    + [1. ë‹¨ì–´ í† í°í™” Word Tokenization](#1-ë‹¨ì–´-í† í°í™”-word-tokenization)
    + [2. í† í°í™” ì¤‘ ìƒê¸°ëŠ” ì„ íƒì˜ ìˆœê°„](#2-í† í°í™”-ì¤‘-ìƒê¸°ëŠ”-ì„ íƒì˜-ìˆœê°„)
    + [3. í† í°í™”ì—ì„œ ê³ ë ¤í•´ì•¼í•  ì‚¬í•­](#3-í† í°í™”ì—ì„œ-ê³ ë ¤í•´ì•¼í• -ì‚¬í•­)
    + [4. ë¬¸ì¥ í† í°í™” Sentence Tokenization](#4-ë¬¸ì¥-í† í°í™”-sentence-tokenization)
    + [5. ì´ì§„ ë¶„ë¥˜ê¸° Binary Classifier](#5-ì´ì§„-ë¶„ë¥˜ê¸°-binary-classifier)
    + [6. í•œêµ­ì–´ì—ì„œì˜ í† í°í™”ì˜ ì–´ë ¤ì›€.](#6-í•œêµ­ì–´ì—ì„œì˜-í† í°í™”ì˜-ì–´ë ¤ì›€)
    + [7. í’ˆì‚¬ íƒœê¹…(Part-of-speech tagging)](#7-í’ˆì‚¬-íƒœê¹…part-of-speech-tagging)
    + [8. NLTKì™€ KoNLPyë¥¼ ì´ìš©í•œ ì˜ì–´, í•œêµ­ì–´ í† í°í™” ì‹¤ìŠµ](#8-nltkì™€-konlpyë¥¼-ì´ìš©í•œ-ì˜ì–´-í•œêµ­ì–´-í† í°í™”-ì‹¤ìŠµ)
  * [ì •ì œ(Cleaning) and  ì •ê·œí™”(Normalization)](#ì •ì œcleaning-and--ì •ê·œí™”normalization)
    + [1. ê·œì¹™ì— ê¸°ë°˜í•œ í‘œê¸°ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì˜ í†µí•©](#1-ê·œì¹™ì—-ê¸°ë°˜í•œ-í‘œê¸°ê°€-ë‹¤ë¥¸-ë‹¨ì–´ë“¤ì˜-í†µí•©)
    + [2. ëŒ€, ì†Œë¬¸ì í†µí•©](#2-ëŒ€-ì†Œë¬¸ì-í†µí•©)
    + [3. ë¶ˆí•„ìš”í•œ ë‹¨ì–´ì˜ ì œê±° (Removing Unnecessary Words)](#3-ë¶ˆí•„ìš”í•œ-ë‹¨ì–´ì˜-ì œê±°-removing-unnecessary-words)
    + [4. ì •ê·œ í‘œí˜„ì‹(Regular Expression)](#4-ì •ê·œ-í‘œí˜„ì‹regular-expression)
  * [ì–´ê°„ ì¶”ì¶œ(Stemming) and í‘œì œì–´ ì¶”ì¶œ(Lemmatization)](#ì–´ê°„-ì¶”ì¶œstemming-and-í‘œì œì–´-ì¶”ì¶œlemmatization)
    + [1. í‘œì œì–´ ì¶”ì¶œ(Lemmatization)](#1-í‘œì œì–´-ì¶”ì¶œlemmatization)
    + [2. ì–´ê°„ ì¶”ì¶œ(Stemming)](#2-ì–´ê°„-ì¶”ì¶œstemming)
    + [3. í•œêµ­ì–´ì—ì„œì˜ ì–´ê°„ ì¶”ì¶œ](#3-í•œêµ­ì–´ì—ì„œì˜-ì–´ê°„-ì¶”ì¶œ)
  * [ë¶ˆìš©ì–´ (Stopword)](#ë¶ˆìš©ì–´-stopword)
    + [1. NLTKì—ì„œ ë¶ˆìš©ì–´ í™•ì¸í•˜ê¸°](#1-nltkì—ì„œ-ë¶ˆìš©ì–´-í™•ì¸í•˜ê¸°)
    + [2. NLTKë¥¼ í†µí•´ì„œ ë¶ˆìš©ì–´ ì œê±°í•˜ê¸°](#2-nltkë¥¼-í†µí•´ì„œ-ë¶ˆìš©ì–´-ì œê±°í•˜ê¸°)
    + [3. í•œêµ­ì–´ì—ì„œ ë¶ˆìš©ì–´ ì œê±°í•˜ê¸°](#3-í•œêµ­ì–´ì—ì„œ-ë¶ˆìš©ì–´-ì œê±°í•˜ê¸°)
  * [ì •ê·œ í‘œí˜„ì‹(Regular Expression)](#ì •ê·œ-í‘œí˜„ì‹regular-expression)
    + [1. ì •ê·œ í‘œí˜„ì‹ ë¬¸ë²•ê³¼ ëª¨ë“ˆ í•¨ìˆ˜](#1-ì •ê·œ-í‘œí˜„ì‹-ë¬¸ë²•ê³¼-ëª¨ë“ˆ-í•¨ìˆ˜)
    + [2. ì •ê·œ í‘œí˜„ì‹ ì‹¤ìŠµ](#2-ì •ê·œ-í‘œí˜„ì‹-ì‹¤ìŠµ)
    + [3. ì •ê·œ í‘œí˜„ì‹ ëª¨ë“ˆ í•¨ìˆ˜ ì˜ˆì œ](#3-ì •ê·œ-í‘œí˜„ì‹-ëª¨ë“ˆ-í•¨ìˆ˜-ì˜ˆì œ)
    + [4. ì •ê·œ í‘œí˜„ì‹ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì˜ˆì œ](#4-ì •ê·œ-í‘œí˜„ì‹-í…ìŠ¤íŠ¸-ì „ì²˜ë¦¬-ì˜ˆì œ)
    + [5. ì •ê·œ í‘œí˜„ì‹ì„ ì´ìš©í•œ í† í°í™”](#5-ì •ê·œ-í‘œí˜„ì‹ì„-ì´ìš©í•œ-í† í°í™”)
  * [ì •ìˆ˜ ì¸ì½”ë”© (Integer Encoding)](#ì •ìˆ˜-ì¸ì½”ë”©-integer-encoding)
    + [1. ì •ìˆ˜ ì¸ì½”ë”© (Integer Encoding)](#1-ì •ìˆ˜-ì¸ì½”ë”©-integer-encoding)
    + [2. ì¼€ë¼ìŠ¤ (Keras)ì˜ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬](#2-ì¼€ë¼ìŠ¤-kerasì˜-í…ìŠ¤íŠ¸-ì „ì²˜ë¦¬)
  * [íŒ¨ë”© (Padding)](#íŒ¨ë”©-padding)
    + [1. Numpy ë¡œ íŒ¨ë”©í•˜ê¸°](#1-numpy-ë¡œ-íŒ¨ë”©í•˜ê¸°)
    + [2. ì¼€ë¼ìŠ¤ ì „ì²˜ë¦¬ ë„êµ¬ë¡œ íŒ¨ë”©](#2-ì¼€ë¼ìŠ¤-ì „ì²˜ë¦¬-ë„êµ¬ë¡œ-íŒ¨ë”©)
  * [ì› - í•« ì¸ì½”ë”© (One-Hot Encoding)](#ì›--í•«-ì¸ì½”ë”©-onehot-encoding)
    + [1. ì› - í•« ì¸ì½”ë”© (One - Hot Encoding) ì´ë€?](#1-ì›--í•«-ì¸ì½”ë”©-one--hot-encoding-ì´ë€)
    + [2. ì¼€ë¼ìŠ¤ (Keras)ë¥¼ ì´ìš©í•œ ì›-í•« ì¸ì½”ë”© (One-Hot-Encoding)](#2-ì¼€ë¼ìŠ¤-kerasë¥¼-ì´ìš©í•œ-ì›í•«-ì¸ì½”ë”©-onehotencoding)
    + [3. ì› - í•« ì¸ì½”ë”©(One-Hot Encoding)ì˜ í•œê³„](#3-ì›--í•«-ì¸ì½”ë”©onehot-encodingì˜-í•œê³„)
  * [ë°ì´í„°ì˜ ë¶„ë¦¬ (Splitting Data)](#ë°ì´í„°ì˜-ë¶„ë¦¬-splitting-data)
    + [1. ì§€ë„ í•™ìŠµ (Supervised Learning)](#1-ì§€ë„-í•™ìŠµ-supervised-learning)
    + [2. Xì™€ y ë¶„ë¦¬í•˜ê¸°](#2-xì™€-y-ë¶„ë¦¬í•˜ê¸°)
    + [3. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬í•˜ê¸°](#3-í…ŒìŠ¤íŠ¸-ë°ì´í„°-ë¶„ë¦¬í•˜ê¸°)
  * [í•œêµ­ì–´ ì „ì²˜ë¦¬ íŒ¨í‚¤ì§€ (Text Preprocessing Tools for Korean Text)](#í•œêµ­ì–´-ì „ì²˜ë¦¬-íŒ¨í‚¤ì§€-text-preprocessing-tools-for-korean-text)
    + [1. PyKoSpacing](#1-pykospacing)
    + [2. Py - Hanspell](#2-py--hanspell)
    + [3. SOYNLP ë¥¼ ì´ìš©í•œ ë‹¨ì–´ í† í°í™”](#3-soynlp-ë¥¼-ì´ìš©í•œ-ë‹¨ì–´-í† í°í™”)
    + [4. SOYNLP ë¥¼ ì´ìš©í•œ ë°˜ë³µë˜ëŠ” ë¬¸ì ì •ì œ](#4-soynlp-ë¥¼-ì´ìš©í•œ-ë°˜ë³µë˜ëŠ”-ë¬¸ì-ì •ì œ)
    + [5. Customized KoNLPy](#5-customized-konlpy)
- [ì–¸ì–´ ëª¨ë¸ (Language Model)](#ì–¸ì–´-ëª¨ë¸-language-model)
  * [ì–¸ì–´ ëª¨ë¸ (Language Model) ì´ë€?](#ì–¸ì–´-ëª¨ë¸-language-model-ì´ë€)
    + [1. ì–¸ì–´ ëª¨ë¸ (Language Model)](#1-ì–¸ì–´-ëª¨ë¸-language-model)
    + [2. ë‹¨ì–´ ì‹œí€€ìŠ¤ì˜ í™•ë¥  í• ë‹¹](#2-ë‹¨ì–´-ì‹œí€€ìŠ¤ì˜-í™•ë¥ -í• ë‹¹)
    + [3. ì£¼ì–´ì§„ ì´ì „ ë‹¨ì–´ë“¤ë¡œë¶€í„° ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡í•˜ê¸°](#3-ì£¼ì–´ì§„-ì´ì „-ë‹¨ì–´ë“¤ë¡œë¶€í„°-ë‹¤ìŒ-ë‹¨ì–´-ì˜ˆì¸¡í•˜ê¸°)
    + [4. ì–¸ì–´ ëª¨ë¸ì˜ ê°„ë‹¨í•œ ì§ê´€](#4-ì–¸ì–´-ëª¨ë¸ì˜-ê°„ë‹¨í•œ-ì§ê´€)
    + [5. ê²€ìƒ‰ ì—”ì§„ì—ì„œì˜ ì–¸ì–´ ëª¨ë¸ì˜ ì˜ˆ](#5-ê²€ìƒ‰-ì—”ì§„ì—ì„œì˜-ì–¸ì–´-ëª¨ë¸ì˜-ì˜ˆ)
  * [í†µê³„ì  ì–¸ì–´ ëª¨ë¸ (Statistical Language Model, SLM)](#í†µê³„ì -ì–¸ì–´-ëª¨ë¸-statistical-language-model-slm)
    + [1. ì¡°ê±´ë¶€ í™•ë¥ ](#1-ì¡°ê±´ë¶€-í™•ë¥ )
    + [2. ë¬¸ì¥ì— ëŒ€í•œ í™•ë¥ ](#2-ë¬¸ì¥ì— ëŒ€í•œ í™•ë¥ )
    + [3. ì¹´ìš´íŠ¸ ê¸°ë°˜ì˜ ì ‘ê·¼](#3-ì¹´ìš´íŠ¸-ê¸°ë°˜ì˜ ì ‘ê·¼)
    + [4. ì¹´ìš´íŠ¸ ê¸°ë°˜ ì ‘ê·¼ì˜ í•œê³„ - í¬ì†Œ ë¬¸ì œ (Sparsity Problem)](#4-ì¹´ìš´íŠ¸-ê¸°ë°˜-ì ‘ê·¼ì˜-í•œê³„--í¬ì†Œ-ë¬¸ì œ-sparsity-problem)
  * [N-gram ì–¸ì–´ ëª¨ë¸ (N-gram Language Model)](#ngram-ì–¸ì–´-ëª¨ë¸-ngram-language-model)
    + [1. Corpus ì—ì„œ ì¹´ìš´íŠ¸ í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ì˜ ê°ì†Œ](#1-corpus-ì—ì„œ-ì¹´ìš´íŠ¸-í•˜ì§€-ëª»í•˜ëŠ”-ê²½ìš°ì˜-ê°ì†Œ)
    + [2. N-gram](#2-ngram)
    + [3. N-gram Language Modelì˜ í•œê³„](#3-ngram-language-modelã…¢-í•œê³„)
    + [4. ì ìš© ë¶„ì•¼(Domain)ì— ë§ëŠ” ì½”í¼ìŠ¤ì˜ ìˆ˜ì§‘](#4-ì ìš©-ë¶„ì•¼domainì—-ë§ëŠ”-ì½”í¼ìŠ¤ì˜-ìˆ˜ì§‘)
    + [5. ì¸ê³µ ì‹ ê²½ë§ì„ ì´ìš©í•œ ì–¸ì–´ ëª¨ë¸(Neural Network Based Language Model)](#5-ì¸ê³µ-ì‹ ê²½ë§ì„-ì´ìš©í•œ-ì–¸ì–´-ëª¨ë¸neural-network-based-language-model)
  * [í•œêµ­ì–´ì—ì„œì˜ ì–¸ì–´ ëª¨ë¸ (Language Model for Korean Sentences)](#í•œêµ­ì–´ì—ì„œì˜-ì–¸ì–´-ëª¨ë¸-language-model-for-korean-sentences)
    + [1. í•œêµ­ì–´ëŠ” ì–´ìˆœì´ ì¤‘ìš”í•˜ì§€ ì•Šë‹¤.](#1-í•œêµ­ì–´ëŠ”-ì–´ìˆœì´-ì¤‘ìš”í•˜ì§€-ì•Šë‹¤)
    + [2. í•œêµ­ì–´ëŠ” êµì°©ì–´ì´ë‹¤.](#2-í•œêµ­ì–´ëŠ”-êµì°©ì–´ì´ë‹¤)
    + [í•œêµ­ì–´ëŠ” ë„ì–´ì“°ê¸°ê°€ ì œëŒ€ë¡œ ì§€ì¼œì§€ì§€ ì•ŠëŠ”ë‹¤.](#í•œêµ­ì–´ëŠ”-ë„ì–´ì“°ê¸°ê°€-ì œëŒ€ë¡œ-ì§€ì¼œì§€ì§€-ì•ŠëŠ”ë‹¤)
  * [í„í”Œë ‰ì„œí‹° (Perplexity)](#í„í”Œë ‰ì„œí‹°-perplexity)
    + [1. ì–¸ì–´ ëª¨ë¸ì˜ í‰ê°€ ë°©ë²• (Evaluation metric) : Perplexity ì¤„ì—¬ì„œ PPL](#1-ì–¸ì–´-ëª¨ë¸ì˜-í‰ê°€-ë°©ë²•-evaluation-metric--perplexity-ì¤„ì—¬ì„œ-ppl)
    + [2. ë¶„ê¸° ê³„ìˆ˜(Branching factor)](#2-ë¶„ê¸°-ê³„ìˆ˜branching-factor)
    + [3. ê¸°ì¡´ ì–¸ì–´ ëª¨ë¸ vs ì¸ê³µ ì‹ ê²½ë§ì„ ì´ìš©í•œ ì–¸ì–´ ëª¨ë¸](#3-ê¸°ì¡´-ì–¸ì–´-ëª¨ë¸-vs-ì¸ê³µ-ì‹ ê²½ë§ì„-ì´ìš©í•œ-ì–¸ì–´-ëª¨ë¸)
- [ì¹´ìš´íŠ¸ ê¸°ë°˜ì˜ ë‹¨ì–´ í‘œí˜„ (Count based word Representation)](#ì¹´ìš´íŠ¸-ê¸°ë°˜ì˜-ë‹¨ì–´-í‘œí˜„-count-based-word-representation)
  * [ë‹¤ì–‘í•œ ë‹¨ì–´ì˜ í‘œí˜„ ë°©ë²•](#ë‹¤ì–‘í•œ-ë‹¨ì–´ì˜-í‘œí˜„-ë°©ë²•)
    + [1. ë‹¨ì–´ì˜ í‘œí˜„ ë°©ë²•](#1-ë‹¨ì–´ì˜-í‘œí˜„-ë°©ë²•)
    + [2. ë‹¨ì–´ í‘œí˜„ì˜ ì¹´í…Œê³ ë¦¬í™”](#2-ë‹¨ì–´-í‘œí˜„ì˜-ì¹´í…Œê³ ë¦¬í™”)
  * [Bag of Words(BoW)](#bag-of-wordsbow)
    + [2. Bag of Words ì˜ ë‹¤ë¥¸ ì˜ˆì œë“¤](#2-bag-of-words-ì˜-ë‹¤ë¥¸-ì˜ˆì œë“¤)
    + [3. CounVectorizer í´ë˜ìŠ¤ë¡œ BoW ë§Œë“¤ê¸°](#3-counvectorizer-í´ë˜ìŠ¤ë¡œ-bow-ë§Œë“¤ê¸°)
    + [4. ë¶ˆìš©ì–´ë¥¼ ì œê±°í•œ BoW ë§Œë“¤ê¸°](#4-ë¶ˆìš©ì–´ë¥¼-ì œê±°í•œ-bow-ë§Œë“¤ê¸°)
  * [ë¬¸ì„œ ë‹¨ì–´ í–‰ë ¬ (Document - Term Matrix, DTM)](#ë¬¸ì„œ-ë‹¨ì–´-í–‰ë ¬-document--term-matrix-dtm)
    + [1. ë¬¸ì„œ ë‹¨ì–´ í–‰ë ¬ (Document - Term Matrix, DTM) ì˜ í‘œê¸°ë²•](#1-ë¬¸ì„œ-ë‹¨ì–´-í–‰ë ¬-document--term-matrix-dtm-ì˜-í‘œê¸°ë²•)
    + [2. ë¬¸ì„œ ë‹¨ì–´ í–‰ë ¬ì˜ í•œê³„](#2-ë¬¸ì„œ-ë‹¨ì–´-í–‰ë ¬ì˜-í•œê³„)
  * [TF-IDF (Term Frequency-Inverse Document Freqency)](#tfidf-term-frequencyinverse-document-freqency)
    + [1. TF - IDF (ë‹¨ì–´ ë¹ˆë„- ì—­ ë¬¸ì„œ ë¹ˆë„, Term Freqency - Inverse Document Frequency)](#1-tf--idf-ë‹¨ì–´-ë¹ˆë„-ì—­-ë¬¸ì„œ-ë¹ˆë„-term-freqency--inverse-document-frequency)
    + [2. íŒŒì´ì¬ìœ¼ë¡œ TF-IDF ì§ì ‘ êµ¬í˜„í•˜ê¸°](#2-íŒŒì´ì¬ìœ¼ë¡œ-tfidf-ì§ì ‘-êµ¬í˜„í•˜ê¸°)
    + [3. ì‚¬ì´í‚·ëŸ°ì„ ì´ìš©í•œ DTMê³¼ TF-IDF ì‹¤ìŠµ](#3-ì‚¬ì´í‚·ëŸ°ì„-ì´ìš©í•œ-dtmê³¼-tfidf-ì‹¤ìŠµ)
- [ë¬¸ì„œ ìœ ì‚¬ë„ (Document Similarity)](#ë¬¸ì„œ-ìœ ì‚¬ë„-document-similarity)
  * [ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (Cosine Similarity)](#ì½”ì‚¬ì¸-ìœ ì‚¬ë„-cosine-similarity)
    + [1. ì½”ì‚¬ì¸ ìœ ì‚¬ë„](#1-ì½”ì‚¬ì¸-ìœ ì‚¬ë„)
    + [2. ìœ ì‚¬ë„ë¥¼ ì´ìš©í•œ ì¶”ì²œ ì‹œìŠ¤í…œ êµ¬í˜„í•˜ê¸°](#2-ìœ ì‚¬ë„ë¥¼-ì´ìš©í•œ-ì¶”ì²œ-ì‹œìŠ¤í…œ-êµ¬í˜„í•˜ê¸°)
  * [ì—¬ëŸ¬ê°€ì§€ ìœ ì‚¬ë„ ê¸°ë²•](#ì—¬ëŸ¬ê°€ì§€-ìœ ì‚¬ë„-ê¸°ë²•)
    + [1. ìœ í´ë¦¬ë“œ ê±°ë¦¬ (Euclidean distance)](#1-ìœ í´ë¦¬ë“œ-ê±°ë¦¬-euclidean-distance)
    + [2. ìì¹´ë“œ ìœ ì‚¬ë„ (Jaccard similarity)](#2-ìì¹´ë“œ-ìœ ì‚¬ë„-jaccard-similarity)
- [í† í”½ ëª¨ë¸ë§ (Topic Modeling)](#í† í”½-ëª¨ë¸ë§-topic-modeling)
  * [ì ì¬ ì˜ë¯¸ ë¶„ì„ (Latent Semantic Analysis, LSA)](#ì ì¬-ì˜ë¯¸-ë¶„ì„-latent-semantic-analysis-lsa)
    + [1. íŠ¹ì´ê°’ ë¶„í•´ (Singular Value Decomposition, SVD)](#1-íŠ¹ì´ê°’-ë¶„í•´-singular-value-decomposition-svd)
    + [2. ì ˆë‹¨ëœ SVD (Truncated SVD)](#2-ì ˆë‹¨ëœ-svd-truncated-svd)
    + [3. ì ì¬ ì˜ë¯¸ ë¶„ì„ (Latent Semantic Analysis , LSA)](#3-ì ì¬-ì˜ë¯¸-ë¶„ì„-latent-semantic-analysis--lsa)
    + [4. ì‹¤ìŠµì„ í†µí•œ ì´í•´](#4-ì‹¤ìŠµì„-í†µí•œ-ì´í•´)
    + [5.  LSAì˜ ì¥ë‹¨ì  (Pros and Cons of LSA)](#5-lsaì˜-ì¥ë‹¨ì -pros-and-cons-of-lsa)
  * [ì ì¬ ë””ë¦¬í´ë ˆ í• ë‹¹ (Latent Dirichlet Allocation, LDA)](#-ì ì¬-ë””ë¦¬í´ë˜-í• ë‹¹-latent-dirichlet-allocation-lda)
    + [1. ì ì¬ ë””ë¦¬í´ë ˆ í• ë‹¹ (Latent Dirichlet Allocation, LDA) ê°œìš”](#1-ì ì¬-ë””ë¦¬í´ë ˆ-í• ë‹¹-latent-dirichlet-allocation-lda-ê°œìš”)
    + [2. LDAì˜ ê°€ì •](#2-ldaì˜-ê°€ì •)
    + [3. LDA ìˆ˜í–‰í•˜ê¸°](#3-lda-ìˆ˜í–‰í•˜ê¸°)
    + [4. ì ì¬ ë””ë¦¬í´ë ˆ í• ë‹¹ê³¼ ì ì¬ ì˜ë¯¸ ë¶„ì„ì˜ ì°¨ì´](#4-ì ì¬-ë””ë¦¬í´ë ˆ-í• ë‹¹ê³¼-ì ì¬-ì˜ë¯¸-ë¶„ì„ì˜-ì°¨ì´)



# ìì—°ì–´ ì²˜ë¦¬ë€?

ìì—°ì–´(natural language)ë€ ìš°ë¦¬ê°€ ì¼ìƒ ìƒí™œì—ì„œ ì‚¬ìš©í•˜ëŠ” ì–¸ì–´ .

ìì—°ì–´ ì²˜ë¦¬(natural language processing)ë€ ì´ëŸ¬í•œ ìì—°ì–´ì˜ ì˜ë¯¸ë¥¼ ë¶„ì„í•˜ì—¬ ì»´í“¨í„°ê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì¼.

ìŒì„± ì¸ì‹, ë‚´ìš© ìš”ì•½, ë²ˆì—­, ì‚¬ìš©ìì˜ ê°ì„± ë¶„ì„, í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‘ì—…(ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜, ë‰´ìŠ¤ ê¸°ì‚¬ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜), ì§ˆì˜ ì‘ë‹µ ì‹œìŠ¤í…œ, ì±—ë´‡ê³¼ ê°™ì€ ê³³ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë¶„ì•¼

---

# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (Text preprocessing)

ìš©ë„ì— ë§ê²Œ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ì „ì— ì²˜ë¦¬í•˜ëŠ” ì‘ì—….

ìì—°ì–´ ì²˜ë¦¬ì—ì„œ í¬ë¡¤ë§ ë“±ìœ¼ë¡œ ì–»ì–´ë‚¸ ì½”í¼ìŠ¤ ë°ì´í„°ê°€ í•„ìš”ì— ë§ê²Œ ì „ì²˜ë¦¬ë˜ì§€ ì•Šì€ ìƒíƒœë¼ë©´???

í† í°í™”(tokenization) & ì •ì œ(cleaning) & ì •ê·œí™”(normalization)í•˜ëŠ” ì¼ì„ í•˜ê²Œ ë©ë‹ˆë‹¤.

## í† í°í™” Tokenization

ì£¼ì–´ì§„ ì½”í¼ìŠ¤(corpus)ì—ì„œ í† í°(token)ì´ë¼ ë¶ˆë¦¬ëŠ” ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…

í† í°ì˜ ë‹¨ìœ„ê°€ ìƒí™©ì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ, ë³´í†µ ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„ë¡œ í† í°ì„ ì •ì˜

### 1. ë‹¨ì–´ í† í°í™” Word Tokenization

í† í°ì˜ ê¸°ì¤€ì„ ë‹¨ì–´(word)ë¡œ í•˜ëŠ” ê²½ìš°, ë‹¨ì–´ í† í°í™”(word tokenization)ë¼ê³  í•©ë‹ˆë‹¤. 

ë‹¤ë§Œ, ì—¬ê¸°ì„œ ë‹¨ì–´(word)ëŠ” ë‹¨ì–´ ë‹¨ìœ„ ì™¸ì—ë„ ë‹¨ì–´êµ¬, ì˜ë¯¸ë¥¼ ê°–ëŠ” ë¬¸ìì—´ë¡œë„ ê°„ì£¼ë˜ê¸°ë„ í•©ë‹ˆë‹¤.

ì…ë ¥ìœ¼ë¡œë¶€í„° êµ¬ë‘ì (punctuation)ê³¼ ê°™ì€ ë¬¸ìëŠ” ì œì™¸ì‹œí‚¤ëŠ” ê°„ë‹¨í•œ ë‹¨ì–´ í† í°í™” ì‘ì—…
ì…ë ¥: Time is an illusion. Lunchtime double so!
ì¶œë ¥ : "Time", "is", "an", "illusion", "Lunchtime", "double", "so"

ë³´í†µ í† í°í™” ì‘ì—…ì€ ë‹¨ìˆœíˆ êµ¬ë‘ì ì´ë‚˜ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì „ë¶€ ì œê±°í•˜ëŠ” ì •ì œ(cleaning) ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œ í•´ê²°ë˜ì§€ ì•ŠëŠ”ë‹¤. 

êµ¬ë‘ì , íŠ¹ìˆ˜ë¬¸ìë¥¼ ì „ë¶€ ì œê±°í•˜ë©´ í† í°ì´ ì˜ë¯¸ë¥¼ ìƒì–´ë²„ë¦¬ëŠ” ê²½ìš°ê°€ ë°œìƒí•˜ê¸°ë„ í•©ë‹ˆë‹¤. ì‹¬ì§€ì–´ í•œêµ­ì–´ëŠ” ë„ì–´ì“°ê¸°ë§Œìœ¼ë¡œëŠ” ë‹¨ì–´ í† í°ì„ êµ¬ë¶„í•˜ê¸° ì–´ë µë‹¤. 

---

### 2. í† í°í™” ì¤‘ ìƒê¸°ëŠ” ì„ íƒì˜ ìˆœê°„

```python
from nltk.tokenize import word_tokenize  
print(word_tokenize("Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."))  
['Do', "n't", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', "'s", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']

from nltk.tokenize import WordPunctTokenizer  
print(WordPunctTokenizer().tokenize("Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."))
['Don', "'", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', "'", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']

from tensorflow.keras.preprocessing.text import text_to_word_sequence
print(text_to_word_sequence("Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."))
["don't", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', "jone's", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']
```

---

### 3. í† í°í™”ì—ì„œ ê³ ë ¤í•´ì•¼í•  ì‚¬í•­

1) êµ¬ë‘ì ì´ë‚˜ íŠ¹ìˆ˜ ë¬¸ìë¥¼ ë‹¨ìˆœ ì œì™¸í•´ì„œëŠ” ì•ˆ ëœë‹¤.

Ph.D  AT&T $45.55 01/02/06

2) ì¤„ì„ë§ê³¼ ë‹¨ì–´ ë‚´ì— ë„ì–´ì“°ê¸°ê°€ ìˆëŠ” ê²½ìš°.

New York   rock 'n' roll   we're

3) í‘œì¤€ í† í°í™” ì˜ˆì œ

í‘œì¤€ìœ¼ë¡œ ì“°ì´ê³  ìˆëŠ” í† í°í™” ë°©ë²• ì¤‘ í•˜ë‚˜ì¸ Penn Treebank Tokenizationì˜ ê·œì¹™

ê·œì¹™ 1. í•˜ì´í‘¼ìœ¼ë¡œ êµ¬ì„±ëœ ë‹¨ì–´ëŠ” í•˜ë‚˜ë¡œ ìœ ì§€í•œë‹¤.

ê·œì¹™ 2. doesn'tì™€ ê°™ì´ ì•„í¬ìŠ¤íŠ¸ë¡œí”¼ë¡œ 'ì ‘ì–´'ê°€ í•¨ê»˜í•˜ëŠ” ë‹¨ì–´ëŠ” ë¶„ë¦¬í•´ì¤€ë‹¤.

```python
from nltk.tokenize import TreebankWordTokenizer
tokenizer=TreebankWordTokenizer()
text="Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own."
print(tokenizer.tokenize(text))
['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', "n't", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']
```

---

### 4. ë¬¸ì¥ í† í°í™” Sentence Tokenization

ê°–ê³ ìˆëŠ” ì½”í¼ìŠ¤ ë‚´ì—ì„œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ êµ¬ë¶„í•˜ëŠ” ì‘ì—…ìœ¼ë¡œ ë¬¸ì¥ ë¶„ë¥˜(sentence segmentation)ë¼ê³ ë„ ë¶€ë¥¸ë‹¤.

ì£¼ì˜) ë§ˆì¹¨í‘œëŠ” ë¬¸ì¥ì˜ ëì´ ì•„ë‹ˆë”ë¼ë„ ë“±ì¥í•  ìˆ˜ ìˆë‹¤.

'IP 192.168.56.31 ì„œë²„ì— ë“¤ì–´ê°€ì„œ ë¡œê·¸ íŒŒì¼ ì €ì¥í•´ì„œ ukairia777@gmail.comë¡œ ê²°ê³¼ ì¢€ ë³´ë‚´ì¤˜. ê·¸ëŸ¬ê³ ë‚˜ì„œ ì ì‹¬ ë¨¹ìœ¼ëŸ¬ ê°€ì.'

NLTKì—ì„œëŠ” ì˜ì–´ ë¬¸ì¥ì˜ í† í°í™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” sent_tokenizeë¥¼ ì§€ì›í•˜ê³  ìˆë‹¤.

```python
from nltk.tokenize import sent_tokenize
text="His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near."
print(sent_tokenize(text))
['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']

from nltk.tokenize import sent_tokenize
text="I am actively looking for Ph.D. students. and you are a Ph.D student."
print(sent_tokenize(text))
['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']
```

í•œêµ­ì–´ì— ëŒ€í•œ ë¬¸ì¥ í† í°í™” ë„êµ¬ ë˜í•œ ì¡´ì¬í•©ë‹ˆë‹¤. 

```python
import kss

text='ë”¥ ëŸ¬ë‹ ìì—°ì–´ ì²˜ë¦¬ê°€ ì¬ë¯¸ìˆê¸°ëŠ” í•©ë‹ˆë‹¤. ê·¸ëŸ°ë° ë¬¸ì œëŠ” ì˜ì–´ë³´ë‹¤ í•œêµ­ì–´ë¡œ í•  ë•Œ ë„ˆë¬´ ì–´ë ¤ì›Œìš”. ë†ë‹´ì•„ë‹ˆì—ìš”. ì´ì œ í•´ë³´ë©´ ì•Œê±¸ìš”?'
print(kss.split_sentences(text))
['ë”¥ ëŸ¬ë‹ ìì—°ì–´ ì²˜ë¦¬ê°€ ì¬ë¯¸ìˆê¸°ëŠ” í•©ë‹ˆë‹¤.', 'ê·¸ëŸ°ë° ë¬¸ì œëŠ” ì˜ì–´ë³´ë‹¤ í•œêµ­ì–´ë¡œ í•  ë•Œ ë„ˆë¬´ ì–´ë ¤ì›Œìš”.', 'ë†ë‹´ì•„ë‹ˆì—ìš”.', 'ì´ì œ í•´ë³´ë©´ ì•Œê±¸ìš”?']
```

---

### 5. ì´ì§„ ë¶„ë¥˜ê¸° Binary Classifier

ë¬¸ì¥ í† í°í™”ì—ì„œì˜ ì˜ˆì™¸ ì‚¬í•­ì„ ë°œìƒì‹œí‚¤ëŠ” ë§ˆì¹¨í‘œì˜ ì²˜ë¦¬ë¥¼ ìœ„í•´ì„œ ì…ë ¥ì— ë”°ë¼ ë‘ ê°œì˜ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜í•˜ëŠ” ì´ì§„ ë¶„ë¥˜ê¸°(binary classifier)ë¥¼ ì‚¬ìš©í•˜ê¸°ë„ í•œë‹¤.

ë¬¼ë¡ , ì—¬ê¸°ì„œ ë§í•˜ëŠ” ë‘ ê°œì˜ í´ë˜ìŠ¤ëŠ”

1. ë§ˆì¹¨í‘œ(.)ê°€ ë‹¨ì–´ì˜ ì¼ë¶€ë¶„ì¼ ê²½ìš°. ì¦‰, ë§ˆì¹¨í‘œê°€ ì•½ì–´(abbreivation)ë¡œ ì“°ì´ëŠ” ê²½ìš°

2. ë§ˆì¹¨í‘œ(.)ê°€ ì •ë§ë¡œ ë¬¸ì¥ì˜ êµ¬ë¶„ì(boundary)ì¼ ê²½ìš°ë¥¼ ì˜ë¯¸í•  ê²ƒì…ë‹ˆë‹¤.

ì´ëŸ¬í•œ ë¬¸ì¥ í† í°í™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ë¡œëŠ” NLTK, OpenNLP, ìŠ¤íƒ í¬ë“œ CoreNLP, splitta, LingPipe ë“±ì´ ìˆìŠµë‹ˆë‹¤.

### 6. í•œêµ­ì–´ì—ì„œì˜ í† í°í™”ì˜ ì–´ë ¤ì›€.

í•œêµ­ì–´ì˜ ê²½ìš°ì—ëŠ” ë„ì–´ì“°ê¸° ë‹¨ìœ„ê°€ ë˜ëŠ” ë‹¨ìœ„ë¥¼ 'ì–´ì ˆ'ì´ë¼ê³  í•˜ëŠ”ë° ì¦‰, ì–´ì ˆ í† í°í™”ëŠ” í•œêµ­ì–´ NLPì—ì„œ ì§€ì–‘ë˜ê³  ìˆë‹¤.

ì–´ì ˆ í† í°í™”ì™€ ë‹¨ì–´ í† í°í™”ê°€ ê°™ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤. 

ê·¸ ì´ìœ ëŠ” í•œêµ­ì–´ê°€ ì˜ì–´ì™€ëŠ” ë‹¤ë¥¸ í˜•íƒœë¥¼ ê°€ì§€ëŠ” ì–¸ì–´ì¸ êµì°©ì–´ë¼ëŠ” ì ì—ì„œ ê¸°ì¸í•©ë‹ˆë‹¤. êµì°©ì–´ë€ ì¡°ì‚¬, ì–´ë¯¸ ë“±ì„ ë¶™ì—¬ì„œ ë§ì„ ë§Œë“œëŠ” ì–¸ì–´ë¥¼ ë§í•©ë‹ˆë‹¤.

í•œêµ­ì–´ í† í°í™”ì—ì„œëŠ” í˜•íƒœì†Œ(morpheme)ë€ ê°œë…ì„ ë°˜ë“œì‹œ ì´í•´í•´ì•¼ í•©ë‹ˆë‹¤.

1) í•œêµ­ì–´ëŠ” êµì°©ì–´ì´ë‹¤.

'ê·¸ê°€', 'ê·¸ì—ê²Œ', 'ê·¸ë¥¼' , ... 

ëŒ€ë¶€ë¶„ì˜ í•œêµ­ì–´ NLPì—ì„œ ì¡°ì‚¬ëŠ” ë¶„ë¦¬í•´ì¤„ í•„ìš”ê°€ ìˆë‹¤.

í•œêµ­ì–´ì—ì„œëŠ” í˜•íƒœì†Œ í† í°í™”ë¥¼ ìˆ˜í–‰í•´ì•¼ í•œë‹¤.

2) í•œêµ­ì–´ëŠ” ë„ì–´ì“°ê¸°ê°€ ì˜ì–´ë³´ë‹¤ ì˜ ì§€ì¼œì§€ì§€ ì•ŠëŠ”ë‹¤.

### 7. í’ˆì‚¬ íƒœê¹…(Part-of-speech tagging)

ë‹¨ì–´ëŠ” í‘œê¸°ëŠ” ê°™ì§€ë§Œ, í’ˆì‚¬ì— ë”°ë¼ì„œ ë‹¨ì–´ì˜ ì˜ë¯¸ê°€ ë‹¬ë¼ì§€ê¸°ë„ í•œë‹¤.

ex) 'ëª»' : nail, not

í’ˆì‚¬ íƒœê¹… : ë‹¨ì–´ í† í°í™” ê³¼ì •ì—ì„œ ê° ë‹¨ì–´ê°€ ì–´ë–¤ í’ˆì‚¬ë¡œ ì“°ì˜€ëŠ” ì§€ë¥¼ êµ¬ë¶„í•˜ëŠ” ì‘ì—…

### 8. NLTKì™€ KoNLPyë¥¼ ì´ìš©í•œ ì˜ì–´, í•œêµ­ì–´ í† í°í™” ì‹¤ìŠµ

NLTK ì—ì„œ ì˜ì–´ ì½”í¼ìŠ¤ì— í’ˆì‚¬ íƒœê¹… ê¸°ëŠ¥ì„ ì§€ì›í•˜ê³  ìˆë‹¤. 

```python
from nltk.tokenize import word_tokenize
text="I am actively looking for Ph.D. students. and you are a Ph.D. student."
print(word_tokenize(text))

['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']

from nltk.tag import pos_tag
x=word_tokenize(text)
pos_tag(x)

[('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]
```

í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•´ì„œëŠ” KoNLPy íŒ¨í‚¤ì§€ ì‚¬ìš©.

KoNLPy ë¥¼ í†µí•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ë¡œ Okt, Mecab, Komoran, Hannanum, Kkma ê°€ ìˆë‹¤. 

Okt ì‚¬ìš©

```python
from konlpy.tag import Okt  
okt=Okt()  
print(okt.morphs("ì—´ì‹¬íˆ ì½”ë”©í•œ ë‹¹ì‹ , ì—°íœ´ì—ëŠ” ì—¬í–‰ì„ ê°€ë´ìš”"))

['ì—´ì‹¬íˆ', 'ì½”ë”©', 'í•œ', 'ë‹¹ì‹ ', ',', 'ì—°íœ´', 'ì—ëŠ”', 'ì—¬í–‰', 'ì„', 'ê°€ë´ìš”']

print(okt.pos("ì—´ì‹¬íˆ ì½”ë”©í•œ ë‹¹ì‹ , ì—°íœ´ì—ëŠ” ì—¬í–‰ì„ ê°€ë´ìš”"))

[('ì—´ì‹¬íˆ','Adverb'), ('ì½”ë”©', 'Noun'), ('í•œ', 'Josa'), ('ë‹¹ì‹ ', 'Noun'), (',', 'Punctuation'), ('ì—°íœ´', 'Noun'), ('ì—ëŠ”', 'Josa'), ('ì—¬í–‰', 'Noun'), ('ì„', 'Josa'), ('ê°€ë´ìš”', 'Verb')]

print(okt.nouns("ì—´ì‹¬íˆ ì½”ë”©í•œ ë‹¹ì‹ , ì—°íœ´ì—ëŠ” ì—¬í–‰ì„ ê°€ë´ìš”"))

['ì½”ë”©', 'ë‹¹ì‹ ', 'ì—°íœ´', 'ì—¬í–‰']
```

Kkma ì‚¬ìš©

```python
from konlpy.tag import Kkma  
kkma=Kkma()  
print(kkma.morphs("ì—´ì‹¬íˆ ì½”ë”©í•œ ë‹¹ì‹ , ì—°íœ´ì—ëŠ” ì—¬í–‰ì„ ê°€ë´ìš”"))

['ì—´ì‹¬íˆ', 'ì½”ë”©', 'í•˜', 'ã„´', 'ë‹¹ì‹ ', ',', 'ì—°íœ´', 'ì—', 'ëŠ”', 'ì—¬í–‰', 'ì„', 'ê°€ë³´', 'ì•„ìš”']

print(kkma.pos("ì—´ì‹¬íˆ ì½”ë”©í•œ ë‹¹ì‹ , ì—°íœ´ì—ëŠ” ì—¬í–‰ì„ ê°€ë´ìš”"))

[('ì—´ì‹¬íˆ','MAG'), ('ì½”ë”©', 'NNG'), ('í•˜', 'XSV'), ('ã„´', 'ETD'), ('ë‹¹ì‹ ', 'NP'), (',', 'SP'), ('ì—°íœ´', 'NNG'), ('ì—', 'JKM'), ('ëŠ”', 'JX'), ('ì—¬í–‰', 'NNG'), ('ì„', 'JKO'), ('ê°€ë³´', 'VV'), ('ì•„ìš”', 'EFN')]

print(kkma.nouns("ì—´ì‹¬íˆ ì½”ë”©í•œ ë‹¹ì‹ , ì—°íœ´ì—ëŠ” ì—¬í–‰ì„ ê°€ë´ìš”"))

['ì½”ë”©', 'ë‹¹ì‹ ', 'ì—°íœ´', 'ì—¬í–‰']
```

---

## ì •ì œ(Cleaning) and  ì •ê·œí™”(Normalization)

**í† í°í™”(tokenization)** : ì½”í¼ìŠ¤ì—ì„œ ìš©ë„ì— ë§ê²Œ í† í°ì„ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…

í† í°í™” ì‘ì—… ì „, í›„ì— ë°ì´í„°ë¥¼ ìš©ë„ì— ë§ê²Œ ì •ì œ ë° ì •ê·œí™”

**ì •ì œ(Cleaning)** : ê°–ê³  ìˆëŠ” ì½”í¼ìŠ¤ë¡œë¶€í„° ë…¸ì´ì¦ˆ ë°ì´í„° ì œê±°

**ì •ê·œí™”(Normalization)** : í‘œí˜„ ë°©ë²•ì´ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì„ í†µí•©ì‹œì¼œì„œ ê°™ì€ ë‹¨ì–´ë¡œ ë§Œë“¤ì–´ì¤Œ

### 1. ê·œì¹™ì— ê¸°ë°˜í•œ í‘œê¸°ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì˜ í†µí•©

ê°™ì€ ì˜ë¯¸ë¥¼ ê°–ê³  ìˆìŒì—ë„, í‘œê¸°ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì„ í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œ ì •ê·œí™” í•˜ëŠ” ë°©ë²•. 

ex) USA, US    uh-huh, uhhuh 

ë‹¤ìŒ ì±•í„°ì—ì„œ í‘œê¸°ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì„ í†µí•©í•˜ëŠ” ë°©ë²•ì¸ 

**ì–´ê°„ ì¶”ì¶œ**(stemming), **í‘œì œì–´ ì¶”ì¶œ(**lemmatization) ë°°ìš¸ ì˜ˆì •

### 2. ëŒ€, ì†Œë¬¸ì í†µí•©

ë¬´ì‘ì • í†µí•©í•´ì„œëŠ” ì•ˆ ëœë‹¤!

ğŸ’¡US(ë¯¸êµ­) ê³¼ us(ìš°ë¦¬) 

í•˜ì§€ë§Œ ê²°êµ­ì—ëŠ” ì˜ˆì™¸ ì‚¬í•­ì„ í¬ê²Œ ê³ ë ¤í•˜ì§€ ì•Šê³ , ëª¨ë“  ì½”í¼ìŠ¤ë¥¼ ì†Œë¬¸ìë¡œ ë°”ê¾¸ëŠ” ê²ƒì´ ì¢…ì¢… ë” ì‹¤ìš©ì ì¸ í•´ê²°ì±…ì´ ëœë‹¤.

### 3. ë¶ˆí•„ìš”í•œ ë‹¨ì–´ì˜ ì œê±° (Removing Unnecessary Words)

ë¶ˆí•„ìš” ë‹¨ì–´ë“¤ì„ ì œê±° í•˜ëŠ” ë°©ë²•

- ë¶ˆìš©ì–´
- ë“±ì¥ ë¹ˆë„ê°€ ì ì€ ë‹¨ì–´ (Removing Rare Words)
- ê¸¸ì´ê°€ ì§§ì€ ë‹¨ì–´ (Removing words with very a short length)

ì˜ì–´ ë‹¨ì–´ì˜ í‰ê·  ê¸¸ì´ëŠ” 6-7 / í•œêµ­ì–´ ë‹¨ì–´ì˜ í‰ê·  ê¸¸ì´ 2-3 ë¡œ ì¶”ì •

ex) dragon ìš© / school í•™êµ 

ì´ëŸ¬í•œ íŠ¹ì„±ìœ¼ë¡œ ì¸í•´ ì˜ì–´ëŠ” ê¸¸ì´ê°€ 2-3 ì´í•˜ì¸ ë‹¨ì–´ë¥¼ ì œê±°í•˜ëŠ” ê²ƒ ë§Œìœ¼ë¡œë„ í¬ê²Œ ì˜ë¯¸ë¥¼ ê°–ì§€ ëª»í•˜ëŠ” ë‹¨ì–´ë¥¼ ì¤„ì´ëŠ” íš¨ê³¼ë¥¼ ê°–ê³  ìˆë‹¤. 

```python
import re
text = "I was wondering if anyone out there could enlighten me on this car."
shortword = re.compile(r'\W*\b\w{1,2}\b')
print(shortword.sub('', text))

was wondering anyone out there could enlighten this car.
```

### 4. ì •ê·œ í‘œí˜„ì‹(Regular Expression)

ì–»ì–´ë‚¸ ì½”í¼ìŠ¤ì—ì„œ ë…¸ì´ì¦ˆ ë°ì´í„°ì˜ íŠ¹ì§•ì„ ì¡ì•„ë‚¼ ìˆ˜ ìˆë‹¤ë©´, **ì •ê·œ í‘œí˜„ì‹**ì„ í†µí•´ì„œ ì´ë¥¼ ì œê±°í•  ìˆ˜ ìˆëŠ” ê²½ìš°ë„ ë§ë‹¤. 

---

## ì–´ê°„ ì¶”ì¶œ(Stemming) and í‘œì œì–´ ì¶”ì¶œ(Lemmatization)

ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ì „ì²˜ë¦¬, ë” ì •í™•íˆëŠ” ì •ê·œí™”ì˜ ì§€í–¥ì ì€ ì–¸ì œë‚˜ ê°–ê³  ìˆëŠ” ì½”í¼ìŠ¤ë¡œë¶€í„° **ë³µì¡ì„±ì„ ì¤„ì´ëŠ” ì¼**

ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì œë¥¼ í’€ê³ ì í•˜ëŠ” **BoW(Bag of Words)**í‘œí˜„ì„ ì‚¬ìš©í•˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ ë¬¸ì œì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë¨.

**í‘œì œì–´ ì¶”ì¶œ**ê³¼ **ì–´ê°„ ì¶”ì¶œ**ì˜ **ì°¨ì´** 
í‘œì œì–´ ì¶”ì¶œ 
ë¬¸ë§¥ì„ ê³ ë ¤í•˜ë©°, ìˆ˜í–‰í–ˆì„ ë•Œì˜ ê²°ê³¼ëŠ” í•´ë‹¹ ë‹¨ì–´ì˜ í’ˆì‚¬ ì •ë³´ë¥¼ ë³´ì¡´.(POS íƒœê·¸ë¥¼ ë³´ì¡´í•œë‹¤ê³  ìƒê°í•˜ë©´ ë¨)

ì–´ê°„ ì¶”ì¶œ
í’ˆì‚¬ ì •ë³´ê°€ ë³´ì¡´ë˜ì§€ ì•ŠìŒ. (POS íƒœê·¸ ê³ ë ¤ X)
ì–´ê°„ ì¶”ì¶œ í•œ ê²°ê³¼ê°€ ì‚¬ì „ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ì¼ ê²½ìš°ê°€ ë§ë‹¤.

### 1. í‘œì œì–´ ì¶”ì¶œ(Lemmatization)

ex) am, are, is ì˜ ë¿Œë¦¬ ë‹¨ì–´ be  â†’ ì´ ë‹¨ì–´ë“¤ì˜ í‘œì œì–´ be

í‘œì œì–´ ì¶”ì¶œì„ í•˜ëŠ” ê°€ì¥ ì„¬ì„¸í•œ ë°©ë²•

ë‹¨ì–´ì˜ **í˜•íƒœí•™ì  íŒŒì‹±**ì„ ë¨¼ì € ì§„í–‰

í˜•íƒœì†ŒëŠ” ë‘ ê°€ì§€ ì¢…ë¥˜ê°€ ìˆë‹¤. ì–´ê°„(stem)ê³¼ ì ‘ì‚¬(affix)
1) ì–´ê°„ stem
: ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë‹´ê³  ìˆëŠ” ë‹¨ì–´ì˜ í•µì‹¬ ë¶€ë¶„

2)ì ‘ì‚¬ affix
: ë‹¨ì–´ì— ì¶”ê°€ì ì¸ ì˜ë¯¸ë¥¼ ì£¼ëŠ” ë¶€ë¶„

**í˜•íƒœí•™ì  íŒŒì‹±**ì´ë€?

ì´ ë‘ ê°€ì§€ êµ¬ì„± ìš”ì†Œë¥¼ ë¶„ë¦¬í•˜ëŠ” ì‘ì—…

ex) cats â†’ cat(ì–´ê°„) + -s(ì ‘ì‚¬)

```python
from nltk.stem import WordNetLemmatizer
n=WordNetLemmatizer()
words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']
print([n.lemmatize(w) for w in words])

['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 
'fly', **'dy'**, 'watched', **'ha'**, 'starting']
```

'dy' ë‚˜ 'ha' ì™€ ê°™ì´ ì˜ë¯¸ë¥¼ ì•Œ ìˆ˜ ì—†ëŠ” ì ì ˆí•˜ì§€ ëª»í•œ ë‹¨ì–´ ì¶œë ¥

lemmatizer ê°€ ë³¸ë˜ ë‹¨ì–´ì˜ í’ˆì‚¬ ì •ë³´ë¥¼ ì•Œì•„ì•¼ë§Œ ì •í™•í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. 

WordNetLemmatizer ëŠ” ì…ë ¥ìœ¼ë¡œ ë‹¨ì–´ê°€ ë™ì‚¬ í’ˆì‚¬ë¼ëŠ” ì‚¬ì‹¤ì„ ì•Œë ¤ì¤„ ìˆ˜ ìˆìŒ.

```python
n.lemmatize('dies', 'v')

'die'

n.lemmatize('has', 'v')

'have'
```

### 2. ì–´ê°„ ì¶”ì¶œ(Stemming)

í˜•íƒœí•™ì  ë¶„ì„ì„ **ë‹¨ìˆœí™”í•œ ë²„ì ¼**ì´ë¼ê³  ë³¼ ìˆ˜ë„ ìˆê³ , 
**ì •í•´ì§„ ê·œì¹™**ë§Œ ë³´ê³  ë‹¨ì–´ì˜ ì–´ë¯¸ë¥¼ ìë¥´ëŠ” ì–´ë¦¼ì§ì‘ì˜ ì‘ì—…ì´ë¼ ë³¼ ìˆ˜ ë„ ìˆë‹¤.

ì¦‰, **ì„¬ì„¸í•œ ì‘ì—…ì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì—** ê²°ê³¼ ë‹¨ì–´ê°€ ì‚¬ì „ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ì¼ ìˆ˜ ìˆë‹¤.

ex) ì–´ê°„ ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜ì¸ í¬í„° ì•Œê³ ë¦¬ì¦˜(Porter Algorithm)

```python
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
s = PorterStemmer()
text="This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes."
words=word_tokenize(text)
print(words)

['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', "'s", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']

print([s.stem(w) for w in words])

['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', "'s", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']
```

```python
# ê°€ë ¹, í¬í„° ì•Œê³ ë¦¬ì¦˜ì˜ ì–´ê°„ ì¶”ì¶œì€ ì´ëŸ¬í•œ ê·œì¹™ì„ ê°€ì§.
# ALIZE â†’ AL
# ANCE â†’ ì œê±°
# ICAL â†’ IC

words=['formalize', 'allowance', 'electricical']
print([s.stem(w) for w in words])

['formal', 'allow', 'electric']
```

ê°™ì€ ë‹¨ì–´ì— ëŒ€í•´ì„œ í‘œì œì–´ ì¶”ì¶œê³¼ ì–´ê°„ ì¶”ì¶œì˜ ê²°ê³¼ ì°¨ì´

**Stemming** 

am â†’ am

the going â†’ the go

having â†’ hav

**Lemmatization**

am â†’ be

the going â†’ the going

having â†’ have

### 3. í•œêµ­ì–´ì—ì„œì˜ ì–´ê°„ ì¶”ì¶œ

í•œêµ­ì–´ëŠ” 5ì–¸ 9í’ˆì‚¬ì˜ êµ¬ì¡°ë¥¼ ê°€ì§

ê·¸ ì¤‘ì— **ìš©ì–¸**ì— í•´ë‹¹ë˜ëŠ” '**ë™ì‚¬**'ì™€ '**í˜•ìš©ì‚¬**'ëŠ” 

**ì–´ê°„**(stem)ê³¼ **ì–´ë¯¸**(ending)ì˜ ê²°í•©ìœ¼ë¡œ êµ¬ì„±ë¨. 

**(1) í™œìš© (conjugation)**

í™œìš©ì´ë€?     ìš©ì–¸ì˜ ì–´ê°„ê³¼ ì–´ë¯¸ë¥¼ ê°€ì§€ëŠ” ì¼

í™œìš©ì€ ì–´ê°„ì´ ì–´ë¯¸ë¥¼ ì·¨í•  ë•Œ, 

ì–´ê°„ì˜ ëª¨ìŠµì´ ì¼ì •í•˜ë‹¤ë©´ **ê·œì¹™ í™œìš©**

ex) ì¡/ì–´ê°„ + ë‹¤/ì–´ë¯¸

ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì–´ë¯¸ë¥¼ ë‹¨ìˆœíˆ ë¶„ë¦¬í•´ì£¼ë©´ ì–´ê°„ ì¶”ì¶œì´ ë¨.

ì–´ê°„ì´ë‚˜ ì–´ë¯¸ì˜ ëª¨ìŠµì´ ë³€í•˜ëŠ” **ë¶ˆê·œì¹™ í™œìš©**

ex) 'ë“£-'  â†’  'ë“£/ë“¤-'  

ë‹¨ìˆœí•œ ë¶„ë¦¬ ë§Œìœ¼ë¡œ ì–´ê°„ ì¶”ì¶œì´ ë˜ì§€ ì•Šê³  ì¢€ ë” ë³µì¡í•œ ê·œì¹™ì„ í•„ìš”ë¡œ í•¨.

---

## ë¶ˆìš©ì–´ (Stopword)

ìì£¼ ë“±ì¥í•˜ì§€ë§Œ ë¶„ì„ì„ í•˜ëŠ” ê²ƒì— ìˆì–´ì„œëŠ” í° ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ” ë‹¨ì–´

ex) I, my, me

NLTKì—ì„œëŠ” ìœ„ì™€ ê°™ì€ 100ì—¬ê°œ ì´ìƒì˜ ì˜ì–´ ë‹¨ì–´ë“¤ì„ ë¶ˆìš©ì–´ë¡œ íŒ¨í‚¤ì§€ ë‚´ì—ì„œ ë¯¸ë¦¬ ì •ì˜í•˜ê³  ìˆë‹¤.

### 1. NLTKì—ì„œ ë¶ˆìš©ì–´ í™•ì¸í•˜ê¸°

```python
from nltk.corpus import stopwords  
stopwords.words('english')[:10]

['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']
```

### 2. **NLTKë¥¼ í†µí•´ì„œ ë¶ˆìš©ì–´ ì œê±°í•˜ê¸°**

```python
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize 

example = "Family is not an important thing. It's everything."
stop_words = set(stopwords.words('english')) 

word_tokens = word_tokenize(example)

result = []
for w in word_tokens: 
    if w not in stop_words: 
        result.append(w) 

print(word_tokens) 
print(result)

['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', "'s", 'everything', '.']
['Family', 'important', 'thing', '.', 'It', "'s", 'everything', '.']
```

### 3. **í•œêµ­ì–´ì—ì„œ ë¶ˆìš©ì–´ ì œê±°í•˜ê¸°**

ê°„ë‹¨í•˜ê²ŒëŠ” í† í°í™” í›„ì— ì¡°ì‚¬, ì ‘ì†ì‚¬ ë“±ì„ ì œê±°í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤.

í•˜ì§€ë§Œ, ëª…ì‚¬, í˜•ìš©ì‚¬ì™€ ê°™ì€ ë‹¨ì–´ë“¤ ì¤‘ì—ì„œë„ ë¶ˆìš©ì–´ë¡œì„œ ì œê±°í•˜ê³  ì‹¶ì€ ë‹¨ì–´ë“¤ì´ ìƒê¸°ê¸°ë„ í•œë‹¤. 

ê²°êµ­ì— ì‚¬ìš©ìê°€ ì§ì ‘ ë¶ˆìš©ì–´ ì‚¬ì „ì„ ë§Œë“¤ê²Œ ë˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤. 

```python
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize 

example = "ê³ ê¸°ë¥¼ ì•„ë¬´ë ‡ê²Œë‚˜ êµ¬ìš°ë ¤ê³  í•˜ë©´ ì•ˆ ë¼. ê³ ê¸°ë¼ê³  ë‹¤ ê°™ì€ ê²Œ ì•„ë‹ˆê±°ë“ . ì˜ˆì»¨ëŒ€ ì‚¼ê²¹ì‚´ì„ êµ¬ìš¸ ë•ŒëŠ” ì¤‘ìš”í•œ ê²Œ ìˆì§€."
stop_words = "ì•„ë¬´ê±°ë‚˜ ì•„ë¬´ë ‡ê²Œë‚˜ ì–´ì°Œí•˜ë“ ì§€ ê°™ë‹¤ ë¹„ìŠ·í•˜ë‹¤ ì˜ˆì»¨ëŒ€ ì´ëŸ´ì •ë„ë¡œ í•˜ë©´ ì•„ë‹ˆê±°ë“ "
# ìœ„ì˜ ë¶ˆìš©ì–´ëŠ” ëª…ì‚¬ê°€ ì•„ë‹Œ ë‹¨ì–´ ì¤‘ì—ì„œ ì €ìê°€ ì„ì˜ë¡œ ì„ ì •í•œ ê²ƒìœ¼ë¡œ ì‹¤ì œ ì˜ë¯¸ìˆëŠ” ì„ ì • ê¸°ì¤€ì´ ì•„ë‹˜
stop_words=stop_words.split(' ')
word_tokens = word_tokenize(example)

result = [] 
for w in word_tokens: 
    if w not in stop_words: 
        result.append(w) 
# ìœ„ì˜ 4ì¤„ì€ ì•„ë˜ì˜ í•œ ì¤„ë¡œ ëŒ€ì²´ ê°€ëŠ¥
# result=[word for word in word_tokens if not word in stop_words]

print(word_tokens) 
print(result)

['ê³ ê¸°ë¥¼', 'ì•„ë¬´ë ‡ê²Œë‚˜', 'êµ¬ìš°ë ¤ê³ ', 'í•˜ë©´', 'ì•ˆ', 'ë¼', '.', 'ê³ ê¸°ë¼ê³ ', 'ë‹¤', 'ê°™ì€', 'ê²Œ', 'ì•„ë‹ˆê±°ë“ ', '.', 'ì˜ˆì»¨ëŒ€', 'ì‚¼ê²¹ì‚´ì„', 'êµ¬ìš¸', 'ë•ŒëŠ”', 'ì¤‘ìš”í•œ', 'ê²Œ', 'ìˆì§€', '.']
['ê³ ê¸°ë¥¼', 'êµ¬ìš°ë ¤ê³ ', 'ì•ˆ', 'ë¼', '.', 'ê³ ê¸°ë¼ê³ ', 'ë‹¤', 'ê°™ì€', 'ê²Œ', '.', 'ì‚¼ê²¹ì‚´ì„', 'êµ¬ìš¸', 'ë•ŒëŠ”', 'ì¤‘ìš”í•œ', 'ê²Œ', 'ìˆì§€', '.']
```

í•œêµ­ì–´ ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ëŠ” ë” ì¢‹ì€ ë°©ë²•ì€ 
ì½”ë“œ ë‚´ì—ì„œ ì§ì ‘ ì •ì˜í•˜ì§€ ì•Šê³  **txt íŒŒì¼ì´ë‚˜ csv íŒŒì¼ë¡œ ìˆ˜ë§ì€ ë¶ˆìš©ì–´ë¥¼ ì •ë¦¬í•´ë†“ê³ **, ì´ë¥¼ ë¶ˆëŸ¬ì™€ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤. 

---

## ì •ê·œ í‘œí˜„ì‹(Regular Expression)

ì •ê·œ í‘œí˜„ì‹ ëª¨ë“ˆ re ë¥¼ ì´ìš©í•˜ì—¬ íŠ¹ì • ê·œì¹™ì´ ìˆëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì •ì œ.

### 1. ì •ê·œ í‘œí˜„ì‹ ë¬¸ë²•ê³¼ ëª¨ë“ˆ í•¨ìˆ˜

**1) ì •ê·œ í‘œí˜„ì‹ ë¬¸ë²•**

<img src = "/image/Untitled.png" width = "600px">

**2) ì •ê·œí‘œí˜„ì‹ ëª¨ë“ˆ í•¨ìˆ˜**

<img src = "/image/Untitled 1.png" width = "600px">

### 2. ì •ê·œ í‘œí˜„ì‹ ì‹¤ìŠµ

- ì˜ˆì œ

    ```python
    import re
    r = re.compile('a.c')
    r.search('kkk') # ì•„ë¬´ëŸ° ê²°ê³¼ë„ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤.

    r.search('kkkkabc')
    <re.Match object; span=(4, 7), match='abc'>
    ```

    ```python
    import re
    r=re.compile("ab?c")
    r.search("abbc") # ì•„ë¬´ëŸ° ê²°ê³¼ë„ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤.

    r.search("abc")
    <_sre.SRE_Match object; span=(0, 3), match='abc'>  
    bê°€ ìˆëŠ” ê²ƒìœ¼ë¡œ íŒë‹¨í•˜ì—¬ abcë¥¼ ë§¤ì¹˜í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    r.search("ac")
    <_sre.SRE_Match object; span=(0, 2), match='ac'>
    ```

    ```python
    import re
    r=re.compile("ab*c")
    r.search("a") # ì•„ë¬´ëŸ° ê²°ê³¼ë„ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤.

    r.search("ac")
    <_sre.SRE_Match object; span=(0, 2), match='ac'>  

    r.search("abc") 
    <_sre.SRE_Match object; span=(0, 3), match='abc'> 

    r.search("abbbbc") 
    <_sre.SRE_Match object; span=(0, 6), match='abbbbc'>
    ```

    ```python
    import re
    r=re.compile("ab+c")
    r.search("ac") # ì•„ë¬´ëŸ° ê²°ê³¼ë„ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤.

    r.search("abc") 
    <_sre.SRE_Match object; span=(0, 3), match='abc'>   

    r.search("abbbbc") 
    <_sre.SRE_Match object; span=(0, 6), match='abbbbc'>
    ```

    ```python
    import re
    r=re.compile("^a")
    r.search("bbc") # ì•„ë¬´ëŸ° ê²°ê³¼ë„ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤.

    r.search("ab")                                                                                                    
    <_sre.SRE_Match object; span=(0, 1), match='a'>
    ```

    ```python
    import re
    r=re.compile("ab{2}c")
    r.search("ac") # ì•„ë¬´ëŸ° ê²°ê³¼ë„ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤.

    r.search("abc") # ì•„ë¬´ëŸ° ê²°ê³¼ë„ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤.

    r.search("abbc")
    <_sre.SRE_Match object; span=(0, 4), match='abbc'>

    r.search("abbbbbc") # ì•„ë¬´ëŸ° ê²°ê³¼ë„ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤.
    ```

    ```python
    import re
    r=re.compile("ab{2,8}c")
    r.search("ac") # ì•„ë¬´ëŸ° ê²°ê³¼ë„ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤.
    r.search("ac") # ì•„ë¬´ëŸ° ê²°ê³¼ë„ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤.
    r.search("abc") # ì•„ë¬´ëŸ° ê²°ê³¼ë„ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤.
    r.search("abbc")
    <_sre.SRE_Match object; span=(0, 4), match='abbc'>
    r.search("abbbbbbbbc")
    <_sre.SRE_Match object; span=(0, 10), match='abbbbbbbbc'>
    r.search("abbbbbbbbbc") # ì•„ë¬´ëŸ° ê²°ê³¼ë„ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤.
    ```

    ```python
    import re
    r=re.compile("a{2,}bc")
    r.search("bc") # ì•„ë¬´ëŸ° ê²°ê³¼ë„ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤.
    r.search("aa") # ì•„ë¬´ëŸ° ê²°ê³¼ë„ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤.
    r.search("aabc")
    <_sre.SRE_Match object; span=(0, 4), match='aabc'>
    r.search("aaaaaaaabc")
    <_sre.SRE_Match object; span=(0, 10), match='aaaaaaaabc'>
    ```

    ```python
    import re
    r=re.compile("[abc]") # [abc]ëŠ” [a-c]ì™€ ê°™ë‹¤.
    r.search("zzz") # ì•„ë¬´ëŸ° ê²°ê³¼ë„ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤.
    r.search("a")
    <_sre.SRE_Match object; span=(0, 1), match='a'> 
    r.search("aaaaaaa")                                                                                               
    <_sre.SRE_Match object; span=(0, 1), match='a'> 
    r.search("baac")      
    <_sre.SRE_Match object; span=(0, 1), match='b'>

    import re
    r=re.compile("[a-z]")
    r.search("AAA") # ì•„ë¬´ëŸ° ê²°ê³¼ë„ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤.
    r.search("aBC")
    <_sre.SRE_Match object; span=(0, 1), match='a'>
    r.search("111") # ì•„ë¬´ëŸ° ê²°ê³¼ë„ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤.
    ```

    ```python
    import re
    r=re.compile("[^abc]")
    r.search("a") # ì•„ë¬´ëŸ° ê²°ê³¼ë„ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤.
    r.search("ab") # ì•„ë¬´ëŸ° ê²°ê³¼ë„ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤.
    r.search("b") # ì•„ë¬´ëŸ° ê²°ê³¼ë„ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤.
    r.search("d")
    <_sre.SRE_Match object; span=(0, 1), match='d'> 
    r.search("1")                                                                                                
    <_sre.SRE_Match object; span=(0, 1), match='1'>
    ```

### 3. ì •ê·œ í‘œí˜„ì‹ ëª¨ë“ˆ í•¨ìˆ˜ ì˜ˆì œ

- ì˜ˆì œ

    **(1) re.match() ì™€ re.search()ì˜ ì°¨ì´**

    ```python
    import re
    r=re.compile("ab.")
    r.search("kkkabc")  
    <_sre.SRE_Match object; span=(3, 6), match='abc'>   

    r.match("kkkabc")  #ì•„ë¬´ëŸ° ê²°ê³¼ë„ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤.
    r.match("abckkk")  
    <_sre.SRE_Match object; span=(0, 3), match='abc'>
    ```

    **(2) re.split()**

    ```python
    import re
    text="ì‚¬ê³¼+ë”¸ê¸°+ìˆ˜ë°•+ë©”ë¡ +ë°”ë‚˜ë‚˜"
    re.split("\+",text)
    ['ì‚¬ê³¼', 'ë”¸ê¸°', 'ìˆ˜ë°•', 'ë©”ë¡ ', 'ë°”ë‚˜ë‚˜']
    ```

    **(3) re.findall()**

    ```python
    import re
    text="ì´ë¦„ : ê¹€ì² ìˆ˜
    ì „í™”ë²ˆí˜¸ : 010 - 1234 - 1234
    ë‚˜ì´ : 30
    ì„±ë³„ : ë‚¨"""  
    re.findall("\d+",text)

    ['010', '1234', '1234', '30']

    re.findall("\d+", "ë¬¸ìì—´ì…ë‹ˆë‹¤.")

    [] # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¦¬í„´í•œë‹¤.
    ```

    **(4) re.sub()**

    ```python
    import re
    text="Regular expression : A regular expression, regex or regexp[1] (sometimes called a rational expression)[2][3] is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern."
    re.sub('[^a-zA-Z]',' ',text)

    'Regular expression   A regular expression  regex or regexp     sometimes called a rational expression        is  in theoretical computer science and formal language theory  a sequence of characters that define a search pattern '
    ```

### 4. ì •ê·œ í‘œí˜„ì‹ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì˜ˆì œ

```python
import re  

text = """100 John    PROF
101 James   STUD
102 Mac   STUD"""  

re.split('\s+', text)  
['100', 'John', 'PROF', '101', 'James', 'STUD', '102', 'Mac', 'STUD']

re.findall('\d+',text)  
['100', '101', '102']

re.findall('[A-Z]',text)
['J', 'P', 'R', 'O', 'F', 'J', 'S', 'T', 'U', 'D', 'M', 'S', 'T', 'U', 'D']
# ì´ëŠ” ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²°ê³¼ê°€ ì•„ë‹™ë‹ˆë‹¤. 
# ì´ ê²½ìš°, ì—¬ëŸ¬ê°€ì§€ ë°©ë²•ì´ ìˆê² ì§€ë§Œ ëŒ€ë¬¸ìê°€ ì—°ì†ì ìœ¼ë¡œ 4ë²ˆ ë“±ì¥í•˜ëŠ” ê²½ìš°ë¡œ ì¡°ê±´ì„ ì¶”ê°€í•´ë´…ì‹œë‹¤.

re.findall('[A-Z]{4}',text)  
['PROF', 'STUD', 'STUD']
# ëŒ€ë¬¸ìë¡œ êµ¬ì„±ëœ ë¬¸ìì—´ë“¤ì„ ì œëŒ€ë¡œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
# ì´ë¦„ì˜ ê²½ìš°ì—ëŠ” ëŒ€ë¬¸ìì™€ ì†Œë¬¸ìê°€ ì„ì—¬ìˆëŠ” ìƒí™©ì…ë‹ˆë‹¤. ì´ë¦„ì— ëŒ€í•œ í–‰ì˜ ê°’ì„ ê°–ê³ ì˜¤ê³  ì‹¶ë‹¤ë©´ ì²˜ìŒì— ëŒ€ë¬¸ìê°€ ë“±ì¥í•˜ê³ , ê·¸ í›„ì— ì†Œë¬¸ìê°€ ì—¬ëŸ¬ë²ˆ ë“±ì¥í•˜ëŠ” ê²½ìš°ì— ë§¤ì¹˜í•˜ê²Œ í•©ë‹ˆë‹¤.

re.findall('[A-Z][a-z]+',text)
['John', 'James', 'Mac'] 

import re
letters_only = re.sub('[^a-zA-Z]', ' ', text)
```

### 5. ì •ê·œ í‘œí˜„ì‹ì„ ì´ìš©í•œ í† í°í™”

NLTKì—ì„œ ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•´ ë‹¨ì–´ í† í°í™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” **RegexpTokenizer** ì§€ì›

```python
import nltk
from nltk.tokenize import RegexpTokenizer
tokenizer=RegexpTokenizer("[\w]+")
print(tokenizer.tokenize("Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop"))

['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']

import nltk
from nltk.tokenize import RegexpTokenizer
tokenizer=RegexpTokenizer("[\s]+", gaps=True)

print(tokenizer.tokenize("Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop"))
["Don't", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name,', 'Mr.', "Jone's", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']

gaps=true ëŠ” í•´ë‹¹ ì •ê·œ í¬í˜„ì‹ì„ í† í°ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ìœ„í•œ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤ëŠ” ì˜ë¯¸
gaps=true ê¸°ì¬í•˜ì§€ ì•Šìœ¼ë©´ ê³µë°±ë“¤ë§Œ ë‚˜ì˜¨ë‹¤.
```

---

## ì •ìˆ˜ ì¸ì½”ë”© (Integer Encoding)

ìì—°ì–´ ì²˜ë¦¬ì—ì„œëŠ” í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë°”ê¾¸ëŠ” ì—¬ëŸ¬ê°€ì§€ ê¸°ë²•ë“¤ì´ ìˆë‹¤.

ë³¸ê²©ì ì¸ ì²« ë‹¨ê³„ë¡œ ê° ë‹¨ì–´ë¥¼ ê³ ìœ í•œ ì •ìˆ˜ì— mapping ì‹œí‚¤ëŠ” ì „ì²˜ë¦¬ ì‘ì—…ì´ í•„ìš”í•  ë•Œê°€ ìˆë‹¤.

ì¸ë±ìŠ¤ë¥¼ ë¶€ì—¬í•˜ëŠ” ë°©ë²•ì€ ëœë¤ìœ¼ë¡œ ë¶€ì—¬í•˜ê¸°ë„ í•˜ì§€ë§Œ, 

ë³´í†µì€ ë‹¨ì–´ì— ëŒ€í•œ ë¹ˆë„ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•œ ë’¤ì— ë¶€ì—¬í•œë‹¤.

### 1. ì •ìˆ˜ ì¸ì½”ë”© (Integer Encoding)

ë‹¨ì–´ë¥¼ ë¹ˆë„ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•œ ë‹¨ì–´ ì§‘í•©ì„ ë§Œë“¤ê³ , ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ìˆ˜ë¥¼ ë¶€ì—¬.

**1) dictionary ì‚¬ìš©í•˜ê¸°**

```python
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
text = "A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain."

text = sent_tokenize(text)
print(text)
['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']

# ì •ì œì™€ ë‹¨ì–´ í† í°í™”
vocab = {} # íŒŒì´ì¬ì˜ dictionary ìë£Œí˜•
sentences = []
stop_words = set(stopwords.words('english'))

for i in text:
    sentence = word_tokenize(i) # ë‹¨ì–´ í† í°í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    result = []

    for word in sentence: 
        word = word.lower() # ëª¨ë“  ë‹¨ì–´ë¥¼ ì†Œë¬¸ìí™”í•˜ì—¬ ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ ì¤„ì…ë‹ˆë‹¤.
        if word not in stop_words: # ë‹¨ì–´ í† í°í™” ëœ ê²°ê³¼ì— ëŒ€í•´ì„œ ë¶ˆìš©ì–´ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
            if len(word) > 2: # ë‹¨ì–´ ê¸¸ì´ê°€ 2ì´í•˜ì¸ ê²½ìš°ì— ëŒ€í•˜ì—¬ ì¶”ê°€ë¡œ ë‹¨ì–´ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
                result.append(word)
                if word not in vocab:
                    vocab[word] = 0 
                vocab[word] += 1
    sentences.append(result) 
print(sentences)

[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]

print(vocab)
{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}

vocab_sorted = sorted(voca.items(), key = lambda x:x[1], reverse=True)

print(vocab_sorted)
[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]

#ì´ì œ ë†’ì€ ë¹ˆë„ìˆ˜ë¥¼ ê°€ì§„ ë‹¨ì–´ì¼ìˆ˜ë¡ ë‚®ì€ ì •ìˆ˜ ì¸ë±ìŠ¤ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.

word_to_index = {}
i=0
for (word, frequency) in vocab_sorted :
    if frequency > 1 : # ì •ì œ(Cleaning) ì±•í„°ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ ë¹ˆë„ìˆ˜ê°€ ì ì€ ë‹¨ì–´ëŠ” ì œì™¸í•œë‹¤.
        i=i+1
        word_to_index[word] = i
print(word_to_index)
{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}
```

ìì—°ì–´ ì²˜ë¦¬ë¥¼ í•˜ë‹¤ ë³´ë©´, ë¹ˆë„ìˆ˜ê°€ ê°€ì¥ ë†’ì€ nê°œì˜ ë‹¨ì–´ë§Œ ì‚¬ìš©í•˜ê³  ì‹¶ì€ ê²½ìš°ê°€ ìˆë‹¤.

ìƒìœ„ nê°œì˜ ë‹¨ì–´ë§Œ ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤ê³  í•˜ë©´ vocabì—ì„œ valueê°€ 1-nê¹Œì§€ì¸ ë‹¨ì–´ë“¤ë§Œ ì‚¬ìš©í•˜ë©´ ëœë‹¤.

```python
vocab_size = 5
words_frequency = [w for w,c in word_to_index.items() if c >= vocab_size + 1] # ì¸ë±ìŠ¤ê°€ 5 ì´ˆê³¼ì¸ ë‹¨ì–´ ì œê±°
for w in words_frequency:
    del word_to_index[w] # í•´ë‹¹ ë‹¨ì–´ì— ëŒ€í•œ ì¸ë±ìŠ¤ ì •ë³´ë¥¼ ì‚­ì œ
print(word_to_index)
{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}
```

word_to_indexë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ í† í°í™”ê°€ ëœ ìƒíƒœë¡œ ì €ì¥ëœ sentencesì— ìˆëŠ” ê° ë‹¨ì–´ë¥¼ ì •ìˆ˜ë¡œ ë°”ê¾¸ëŠ” ì‘ì—….

ê·¸ëŸ°ë° ë‘ ë²ˆì§¸ ë¬¸ì¥ì¸ ['barber', 'good', 'person']ì—ëŠ” ë” ì´ìƒ word_to_indexì—ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ì¸ 'good'ì´ë¼ëŠ” ë‹¨ì–´ê°€ ìˆë‹¤.

ì´ì²˜ëŸ¼ ë‹¨ì–´ ì§‘í•©ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ë“¤ì„ **Out-Of-Vocabulary(ë‹¨ì–´ ì§‘í•©ì— ì—†ëŠ” ë‹¨ì–´)** '**OOV**'. word_to_indexì— 'OOV'ë€ ë‹¨ì–´ë¥¼ ìƒˆë¡­ê²Œ ì¶”ê°€í•˜ê³ , ë‹¨ì–´ ì§‘í•©ì— ì—†ëŠ” ë‹¨ì–´ë“¤ì€ 'OOV'ì˜ ì¸ë±ìŠ¤ë¡œ ì¸ì½”ë”©.

```python
word_to_index['OOV'] = len(word_to_index) + 1

encoded = []
for s in sentences:
    temp = []
    for w in s:
        try:
            temp.append(word_to_index[w])
        except KeyError:
            temp.append(word_to_index['OOV'])
    encoded.append(temp)
print(encoded)
[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]
```

ì´ë³´ë‹¤ ì¢€ ë” ì‰½ê²Œ í•˜ê¸° ìœ„í•´ Counter, FreqDist, enumerate ë˜ëŠ” keras í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥.

**2) Counter ì‚¬ìš©í•˜ê¸°**

```python
print(sentences)
[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]

words = sum(sentences, [])
# ìœ„ ì‘ì—…ì€ words = np.hstack(sentences)ë¡œë„ ìˆ˜í–‰ ê°€ëŠ¥.
print(words)
['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']
```

Counter() ë¡œ ì¤‘ë³µì„ ì œê±°í•˜ê³  ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ë¥¼ ê¸°ë¡

```python
from collections import Counter

vocab = Counter(words) # íŒŒì´ì¬ì˜ Counter ëª¨ë“ˆì„ ì´ìš©í•˜ë©´ ë‹¨ì–´ì˜ ëª¨ë“  ë¹ˆë„ë¥¼ ì‰½ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
print(vocab)
Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})
```

```python
vocab_size = 5
vocab = vocab.most_common(vocab_size) # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ìƒìœ„ 5ê°œì˜ ë‹¨ì–´ë§Œ ì €ì¥
vocab
[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]

# ì´ì œ ë†’ì€ ë¹ˆë„ìˆ˜ë¥¼ ê°€ì§„ ë‹¨ì–´ì¼ìˆ˜ë¡ ë‚®ì€ ì •ìˆ˜ ì¸ë±ìŠ¤ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.

word_to_index = {}
i = 0
for (word, frequency) in vocab :
    i = i+1
    word_to_index[word] = i
print(word_to_index)
{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}
```

**3) NLTKì˜ FreqDist ì‚¬ìš©í•˜ê¸°**

ìœ„ì—ì„œ ì‚¬ìš©í•œ Counter()ë‘ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

```python
from nltk import FreqDist
import numpy as np
# np.hstackìœ¼ë¡œ ë¬¸ì¥ êµ¬ë¶„ì„ ì œê±°í•˜ì—¬ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš© . ex) ['barber', 'person', 'barber', 'good' ... ì¤‘ëµ ...
vocab = FreqDist(np.hstack(sentences))

vocab_size = 5
vocab = vocab.most_common(vocab_size) # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ìƒìœ„ 5ê°œì˜ ë‹¨ì–´ë§Œ ì €ì¥
vocab
[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]

# ì•ì„œ Counter()ë¥¼ ì‚¬ìš©í–ˆì„ ë•Œì™€ ê²°ê³¼ê°€ ê°™ìŠµë‹ˆë‹¤. 
# ì´ì „ ì‹¤ìŠµë“¤ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ë†’ì€ ë¹ˆë„ìˆ˜ë¥¼ ê°€ì§„ ë‹¨ì–´ì¼ìˆ˜ë¡ ë‚®ì€ ì •ìˆ˜ ì¸ë±ìŠ¤ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤. 
# ê·¸ëŸ°ë° ì´ë²ˆì—ëŠ” enumerate()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¢€ ë” ì§§ì€ ì½”ë“œë¡œ ì¸ë±ìŠ¤ë¥¼ ë¶€ì—¬í•˜ê² ìŠµë‹ˆë‹¤.

word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}
print(word_to_index)
{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}
```

**4) enumerate ì´í•´í•˜ê¸°**

```python
test=['a', 'b', 'c', 'd', 'e']
for index, value in enumerate(test): # ì…ë ¥ì˜ ìˆœì„œëŒ€ë¡œ 0ë¶€í„° ì¸ë±ìŠ¤ë¥¼ ë¶€ì—¬í•¨.
  print("value : {}, index: {}".format(value, index))

value : a, index: 0
value : b, index: 1
value : c, index: 2
value : d, index: 3
value : e, index: 4
```

### 2. ì¼€ë¼ìŠ¤ (Keras)ì˜ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬

ë•Œë¡œëŠ” ì •ìˆ˜ ì¸ì½”ë”©ì„ ìœ„í•´ì„œ Kerasì˜ ì „ì²˜ë¦¬ ë„êµ¬ì¸ **Tokenizer** ë¥¼ ì‚¬ìš©í•˜ê¸°ë„ í•œë‹¤.

```python
sentences=[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 
'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], 
['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], 
['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 
'went', 'huge', 'mountain']]

from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts(sentences) 
# fit_on_texts()ì•ˆì— ì½”í¼ìŠ¤ë¥¼ ì…ë ¥ìœ¼ë¡œ í•˜ë©´ ë¹ˆë„ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ ì§‘í•©ì„ ìƒì„±í•œë‹¤.

print(tokenizer.word_index)
{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}
```

ê° ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì¸ë±ìŠ¤ê°€ ë¶€ì—¬ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

```python
# ê° ë‹¨ì–´ê°€ ì¹´ìš´íŠ¸ë¥¼ ìˆ˜í–‰í•˜ì˜€ì„ ë•Œ ëª‡ ê°œì˜€ëŠ”ì§€ë¥¼ ë³´ê³ ì í•œë‹¤ë©´ word_countsë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
print(tokenizer.word_counts)
OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])
```

**texts_to_sequences()** ëŠ” ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ì˜¨ ì½”í¼ìŠ¤ì— ëŒ€í•´ ê° ë‹¨ì–´ë¥¼ ì´ë¯¸ ì •í•´ì§„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜

```python
print(tokenizer.texts_to_sequences(sentences))
[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]
```

**tokenizer = Tokenizer(num_words=ìˆ«ì)** ìœ¼ë¡œ ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ìƒìœ„ ëª‡ ê°œì˜ ë‹¨ì–´ë§Œ ì‚¬ìš©í•˜ê² ë‹¤ê³  ì§€ì •í•  ìˆ˜ ìˆë‹¤.

```python
vocab_size = 5
tokenizer = Tokenizer(num_words = vocab_size + 1) # ìƒìœ„ 5ê°œ ë‹¨ì–´ë§Œ ì‚¬ìš©
tokenizer.fit_on_texts(sentences)

```

num_wordsì—ì„œ +1ì„ ë”í•´ì„œ ê°’ì„ ë„£ì–´ì£¼ëŠ” ì´ìœ !
num_wordsëŠ” ìˆ«ìë¥¼ 0ë¶€í„° ì¹´ìš´íŠ¸. 
ë§Œì•½ 5ë¥¼ ë„£ìœ¼ë©´ 0 ~ 4ë²ˆ ë‹¨ì–´ ë³´ì¡´ì„ ì˜ë¯¸! â†’ 1ë²ˆ ë‹¨ì–´ë¶€í„° 4ë²ˆ ë‹¨ì–´ë§Œ ë‚¨ê²Œë¨
ê·¸ë ‡ê¸° ë•Œë¬¸ì— 1 ~ 5ë²ˆ ë‹¨ì–´ê¹Œì§€ ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤ë©´ num_wordsì— 5+1ì¸ ê°’ì„ ë„£ì–´ì¤˜ì•¼ í•œë‹¤.

ì‹¤ì§ˆì ìœ¼ë¡œ ìˆ«ì 0ì— ì§€ì •ëœ ë‹¨ì–´ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë°ë„ ì¼€ë¼ìŠ¤ í† í¬ë‚˜ì´ì €ê°€ ìˆ«ì 0ê¹Œì§€ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°ë¡œ ì‚°ì •í•˜ëŠ” ì´ìœ ëŠ” ìì—°ì–´ ì²˜ë¦¬ì—ì„œ **íŒ¨ë”©(padding)**ì´ë¼ëŠ” ì‘ì—… ë•Œë¬¸

```python
print(tokenizer.texts_to_sequences(sentences))
[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]
```

1ë²ˆ ë‹¨ì–´ë¶€í„° 5ë²ˆ ë‹¨ì–´ê¹Œì§€ë§Œ ë³´ì¡´ë˜ê³  ë‚˜ë¨¸ì§€ ë‹¨ì–´ë“¤ì€ ì œê±°ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

Keras Tokenizer ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë‹¨ì–´ ì§‘í•©ì— ì—†ëŠ” ë‹¨ì–´ì¸ OOVì— ëŒ€í•´ì„œëŠ” ë‹¨ì–´ë¥¼ ì •ìˆ˜ë¡œ ë°”ê¾¸ëŠ” ê³¼ì •ì—ì„œ ì•„ì˜ˆ ë‹¨ì–´ë¥¼ ì œê±°í•œë‹¤ëŠ” íŠ¹ì§•ì´ ìˆë‹¤. 

ë‹¨ì–´ ì§‘í•©ì— ì—†ëŠ” ë‹¨ì–´ë“¤ì€ OOVë¡œ ê°„ì£¼í•˜ì—¬ ë³´ì¡´í•˜ê³  ì‹¶ë‹¤ë©´ Tokenizerì˜ ì¸ì **oov_token**ì„ ì‚¬ìš©

```python
vocab_size = 5
tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')
# ë¹ˆë„ìˆ˜ ìƒìœ„ 5ê°œ ë‹¨ì–´ë§Œ ì‚¬ìš©. ìˆ«ì 0ê³¼ OOVë¥¼ ê³ ë ¤í•´ì„œ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°ëŠ” +2
tokenizer.fit_on_texts(sentences)

print('ë‹¨ì–´ OOVì˜ ì¸ë±ìŠ¤ : {}'.format(tokenizer.word_index['OOV']))
ë‹¨ì–´ OOVì˜ ì¸ë±ìŠ¤ : 1

print(tokenizer.texts_to_sequences(sentences))
[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]
# ê·¸ ì™¸ ë‹¨ì–´ ì§‘í•©ì— ì—†ëŠ” 'good'ê³¼ ê°™ì€ ë‹¨ì–´ë“¤ì€ ì „ë¶€ 'OOV'ì˜ ì¸ë±ìŠ¤ì¸ 1ë¡œ ì¸ì½”ë”©ë˜ì—ˆë‹¤.

```

---

## íŒ¨ë”© (Padding)

ê° ë¬¸ì¥(ë˜ëŠ” ë¬¸ì„œ)ì˜ ê¸¸ì´ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤. 

ë³‘ë ¬ ì—°ì‚°ì„ ìœ„í•´ ì—¬ëŸ¬ ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ ì„ì˜ë¡œ ë™ì¼í•˜ê²Œ ë§ì¶°ì£¼ëŠ” ì‘ì—…

### 1. Numpy ë¡œ íŒ¨ë”©í•˜ê¸°

```python
sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts(sentences) # fit_on_texts()ì•ˆì— ì½”í¼ìŠ¤ë¥¼ ì…ë ¥ìœ¼ë¡œ í•˜ë©´ ë¹ˆë„ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ ì§‘í•©ì„ ìƒì„±í•œë‹¤.

encoded = tokenizer.texts_to_sequences(sentences)
print(encoded)
[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]
```

ì´ì œ ëª¨ë‘ ë™ì¼í•œ ê¸¸ì´ë¡œ ë§ì¶°ì£¼ê¸° ìœ„í•´ ì´ ì¤‘ì—ì„œ ê°€ì¥ ê¸¸ì´ê°€ ê¸´ ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ ê³„ì‚°

```python
max_len = max(len(item) for item in encoded)
print(max_len)
7
```

ì´ì œ ëª¨ë“  ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ 7ë¡œ!

ì´ë•Œ ê°€ìƒì˜ ë‹¨ì–´ 'PAD'ë¥¼ ì‚¬ìš©

'PAD'ë¼ëŠ” ë‹¨ì–´ê°€ ìˆë‹¤ê³  ê°€ì •í•˜ê³ , ì´ ë‹¨ì–´ëŠ” 0ë²ˆ ë‹¨ì–´ë¼ê³  ì •ì˜

ê¸¸ì´ê°€ 7ë³´ë‹¤ ì§§ì€ ë¬¸ì¥ì—ëŠ” ìˆ«ì 0ì„ ì±„ì›Œì„œ ì „ë¶€ ê¸¸ì´ 7ë¡œ!

```python
for item in encoded: # ê° ë¬¸ì¥ì— ëŒ€í•´ì„œ
    while len(item) < max_len:   # max_lenë³´ë‹¤ ì‘ìœ¼ë©´
        item.append(0)

padded_np = np.array(encoded)
padded_np

array([[ 1,  5,  0,  0,  0,  0,  0],
       [ 1,  8,  5,  0,  0,  0,  0],
       [ 1,  3,  5,  0,  0,  0,  0],
       [ 9,  2,  0,  0,  0,  0,  0],
       [ 2,  4,  3,  2,  0,  0,  0],
       [ 3,  2,  0,  0,  0,  0,  0],
       [ 1,  4,  6,  0,  0,  0,  0],
       [ 1,  4,  6,  0,  0,  0,  0],
       [ 1,  4,  2,  0,  0,  0,  0],
       [ 7,  7,  3,  2, 10,  1, 11],
       [ 1, 12,  3, 13,  0,  0,  0]])
```

**íŒ¨ë”© (Padding) ?** 
ë°ì´í„°ì— íŠ¹ì • ê°’ì„ ì±„ì›Œì„œ ë°ì´í„°ì˜ í¬ê¸°(shape)ë¥¼ ì¡°ì •í•˜ëŠ” ì‘ì—…

ìˆ«ì 0ì„ ì‚¬ìš©í•˜ê³  ìˆë‹¤ë©´ **ì œë¡œ íŒ¨ë”©(zero padding)**

### 2. ì¼€ë¼ìŠ¤ ì „ì²˜ë¦¬ ë„êµ¬ë¡œ íŒ¨ë”©

Keras ì—ì„œ íŒ¨ë”©ì„ ìœ„í•œ ë„êµ¬ **pad_sequences()** ì œê³µ

```python
encoded = tokenizer.texts_to_sequences(sentences)
print(encoded)

[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]

padded = pad_sequences(encoded)
padded
array([[ 0,  0,  0,  0,  0,  1,  5],
       [ 0,  0,  0,  0,  1,  8,  5],
       [ 0,  0,  0,  0,  1,  3,  5],
       [ 0,  0,  0,  0,  0,  9,  2],
       [ 0,  0,  0,  2,  4,  3,  2],
       [ 0,  0,  0,  0,  0,  3,  2],
       [ 0,  0,  0,  0,  1,  4,  6],
       [ 0,  0,  0,  0,  1,  4,  6],
       [ 0,  0,  0,  0,  1,  4,  2],
       [ 7,  7,  3,  2, 10,  1, 11],
       [ 0,  0,  0,  1, 12,  3, 13]], dtype=int32)
```

ë’¤ì— 0ì„ ì±„ìš°ê³  ì‹¶ë‹¤ë©´ ì¸ìë¡œ **padding='post'**.

```python
padded = pad_sequences(encoded, padding = 'post')
padded
array([[ 1,  5,  0,  0,  0,  0,  0],
       [ 1,  8,  5,  0,  0,  0,  0],
       [ 1,  3,  5,  0,  0,  0,  0],
       [ 9,  2,  0,  0,  0,  0,  0],
       [ 2,  4,  3,  2,  0,  0,  0],
       [ 3,  2,  0,  0,  0,  0,  0],
       [ 1,  4,  6,  0,  0,  0,  0],
       [ 1,  4,  6,  0,  0,  0,  0],
       [ 1,  4,  2,  0,  0,  0,  0],
       [ 7,  7,  3,  2, 10,  1, 11],
       [ 1, 12,  3, 13,  0,  0,  0]], dtype=int32)
```

**max_len**ì˜ ì¸ìë¡œ ì •ìˆ˜ë¥¼ ì£¼ë©´, í•´ë‹¹ ì •ìˆ˜ë¡œ ëª¨ë“  ë¬¸ì„œì˜ ê¸¸ì´ë¥¼ ë™ì¼í•˜ê²Œ í•œë‹¤.

```python
padded = pad_sequences(encoded, padding = 'post', maxlen = 5)
padded
array([[ 1,  5,  0,  0,  0],
       [ 1,  8,  5,  0,  0],
       [ 1,  3,  5,  0,  0],
       [ 9,  2,  0,  0,  0],
       [ 2,  4,  3,  2,  0],
       [ 3,  2,  0,  0,  0],
       [ 1,  4,  6,  0,  0],
       [ 1,  4,  6,  0,  0],
       [ 1,  4,  2,  0,  0],
       [ 3,  2, 10,  1, 11],
       [ 1, 12,  3, 13,  0]], dtype=int32)
```

ë§Œì•½, ìˆ«ì 0ì´ ì•„ë‹ˆë¼ ë‹¤ë¥¸ ìˆ«ìë¥¼ íŒ¨ë”©ì„ ìœ„í•œ ìˆ«ìë¡œ ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤ë©´ ì´ ë˜í•œ ê°€ëŠ¥!
pad_sequencesì˜ ì¸ìë¡œ **value**ë¥¼ ì‚¬ìš©í•˜ë©´ 0ì´ ì•„ë‹Œ ë‹¤ë¥¸ ìˆ«ìë¡œ íŒ¨ë”©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

```python
# í˜„ì¬ ì‚¬ìš©ëœ ì •ìˆ˜ë“¤ê³¼ ê²¹ì¹˜ì§€ ì•Šë„ë¡, ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°ì— +1ì„ í•œ ìˆ«ìë¡œ ì‚¬ìš©
last_value = len(tokenizer.word_index) + 1 # ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°ë³´ë‹¤ 1 í° ìˆ«ìë¥¼ ì‚¬ìš©
print(last_value)
14

padded = pad_sequences(encoded, padding = 'post', value = last_value)
padded
array([[ 1,  5, 14, 14, 14, 14, 14],
       [ 1,  8,  5, 14, 14, 14, 14],
       [ 1,  3,  5, 14, 14, 14, 14],
       [ 9,  2, 14, 14, 14, 14, 14],
       [ 2,  4,  3,  2, 14, 14, 14],
       [ 3,  2, 14, 14, 14, 14, 14],
       [ 1,  4,  6, 14, 14, 14, 14],
       [ 1,  4,  6, 14, 14, 14, 14],
       [ 1,  4,  2, 14, 14, 14, 14],
       [ 7,  7,  3,  2, 10,  1, 11],
       [ 1, 12,  3, 13, 14, 14, 14]], dtype=int32)

```

---

## ì› - í•« ì¸ì½”ë”© (One-Hot Encoding)

ì»´í“¨í„°ëŠ” ë¬¸ìë³´ë‹¤ëŠ” ìˆ«ìë¥¼ ë” ì˜ ì²˜ë¦¬.

ìì—°ì–´ ì²˜ë¦¬ì—ì„œëŠ” ë¬¸ìë¥¼ ìˆ«ìë¡œ ë°”ê¾¸ëŠ” ì—¬ëŸ¬ê°€ì§€ ê¸°ë²•ë“¤ì´ ìˆë‹¤.

One-Hot Encoding ì€ ê·¸ ë§ì€ ê¸°ë²• ì¤‘ì—ì„œ ê°€ì¥ ê¸°ë³¸ì ì¸ í‘œí˜„ ë°©ë²•.

One - Hot Encoding ì„ ìœ„í•´ì„œ ë¨¼ì € í•´ì•¼ í•  ì¼ ! â†’ ë‹¨ì–´ ì§‘í•©ì„ ë§Œë“œëŠ” ì¼

**ë‹¨ì–´ ì§‘í•©(vocabulary) :** ì„œë¡œ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì˜ ì§‘í•©

book , books  â† ë‹¤ë¥¸ ë‹¨ì–´

ë‹¨ì–´ ì§‘í•©ì˜ ë‹¨ì–´ë“¤ë¡œ ë¬¸ìë¥¼ ìˆ«ì(ë” êµ¬ì²´ì ìœ¼ë¡œëŠ” **ë²¡í„°**)ë¡œ ë°”ê¾¼ë‹¤.

### 1. ì› - í•« ì¸ì½”ë”© (One - Hot Encoding) ì´ë€?

 

ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°ë¥¼ ë²¡í„°ì˜ ì°¨ì›ìœ¼ë¡œ, 

í‘œí˜„í•˜ê³  ì‹¶ì€ ë‹¨ì–´ì˜ ì¸ë±ìŠ¤ì— 1ì˜ ê°’ì„ ë¶€ì—¬í•˜ê³ , 

ë‹¤ë¥¸ ì¸ë±ìŠ¤ì—ëŠ” 0ì„ ë¶€ì—¬í•˜ëŠ” ë‹¨ì–´ì˜ ë²¡í„° í‘œí˜„ ë°©ì‹. 

```python
from konlpy.tag import Okt  
okt=Okt()  
token=okt.morphs("ë‚˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ë¥¼ ë°°ìš´ë‹¤")  
print(token)
['ë‚˜', 'ëŠ”', 'ìì—°ì–´', 'ì²˜ë¦¬', 'ë¥¼', 'ë°°ìš´ë‹¤']

word2index={}
for voca in token:
     if voca not in word2index.keys():
       word2index[voca]=len(word2index)
print(word2index)
{'ë‚˜': 0, 'ëŠ”': 1, 'ìì—°ì–´': 2, 'ì²˜ë¦¬': 3, 'ë¥¼': 4, 'ë°°ìš´ë‹¤': 5}

def one_hot_encoding(word, word2index):
       one_hot_vector = [0]*(len(word2index))
       index=word2index[word]
       one_hot_vector[index]=1
       return one_hot_vector

one_hot_encoding("ìì—°ì–´",word2index)
[0, 0, 1, 0, 0, 0]
```

### 2. ì¼€ë¼ìŠ¤ (Keras)ë¥¼ ì´ìš©í•œ ì›-í•« ì¸ì½”ë”© (One-Hot-Encoding)

ì¼€ë¼ìŠ¤ëŠ” ì›-í•« ì¸ì½”ë”©ì„ ìˆ˜í–‰í•˜ëŠ” ìœ ìš©í•œ ë„êµ¬ **to_categorical()**ë¥¼ ì§€ì›

```python
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical

text="ë‚˜ë‘ ì ì‹¬ ë¨¹ìœ¼ëŸ¬ ê°ˆë˜ ì ì‹¬ ë©”ë‰´ëŠ” í–„ë²„ê±° ê°ˆë˜ ê°ˆë˜ í–„ë²„ê±° ìµœê³ ì•¼"

t = Tokenizer()
t.fit_on_texts([text])
print(t.word_index) # ê° ë‹¨ì–´ì— ëŒ€í•œ ì¸ì½”ë”© ê²°ê³¼ ì¶œë ¥.
{'ê°ˆë˜': 1, 'ì ì‹¬': 2, 'í–„ë²„ê±°': 3, 'ë‚˜ë‘': 4, 'ë¨¹ìœ¼ëŸ¬': 5, 'ë©”ë‰´ëŠ”': 6, 'ìµœê³ ì•¼': 7}

sub_text="ì ì‹¬ ë¨¹ìœ¼ëŸ¬ ê°ˆë˜ ë©”ë‰´ëŠ” í–„ë²„ê±° ìµœê³ ì•¼"
encoded=t.texts_to_sequences([sub_text])[0]
print(encoded)
[2, 5, 1, 6, 3, 7]

one_hot = to_categorical(encoded)
print(one_hot)
[[0. 0. 1. 0. 0. 0. 0. 0.] #ì¸ë±ìŠ¤ 2ì˜ ì›-í•« ë²¡í„°
 [0. 0. 0. 0. 0. 1. 0. 0.] #ì¸ë±ìŠ¤ 5ì˜ ì›-í•« ë²¡í„°
 [0. 1. 0. 0. 0. 0. 0. 0.] #ì¸ë±ìŠ¤ 1ì˜ ì›-í•« ë²¡í„°
 [0. 0. 0. 0. 0. 0. 1. 0.] #ì¸ë±ìŠ¤ 6ì˜ ì›-í•« ë²¡í„°
 [0. 0. 0. 1. 0. 0. 0. 0.] #ì¸ë±ìŠ¤ 3ì˜ ì›-í•« ë²¡í„°
 [0. 0. 0. 0. 0. 0. 0. 1.]] #ì¸ë±ìŠ¤ 7ì˜ ì›-í•« ë²¡í„°
```

### 3. ì› - í•« ì¸ì½”ë”©(One-Hot Encoding)ì˜ í•œê³„

ì› í•« ë²¡í„°ëŠ” **ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°**ê°€ ê³§ **ë²¡í„°ì˜ ì°¨ì› ìˆ˜**ê°€ ëœë‹¤.

ex) 1,000ê°œì¸ ì½”í¼ìŠ¤ë¥¼ ê°€ì§€ê³  ì› í•« ë²¡í„°ë¥¼ ë§Œë“¤ë©´, ëª¨ë“  ë‹¨ì–´ ê°ê°ì€ ëª¨ë‘ 1,000ê°œì˜ ì°¨ì›ì„ ê°€ì§„ ë²¡í„°ê°€ ëœë‹¤.

ì›-í•« ë²¡í„°ëŠ” ë‹¨ì–´ì˜ **ìœ ì‚¬ë„ë¥¼ í‘œí˜„í•˜ì§€ ëª»í•œë‹¤**ëŠ” ë‹¨ì ì´ ìˆë‹¤.

ex) ëŠ‘ëŒ€, í˜¸ë‘ì´, ê°•ì•„ì§€, ê³ ì–‘ì´ë¼ëŠ” 4ê°œì˜ ë‹¨ì–´ì— ëŒ€í•´ì„œ ì›-í•« ì¸ì½”ë”©ì„ í•´ì„œ ê°ê°, [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]. ì´ ë•Œ ê°•ì•„ì§€ì™€ ëŠ‘ëŒ€ê°€ ìœ ì‚¬í•˜ê³ , í˜¸ë‘ì´ì™€ ê³ ì–‘ì´ê°€ ìœ ì‚¬í•˜ë‹¤ëŠ” ê²ƒì„ í‘œí˜„í•  ìˆ˜ ì—†ë‹¤.

ë‹¨ì–´ ê°„ ìœ ì‚¬ì„±ì„ ì•Œ ìˆ˜ ì—†ë‹¤ëŠ” ë‹¨ì ì€ **ê²€ìƒ‰ ì‹œìŠ¤í…œ** ë“±ì—ì„œ ì‹¬ê°í•œ ë¬¸ì œ.

ë‹¨ì ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¨ì–´ì˜ ì ì¬ ì˜ë¯¸ë¥¼ ë°˜ì˜í•˜ì—¬ ë‹¤ì°¨ì› ê³µê°„ì— ë²¡í„°í™” í•˜ëŠ” ê¸°ë²•ìœ¼ë¡œ í¬ê²Œ ë‘ ê°€ì§€ê°€ ìˆë‹¤.

ì¹´ìš´íŠ¸ ê¸°ë°˜ì˜ ë²¡í„°í™” ë°©ë²•:      LSA, HAL ë“±

ì˜ˆì¸¡ ê¸°ë°˜ìœ¼ë¡œ ë²¡í„°í™” ë°©ë²•:      NNLM, RNNLM, Word2Vec, FastText ë“±

ì¹´ìš´íŠ¸ ê¸°ë°˜ê³¼ ì˜ˆì¸¡ ê¸°ë°˜ ë‘ ê°€ì§€ ë°©ë²•ì„ ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ë°©ë²•:    GloVe

---

## ë°ì´í„°ì˜ ë¶„ë¦¬ (Splitting Data)

ì§€ë„ í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ë¶„ë¦¬ ì‘ì—…

### 1. ì§€ë„ í•™ìŠµ (Supervised Learning)

ì§€ë„ í•™ìŠµì˜ í›ˆë ¨ ë°ì´í„°ëŠ” 

ì •ë‹µì´ ë¬´ì—‡ì¸ì§€ ë§ì¶°ì•¼ í•˜ëŠ” '**ë¬¸ì œ**'ì— í•´ë‹¹ë˜ëŠ” ë°ì´í„°ì™€ 

ë ˆì´ë¸”ì´ë¼ê³  ë¶€ë¥´ëŠ” '**ì •ë‹µ**'ì´ ì í˜€ìˆëŠ” ë°ì´í„°ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.

ê¸°ê³„ëŠ” ì •ë‹µì´ ì í˜€ì ¸ ìˆëŠ” ë¬¸ì œì§€ë¥¼ ë¬¸ì œì™€ ì •ë‹µì„ í•¨ê»˜ ë³´ë©´ì„œ ì—´ì‹¬íˆ ê³µë¶€í•˜ê³ , í–¥í›„ì— ì •ë‹µì´ ì—†ëŠ” ë¬¸ì œì— ëŒ€í•´ì„œë„ ì •ë‹µì„ ì˜ ì˜ˆì¸¡í•´ì•¼ í•œë‹¤.

**<í›ˆë ¨ ë°ì´í„°>**

X_train : ë¬¸ì œì§€ ë°ì´í„°

y_train : ë¬¸ì œì§€ì— ëŒ€í•œ ì •ë‹µ ë°ì´í„°

**<í…ŒìŠ¤íŠ¸ ë°ì´í„°>**

X_test : ì‹œí—˜ì§€ ë°ì´í„°

y_test : ì‹œí—˜ì§€ì— ëŒ€í•œ ì •ë‹µ ë°ì´í„°

### 2. Xì™€ y ë¶„ë¦¬í•˜ê¸°

**1) zip í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ë¶„ë¦¬í•˜ê¸°**

zip()    :    ë™ì¼í•œ ê°œìˆ˜ë¥¼ ê°€ì§€ëŠ” ì‹œí€€ìŠ¤ ìë£Œí˜•ì—ì„œ ê° ìˆœì„œì— ë“±ì¥í•˜ëŠ”

               ì›ì†Œë“¤ë¼ë¦¬ ë¬¶ì–´ì£¼ëŠ” ì—­í• 

```python
X,y = zip(['a', 1], ['b', 2], ['c', 3])
print(X)
print(y)
('a', 'b', 'c')
(1, 2, 3)

sequences=[['a', 1], ['b', 2], ['c', 3]] # ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” í–‰ë ¬ ë˜ëŠ” ë’¤ì—ì„œ ë°°ìš¸ ê°œë…ì¸ 2D í…ì„œ.
X,y = zip(*sequences) # *ë¥¼ ì¶”ê°€
print(X)
print(y)
('a', 'b', 'c')
(1, 2, 3)
```

**2) ë°ì´í„° í”„ë ˆì„ì„ ì´ìš©í•˜ì—¬ ë¶„ë¦¬í•˜ê¸°**

```python
import pandas as pd

values = [['ë‹¹ì‹ ì—ê²Œ ë“œë¦¬ëŠ” ë§ˆì§€ë§‰ í˜œíƒ!', 1],
['ë‚´ì¼ ëµ ìˆ˜ ìˆì„ì§€ í™•ì¸ ë¶€íƒë“œ...', 0],
['ë„ì—°ì”¨. ì˜ ì§€ë‚´ì‹œì£ ? ì˜¤ëœë§Œì…...', 0],
['(ê´‘ê³ ) AIë¡œ ì£¼ê°€ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤!', 1]]
columns = ['ë©”ì¼ ë³¸ë¬¸', 'ìŠ¤íŒ¸ ë©”ì¼ ìœ ë¬´']

df = pd.DataFrame(values, columns=columns)
df

```

<img src = "/image/Untitled 2.png" width = "600px">

```python
X=df['ë©”ì¼ ë³¸ë¬¸']
y=df['ìŠ¤íŒ¸ ë©”ì¼ ìœ ë¬´']

print(X)
0          ë‹¹ì‹ ì—ê²Œ ë“œë¦¬ëŠ” ë§ˆì§€ë§‰ í˜œíƒ!
1      ë‚´ì¼ ëµ ìˆ˜ ìˆì„ì§€ í™•ì¸ ë¶€íƒë“œ...
2      ë„ì—°ì”¨. ì˜ ì§€ë‚´ì‹œì£ ? ì˜¤ëœë§Œì…...
3    (ê´‘ê³ ) AIë¡œ ì£¼ê°€ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤!
Name: ë©”ì¼ ë³¸ë¬¸, dtype: object

print(y)
0    1
1    0
2    0
3    1
Name: ìŠ¤íŒ¸ ë©”ì¼ ìœ ë¬´, dtype: int64

```

**3) Numpyë¥¼ ì´ìš©í•˜ì—¬ ë¶„ë¦¬í•˜ê¸°**

```python
import numpy as np
ar = np.arange(0,16).reshape((4,4))
print(ar)
[[ 0  1  2  3]
 [ 4  5  6  7]
 [ 8  9 10 11]
 [12 13 14 15]]
X=ar[:, :3]
print(X)
[[ 0  1  2]
 [ 4  5  6]
 [ 8  9 10]
 [12 13 14]]
y=ar[:,3]
print(y)
[ 3  7 11 15]
```

### 3. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬í•˜ê¸°

Xì™€ yê°€ ë¶„ë¦¬ëœ ë°ì´í„°ì— ëŒ€í•´ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ëŠ” ê³¼ì •

**1) ì‚¬ì´í‚· ëŸ°ì„ ì´ìš©í•˜ì—¬ ë¶„ë¦¬í•˜ê¸°**

```python
import numpy as np
from sklearn.model_selection import train_test_split
X, y = np.arange(10).reshape((5, 2)), range(5)
# ì‹¤ìŠµì„ ìœ„í•´ ì„ì˜ë¡œ Xì™€ yê°€ ì´ë¯¸ ë¶„ë¦¬ ëœ ë°ì´í„°ë¥¼ ìƒì„±
print(X)
print(list(y)) #ë ˆì´ë¸” ë°ì´í„°
[[0 1]
 [2 3]
 [4 5]
 [6 7]
 [8 9]]
[0, 1, 2, 3, 4]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1234)
#3ë¶„ì˜ 1ë§Œ test ë°ì´í„°ë¡œ ì§€ì •.
#random_state ì§€ì •ìœ¼ë¡œ ì¸í•´ ìˆœì„œê°€ ì„ì¸ ì±„ë¡œ í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ë‚˜ëˆ ì§„ë‹¤.
print(X_train)
print(X_test)
[[2 3]
 [4 5]
 [6 7]]
[[8 9]
 [0 1]]
print(y_train)
print(y_test)
[1, 2, 3]
[4, 0]
```

**2) ìˆ˜ë™ìœ¼ë¡œ ë¶„ë¦¬í•˜ê¸°**

```python
import numpy as np
X, y = np.arange(0,24).reshape((12,2)), range(12)
# ì‹¤ìŠµì„ ìœ„í•´ ì„ì˜ë¡œ Xì™€ yê°€ ì´ë¯¸ ë¶„ë¦¬ ëœ ë°ì´í„°ë¥¼ ìƒì„±
print(X)
[[ 0  1]
 [ 2  3]
 [ 4  5]
 [ 6  7]
 [ 8  9]
 [10 11]
 [12 13]
 [14 15]
 [16 17]
 [18 19]
 [20 21]
 [22 23]]
print(list(y))
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]

n_of_train = int(len(X) * 0.8) # ë°ì´í„°ì˜ ì „ì²´ ê¸¸ì´ì˜ 80%ì— í•´ë‹¹í•˜ëŠ” ê¸¸ì´ê°’ì„ êµ¬í•œë‹¤.
n_of_test = int(len(X) - n_of_train) # ì „ì²´ ê¸¸ì´ì—ì„œ 80%ì— í•´ë‹¹í•˜ëŠ” ê¸¸ì´ë¥¼ ëº€ë‹¤.
print(n_of_train)
print(n_of_test)
9
3

X_test = X[n_of_train:] #ì „ì²´ ë°ì´í„° ì¤‘ì—ì„œ 20%ë§Œí¼ ë’¤ì˜ ë°ì´í„° ì €ì¥
y_test = y[n_of_train:] #ì „ì²´ ë°ì´í„° ì¤‘ì—ì„œ 20%ë§Œí¼ ë’¤ì˜ ë°ì´í„° ì €ì¥
X_train = X[:n_of_train] #ì „ì²´ ë°ì´í„° ì¤‘ì—ì„œ 80%ë§Œí¼ ì•ì˜ ë°ì´í„° ì €ì¥
y_train = y[:n_of_train] #ì „ì²´ ë°ì´í„° ì¤‘ì—ì„œ 80%ë§Œí¼ ì•ì˜ ë°ì´í„° ì €ì¥

print(X_test)
print(list(y_test))

[[18 19]
 [20 21]
 [22 23]]
[9, 10, 11]
```

---

## í•œêµ­ì–´ ì „ì²˜ë¦¬ íŒ¨í‚¤ì§€ (Text Preprocessing Tools for Korean Text)

í˜•íƒœì†Œì™€ ë¬¸ì¥ í† í¬ë‚˜ì´ì§• ë„êµ¬ë“¤ì¸ KoNLPyì™€ KSS(Korean Sentence Splitter)ì™€ í•¨ê»˜ ìœ ìš©í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í•œêµ­ì–´ ì „ì²˜ë¦¬ íŒ¨í‚¤ì§€ë“¤

### 1. PyKoSpacing

PyKoSpacingì€ í•œêµ­ì–´ ë„ì–´ì“°ê¸° íŒ¨í‚¤ì§€ë¡œ ë„ì–´ì“°ê¸°ê°€ ë˜ì–´ìˆì§€ ì•Šì€ ë¬¸ì¥ì„ ë„ì–´ì“°ê¸°ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” íŒ¨í‚¤ì§€

```python
!pip install git+https://github.com/haven-jeon/PyKoSpacing.git

sent = 'ê¹€ì² ìˆ˜ëŠ” ê·¹ì¤‘ ë‘ ì¸ê²©ì˜ ì‚¬ë‚˜ì´ ì´ê´‘ìˆ˜ ì—­ì„ ë§¡ì•˜ë‹¤. ì² ìˆ˜ëŠ” í•œêµ­ ìœ ì¼ì˜ íƒœê¶Œë„ ì „ìŠ¹ìë¥¼ ê°€ë¦¬ëŠ” ê²°ì „ì˜ ë‚ ì„ ì•ë‘ê³  10ë…„ê°„ í•¨ê»˜ í›ˆë ¨í•œ ì‚¬í˜•ì¸ ìœ ì—°ì¬(ê¹€ê´‘ìˆ˜ ë¶„)ë¥¼ ì°¾ìœ¼ëŸ¬ ì†ì„¸ë¡œ ë‚´ë ¤ì˜¨ ì¸ë¬¼ì´ë‹¤.'

new_sent = sent.replace(" ", '') # ë„ì–´ì“°ê¸°ê°€ ì—†ëŠ” ë¬¸ì¥ ì„ì˜ë¡œ ë§Œë“¤ê¸°
print(new_sent)
ê¹€ì² ìˆ˜ëŠ”ê·¹ì¤‘ë‘ì¸ê²©ì˜ì‚¬ë‚˜ì´ì´ê´‘ìˆ˜ì—­ì„ë§¡ì•˜ë‹¤.ì² ìˆ˜ëŠ”í•œêµ­ìœ ì¼ì˜íƒœê¶Œë„ì „ìŠ¹ìë¥¼ê°€ë¦¬ëŠ”ê²°ì „ì˜ë‚ ì„ì•ë‘ê³ 10ë…„ê°„í•¨ê»˜í›ˆë ¨í•œì‚¬í˜•ì¸ìœ ì—°ì¬(ê¹€ê´‘ìˆ˜ë¶„)ë¥¼ì°¾ìœ¼ëŸ¬ì†ì„¸ë¡œë‚´ë ¤ì˜¨ì¸ë¬¼ì´ë‹¤.

from pykospacing import spacing

kospacing_sent = spacing(new_sent)
print(sent)
print(kospacing_sent)
ê¹€ì² ìˆ˜ëŠ” ê·¹ì¤‘ ë‘ ì¸ê²©ì˜ ì‚¬ë‚˜ì´ ì´ê´‘ìˆ˜ ì—­ì„ ë§¡ì•˜ë‹¤. ì² ìˆ˜ëŠ” í•œêµ­ ìœ ì¼ì˜ íƒœê¶Œë„ ì „ìŠ¹ìë¥¼ ê°€ë¦¬ëŠ” ê²°ì „ì˜ ë‚ ì„ ì•ë‘ê³  10ë…„ê°„ í•¨ê»˜ í›ˆë ¨í•œ ì‚¬í˜•ì¸ ìœ ì—°ì¬(ê¹€ê´‘ìˆ˜ ë¶„)ë¥¼ ì°¾ìœ¼ëŸ¬ ì†ì„¸ë¡œ ë‚´ë ¤ì˜¨ ì¸ë¬¼ì´ë‹¤.
ê¹€ì² ìˆ˜ëŠ” ê·¹ì¤‘ ë‘ ì¸ê²©ì˜ ì‚¬ë‚˜ì´ ì´ê´‘ìˆ˜ ì—­ì„ ë§¡ì•˜ë‹¤. ì² ìˆ˜ëŠ” í•œêµ­ ìœ ì¼ì˜ íƒœê¶Œë„ ì „ìŠ¹ìë¥¼ ê°€ë¦¬ëŠ” ê²°ì „ì˜ ë‚ ì„ ì•ë‘ê³  10ë…„ê°„ í•¨ê»˜ í›ˆë ¨í•œ ì‚¬í˜•ì¸ ìœ ì—°ì¬(ê¹€ê´‘ìˆ˜ ë¶„)ë¥¼ ì°¾ìœ¼ëŸ¬ ì†ì„¸ë¡œ ë‚´ë ¤ì˜¨ ì¸ë¬¼ì´ë‹¤.

```

### 2. Py - Hanspell

ë„¤ì´ë²„ í•œê¸€ ë§ì¶¤ë²• ê²€ì‚¬ê¸°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ íŒ¨í‚¤ì§€

```python
!pip install git+https://github.com/ssut/py-hanspell.git

from hanspell import spell_checker

sent = "ë§ì¶¤ë²• í‹€ë¦¬ë©´ ì™¸ ì•Šë˜? ì“°ê³ ì‹¶ì€ëŒ€ë¡œì“°ë©´ë¼ì§€ "
spelled_sent = spell_checker.check(sent)

hanspell_sent = spelled_sent.checked
print(hanspell_sent)
ë§ì¶¤ë²• í‹€ë¦¬ë©´ ì™œ ì•ˆë¼? ì“°ê³  ì‹¶ì€ ëŒ€ë¡œ ì“°ë©´ ë˜ì§€
```

ì´ íŒ¨í‚¤ì§€ëŠ” ë„ì–´ì“°ê¸° ë˜í•œ ë³´ì •í•œë‹¤. 

```python
# PyKoSpacingì— ì‚¬ìš©í•œ ì˜ˆì œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©

spelled_sent = spell_checker.check(new_sent)

hanspell_sent = spelled_sent.checked
print(hanspell_sent)
print(kospacing_sent) # ì•ì„œ ì‚¬ìš©í•œ kospacing íŒ¨í‚¤ì§€ì—ì„œ ì–»ì€ ê²°ê³¼
ê¹€ì² ìˆ˜ëŠ” ê·¹ ì¤‘ ë‘ ì¸ê²©ì˜ ì‚¬ë‚˜ì´ ì´ê´‘ìˆ˜ ì—­ì„ ë§¡ì•˜ë‹¤. ì² ìˆ˜ëŠ” í•œêµ­ ìœ ì¼ì˜ íƒœê¶Œë„ ì „ìŠ¹ìë¥¼ ê°€ë¦¬ëŠ” ê²°ì „ì˜ ë‚ ì„ ì•ë‘ê³  10ë…„ê°„ í•¨ê»˜ í›ˆë ¨í•œ ì‚¬í˜•ì¸ ìœ ì—°ì œ(ê¹€ê´‘ìˆ˜ ë¶„)ë¥¼ ì°¾ìœ¼ëŸ¬ ì†ì„¸ë¡œ ë‚´ë ¤ì˜¨ ì¸ë¬¼ì´ë‹¤.
ê¹€ì² ìˆ˜ëŠ” ê·¹ì¤‘ ë‘ ì¸ê²©ì˜ ì‚¬ë‚˜ì´ ì´ê´‘ìˆ˜ ì—­ì„ ë§¡ì•˜ë‹¤. ì² ìˆ˜ëŠ” í•œêµ­ ìœ ì¼ì˜ íƒœê¶Œë„ ì „ìŠ¹ìë¥¼ ê°€ë¦¬ëŠ” ê²°ì „ì˜ ë‚ ì„ ì•ë‘ê³  10ë…„ê°„ í•¨ê»˜ í›ˆë ¨í•œ ì‚¬í˜•ì¸ ìœ ì—°ì¬(ê¹€ê´‘ìˆ˜ ë¶„)ë¥¼ ì°¾ìœ¼ëŸ¬ ì†ì„¸ë¡œ ë‚´ë ¤ì˜¨ ì¸ë¬¼ì´ë‹¤.

# PyKoSpacingê³¼ ê²°ê³¼ê°€ ê±°ì˜ ë¹„ìŠ·í•˜ì§€ë§Œ ì¡°ê¸ˆ ë‹¤ë¥´ë‹¤.

```

### 3. SOYNLP ë¥¼ ì´ìš©í•œ ë‹¨ì–´ í† í°í™”

soynlpëŠ” í’ˆì‚¬ íƒœê¹…, ë‹¨ì–´ í† í°í™” ë“±ì„ ì§€ì›í•˜ëŠ” ë‹¨ì–´ í† í¬ë‚˜ì´ì €

SOYNLP ?
í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ íŠ¹ì • ë¬¸ì ì‹œí€€ìŠ¤ê°€ í•¨ê»˜ **ìì£¼ ë“±ì¥í•˜ëŠ” ë¹ˆë„ê°€ ë†’ê³ **, 
ì• ë’¤ë¡œ ì¡°ì‚¬ ë˜ëŠ” ì™„ì „íˆ ë‹¤ë¥¸ ë‹¨ì–´ê°€ ë“±ì¥í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•´
í•´ë‹¹ ë¬¸ì ì‹œí€€ìŠ¤ë¥¼ í˜•íƒœì†Œë¼ê³  íŒë‹¨í•˜ëŠ” ë‹¨ì–´ í† í¬ë‚˜ì´ì €

**ë¹„ì§€ë„ í•™ìŠµ**ìœ¼ë¡œ ë‹¨ì–´ í† í°í™”ë¥¼ í•œë‹¤ëŠ” íŠ¹ì§•

ë°ì´í„°ì— **ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´**ë“¤ì„ ë‹¨ì–´ë¡œ **ë¶„ì„**

soynlp ë‹¨ì–´ í† í¬ë‚˜ì´ì €ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ **ë‹¨ì–´ ì ìˆ˜ í‘œ**ë¡œ ë™ì‘

ì´ ì ìˆ˜ëŠ” **ì‘ì§‘ í™•ë¥ (cohesion probability)**ê³¼ **ë¸Œëœì¹­ ì—”íŠ¸ë¡œí”¼(branching entropy)**ë¥¼ í™œìš©

```python
!pip install soynlp
```

SOYNLPê°€ ì–´ë–¤ ì ì—ì„œ ìœ ìš©í•œì§€ ì •ë¦¬

**1) ì‹ ì¡°ì–´ ë¬¸ì œ**

**ê¸°ì¡´ì˜ í˜•íƒœì†Œ ë¶„ì„ê¸°**ëŠ” **ì‹ ì¡°ì–´**ë‚˜ í˜•íƒœì†Œ ë¶„ì„ê¸°ì— **ë“±ë¡ë˜ì§€ ì•Šì€ ë‹¨ì–´** ê°™ì€ ê²½ìš°ì—ëŠ” ì œëŒ€ë¡œ êµ¬ë¶„í•˜ì§€ ëª»í•˜ëŠ” ë‹¨ì 

```python
from konlpy.tag import Okt
tokenizer = Okt()
print(tokenizer.morphs('ì—ì´ë¹„ì‹ìŠ¤ ì´ëŒ€íœ˜ 1ì›” ìµœì• ëŒ ê¸°ë¶€ ìš”ì •'))
['ì—ì´', 'ë¹„ì‹ìŠ¤', 'ì´ëŒ€', 'íœ˜', '1ì›”', 'ìµœì• ', 'ëŒ', 'ê¸°ë¶€', 'ìš”ì •']
```

**2) í•™ìŠµí•˜ê¸°**

soynlpëŠ” ê¸°ë³¸ì ìœ¼ë¡œ **í•™ìŠµì— ê¸°ë°˜**í•œ í† í¬ë‚˜ì´ì €ì´ë¯€ë¡œ í•™ìŠµì— í•„ìš”í•œ í•œêµ­ì–´ ë¬¸ì„œë¥¼ ë‹¤ìš´ë¡œë“œ

```python
import urllib.request
from soynlp import DoublespaceLineCorpus
from soynlp.word import WordExtractor
urllib.request.urlretrieve("https://raw.githubusercontent.com/lovit/soynlp/master/tutorials/2016-10-20.txt", filename="2016-10-20.txt")
í›ˆë ¨ ë°ì´í„°ë¥¼ ë‹¤ìˆ˜ì˜ ë¬¸ì„œë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.

# í›ˆë ¨ ë°ì´í„°ë¥¼ ë‹¤ìˆ˜ì˜ ë¬¸ì„œë¡œ ë¶„ë¦¬
corpus = DoublespaceLineCorpus("2016-10-20.txt")
len(corpus)
30091
```

ìƒìœ„ 3ê°œì˜ ë¬¸ì„œë§Œ ì¶œë ¥

```python
i = 0
for document in corpus:
  if len(document) > 0:
    print(document)
    i = i+1
  if i == 3:
    break

19  1990  52 1 22
ì˜¤íŒ¨ì‚°í„°ë„ ì´ê²©ì „ ìš©ì˜ì ê²€ê±° ì„œìš¸ ì—°í•©ë‰´ìŠ¤ ê²½ì°° ê´€ê³„ìë“¤ì´ 19ì¼ ì˜¤í›„ ì„œìš¸ ê°•ë¶êµ¬ ì˜¤íŒ¨ì‚° í„°ë„ ì¸ê·¼ì—ì„œ ì‚¬ì œ ì´ê¸°ë¥¼ ë°œì‚¬í•´ ê²½ì°°ì„ ì‚´í•´í•œ ìš©ì˜ì ì„±ëª¨ì”¨ë¥¼ ê²€ê±°í•˜ê³  ìˆë‹¤ ... ì¤‘ëµ ... ìˆ²ì—ì„œ ë°œê²¬ëê³  ì¼ë¶€ëŠ” ì„±ì”¨ê°€ ì†Œì§€í•œ ê°€ë°© ì•ˆì— ìˆì—ˆë‹¤
í…Œí—¤ë€ ì—°í•©ë‰´ìŠ¤ ê°•í›ˆìƒ íŠ¹íŒŒì› ì´ìš© ìŠ¹ê°ìˆ˜ ê¸°ì¤€ ì„¸ê³„ ìµœëŒ€ ê³µí•­ì¸ ì•„ëì—ë¯¸ë¦¬íŠ¸ ë‘ë°”ì´êµ­ì œê³µí•­ì€ 19ì¼ í˜„ì§€ì‹œê°„ ì´ ê³µí•­ì„ ì´ë¥™í•˜ëŠ” ëª¨ë“  í•­ê³µê¸°ì˜ íƒ‘ìŠ¹ê°ì€ ì‚¼ì„±ì „ìì˜ ê°¤ëŸ­ì‹œë…¸íŠ¸7ì„ íœ´ëŒ€í•˜ë©´ ì•ˆ ëœë‹¤ê³  ë°í˜”ë‹¤ ... ì¤‘ëµ ... ì´ëŸ° ì¡°ì¹˜ëŠ” ë‘ë°”ì´êµ­ì œê³µí•­ ë¿ ì•„ë‹ˆë¼ ì‹ ê³µí•­ì¸ ë‘ë°”ì´ì›”ë“œì„¼í„°ì—ë„ ì ìš©ëœë‹¤  ë°°í„°ë¦¬ í­ë°œë¬¸ì œë¡œ íšŒìˆ˜ëœ ê°¤ëŸ­ì‹œë…¸íŠ¸7 ì—°í•©ë‰´ìŠ¤ìë£Œì‚¬ì§„
```

soynlpëŠ” í•™ìŠµ ê¸°ë°˜ì˜ ë‹¨ì–´ í† í¬ë‚˜ì´ì €ì´ë¯€ë¡œ í•™ìŠµ ê³¼ì •ì„ ê±°ì³ì•¼ í•œë‹¤. 

ì´ëŠ” ì „ì²´ ì½”í¼ìŠ¤ë¡œë¶€í„° **ì‘ì§‘ í™•ë¥ **ê³¼ **ë¸Œëœì¹­ ì—”íŠ¸ë¡œí”¼** ë‹¨ì–´ ì ìˆ˜í‘œë¥¼ ë§Œë“œëŠ” ê³¼ì •

 **WordExtractor.extract()**ë¥¼ í†µí•´ì„œ ì „ì²´ ì½”í¼ìŠ¤ì— ëŒ€í•´ ë‹¨ì–´ ì ìˆ˜í‘œë¥¼ ê³„ì‚°

```python
word_extractor = WordExtractor()
word_extractor.train(corpus)
word_score_table = word_extractor.extract()
training was done. used memory 5.186 Gb
all cohesion probabilities was computed. # words = 223348
all branching entropies was computed # words = 361598
all accessor variety was computed # words = 361598
```

**3) SOYLPì˜ ì‘ì§‘ í™•ë¥  (cohesion probability)**

ë‚´ë¶€ ë¬¸ìì—´(substring)ì´ ì–¼ë§ˆë‚˜ ì‘ì§‘í•˜ì—¬ ìì£¼ ë“±ì¥í•˜ëŠ” ì§€ë¥¼ íŒë‹¨í•˜ëŠ” ì²™ë„

ë¬¸ìì—´ì„ ë¬¸ì ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ë‚´ë¶€ ë¬¸ìì—´ì„ ë§Œë“œëŠ” ê³¼ì •ì—ì„œ 

ì™¼ìª½ë¶€í„° ìˆœì„œëŒ€ë¡œ ë¬¸ìë¥¼ ì¶”ê°€í•˜ë©´ì„œ 

ê° ë¬¸ìì—´ì´ ì£¼ì–´ì¡Œì„ ë•Œ ê·¸ ë‹¤ìŒ ë¬¸ìê°€ ë‚˜ì˜¬ í™•ë¥ ì„ ê³„ì‚°í•˜ì—¬ ëˆ„ì ê³±ì„ í•œ ê°’

ì´ ê°’ì´ ë†’ì„ìˆ˜ë¡ ì „ì²´ ì½”í¼ìŠ¤ì—ì„œ ì´ ë¬¸ìì—´ ì‹œí€€ìŠ¤ëŠ” í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œ ë“±ì¥í•  ê°€ëŠ¥ì„± ë†’ë‹¤

<img src = "/image/Untitled 3.png" width = "600px">

ex) 'ë°˜í¬í•œê°•ê³µì›ì—'ë¼ëŠ” 7ì˜ ê¸¸ì´ë¥¼ ê°€ì§„ ë¬¸ì ì‹œí€€ìŠ¤ì— ëŒ€í•´ì„œ ê° ë‚´ë¶€ ë¬¸ìì—´ì˜ ìŠ¤ì½”ì–´ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •

<img src = "/image/Untitled 4.png" width = "600px">

```python
word_score_table["ë°˜í¬í•œ"].cohesion_forward
0.08838002913645132
# ê·¸ë ‡ë‹¤ë©´ 'ë°˜í¬í•œê°•'ì˜ ì‘ì§‘ í™•ë¥ ì€ 'ë°˜í¬í•œ'ì˜ ì‘ì§‘ í™•ë¥ ë³´ë‹¤ ë†’ì„ê¹Œìš”?

word_score_table["ë°˜í¬í•œê°•"].cohesion_forward
0.19841268168224552
# 'ë°˜í¬í•œê°•'ì€ 'ë°˜í¬í•œ'ë³´ë‹¤ ì‘ì§‘ í™•ë¥ ì´ ë†’ìŠµë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ 'ë°˜í¬í•œê°•ê³µ'ì€ ì–´ë–¨ê¹Œìš”?

word_score_table["ë°˜í¬í•œê°•ê³µ"].cohesion_forward
0.2972877884078849
# ì—­ì‹œë‚˜ 'ë°˜í¬í•œê°•'ë³´ë‹¤ ì‘ì§‘ í™•ë¥ ì´ ë†’ìŠµë‹ˆë‹¤. 'ë°˜í¬í•œê°•ê³µì›'ì€ ì–´ë–¨ê¹Œìš”?

word_score_table["ë°˜í¬í•œê°•ê³µì›"].cohesion_forward
0.37891487632839754
# 'ë°˜í¬í•œê°•ê³µ'ë³´ë‹¤ ì‘ì§‘ í™•ë¥ ì´ ë†’ìŠµë‹ˆë‹¤. ì—¬ê¸°ë‹¤ê°€ ì¡°ì‚¬ 'ì—'ë¥¼ ë¶™ì¸ 'ë°˜í¬í•œê°•ê³µì›ì—'ëŠ” ì–´ë–¨ê¹Œìš”?

word_score_table["ë°˜í¬í•œê°•ê³µì›ì—"].cohesion_forward
0.33492963377557666
# ì˜¤íˆë ¤ 'ë°˜í¬í•œê°•ê³µì›'ë³´ë‹¤ ì‘ì§‘ë„ê°€ ë‚®ì•„ì§‘ë‹ˆë‹¤.

# ê²°êµ­ ê²°í•©ë„ëŠ” 'ë°˜í¬í•œê°•ê³µì›'ì¼ ë•Œê°€ ê°€ì¥ ë†’ì•˜ë‹¤. 
# ì‘ì§‘ë„ë¡œ íŒë‹¨í•˜ê¸°ì— í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œ íŒë‹¨í•˜ê¸°ì— ê°€ì¥ ì í•©í•œ ë¬¸ìì—´: 'ë°˜í¬í•œê°•ê³µì›'
```

**4) SOYNLPì˜ ë¸Œëœì¹­ ì—”íŠ¸ë¡œí”¼ (branching entropy)**

Branching EntropyëŠ” í™•ë¥  ë¶„í¬ì˜ ì—”íŠ¸ë¡œí”¼ ê°’ì„ ì‚¬ìš©

ì£¼ì–´ì§„ ë¬¸ìì—´ì—ì„œ ì–¼ë§ˆë‚˜ ë‹¤ìŒ ë¬¸ìê°€ ë“±ì¥í•  ìˆ˜ ìˆëŠ” ì§€ë¥¼ íŒë‹¨í•˜ëŠ” ì²™ë„

ë¸Œëœì¹­ ì—”íŠ¸ë¡œí”¼ì˜ ê°’ì€ í•˜ë‚˜ì˜ ì™„ì„±ëœ ë‹¨ì–´ì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ 

ë¬¸ë§¥ìœ¼ë¡œ ì¸í•´ ì ì  ì •í™•íˆ ì˜ˆì¸¡í•  ìˆ˜ ìˆê²Œ ë˜ë©´ì„œ ì ì  ì¤„ì–´ë“œëŠ” ì–‘ìƒì„ ë³´ì¸ë‹¤.

```python
word_score_table["ë””ìŠ¤"].right_branching_entropy
1.6371694761537934

word_score_table["ë””ìŠ¤í”Œ"].right_branching_entropy
-0.0

# 'ë””ìŠ¤' ë‹¤ìŒì—ëŠ” ë‹¤ì–‘í•œ ë¬¸ìê°€ ì˜¬ ìˆ˜ ìˆìœ¼ë‹ˆê¹Œ 1.63ì´ë¼ëŠ” ê°’ì„ ê°€ì§€ëŠ” ë°˜ë©´, 
# 'ë””ìŠ¤í”Œ'ì´ë¼ëŠ” ë¬¸ìì—´ ë‹¤ìŒì—ëŠ” ë‹¤ìŒ ë¬¸ìë¡œ 'ë ˆ'ê°€ ì˜¤ëŠ” ê²ƒì´ ë„ˆë¬´ë‚˜ ëª…ë°±í•˜ê¸° ë•Œë¬¸ì—
# 0ì´ë€ ê°’ì„ ê°€ì§„ë‹¤.

word_score_table["ë””ìŠ¤í”Œë ˆ"].right_branching_entropy
-0.0
word_score_table["ë””ìŠ¤í”Œë ˆì´"].right_branching_entropy
3.1400392861792916

# ê°‘ìê¸° ê°’ì´ ì¦ê°€í•©ë‹ˆë‹¤. 
# ë¬¸ì ì‹œí€€ìŠ¤ 'ë””ìŠ¤í”Œë ˆì´'ë¼ëŠ” ë¬¸ì ì‹œí€€ìŠ¤ ë‹¤ìŒì—ëŠ” ì¡°ì‚¬ë‚˜ ë‹¤ë¥¸ ë‹¨ì–´ì™€ ê°™ì€ 
# ë‹¤ì–‘í•œ ê²½ìš°ê°€ ìˆì„ ìˆ˜ ìˆê¸° ë•Œë¬¸
# í•˜ë‚˜ì˜ ë‹¨ì–´ê°€ ëë‚˜ë©´ ê·¸ ê²½ê³„ ë¶€ë¶„ë¶€í„° ë‹¤ì‹œ ë¸Œëœì¹­ ì—”íŠ¸ë¡œí”¼ ê°’ì´ ì¦ê°€í•˜ê²Œ ë¨ì„ ì˜ë¯¸
```

**5) SOYNLPì˜ L tokenizer**

í•œêµ­ì–´ëŠ” ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆˆ ì–´ì ˆ í† í°ì€ ì£¼ë¡œ L í† í° + R í† í°ì˜ í˜•ì‹ì„ ê°€ì§ˆ ë•Œê°€ ë§ë‹¤

L í† í° + R í† í°ìœ¼ë¡œ ë‚˜ëˆ„ë˜, ë¶„ë¦¬ ê¸°ì¤€ì„ ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ L í† í°ì„ ì°¾ì•„ë‚´ëŠ” ì›ë¦¬

```python
from soynlp.tokenizer import LTokenizer

scores = {word:score.cohesion_forward for word, score in word_score_table.items()}
l_tokenizer = LTokenizer(scores=scores)
l_tokenizer.tokenize("êµ­ì œì‚¬íšŒì™€ ìš°ë¦¬ì˜ ë…¸ë ¥ë“¤ë¡œ ë²”ì£„ë¥¼ ì²™ê²°í•˜ì", flatten=False)
[('êµ­ì œì‚¬íšŒ', 'ì™€'), ('ìš°ë¦¬', 'ì˜'), ('ë…¸ë ¥', 'ë“¤ë¡œ'), ('ë²”ì£„', 'ë¥¼'), ('ì²™ê²°', 'í•˜ì')]
```

**6) ìµœëŒ€ ì ìˆ˜ í† í¬ë‚˜ì´ì €**

ë„ì–´ì“°ê¸°ê°€ ë˜ì§€ ì•ŠëŠ” ë¬¸ì¥ì—ì„œ ì ìˆ˜ê°€ ë†’ì€ ê¸€ì ì‹œí€€ìŠ¤ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì°¾ëŠ” í† í¬ë‚˜ì´ì €

```python
from soynlp.tokenizer import MaxScoreTokenizer

maxscore_tokenizer = MaxScoreTokenizer(scores=scores)
maxscore_tokenizer.tokenize("êµ­ì œì‚¬íšŒì™€ìš°ë¦¬ì˜ë…¸ë ¥ë“¤ë¡œë²”ì£„ë¥¼ì²™ê²°í•˜ì")
['êµ­ì œì‚¬íšŒ', 'ì™€', 'ìš°ë¦¬', 'ì˜', 'ë…¸ë ¥', 'ë“¤ë¡œ', 'ë²”ì£„', 'ë¥¼', 'ì²™ê²°', 'í•˜ì']
```

### 4. SOYNLP ë¥¼ ì´ìš©í•œ ë°˜ë³µë˜ëŠ” ë¬¸ì ì •ì œ

SNSë‚˜ ì±„íŒ… ë°ì´í„°ì™€ ê°™ì€ í•œêµ­ì–´ ë°ì´í„°ì˜ ê²½ìš°

ã…‹ã…‹, ã…‹ã…‹ã…‹, ã…‹ã…‹ã…‹ã…‹ì™€ ê°™ì€ ê²½ìš°ë¥¼ ëª¨ë‘ ì„œë¡œ ë‹¤ë¥¸ ë‹¨ì–´ë¡œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì€ ë¶ˆí•„ìš”!

ì´ì— ë°˜ë³µë˜ëŠ” ê²ƒì€ í•˜ë‚˜ë¡œ ì •ê·œí™”.

```python
from soynlp.normalizer import *
print(emoticon_normalize('ì•œã…‹ã…‹ã…‹ã…‹ì´ì˜í™”ì¡´ì¼ì“°ã… ã… ã… ã… ã… ', num_repeats=2))
print(emoticon_normalize('ì•œã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ì´ì˜í™”ì¡´ì¼ì“°ã… ã… ã… ã… ', num_repeats=2))
print(emoticon_normalize('ì•œã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ì´ì˜í™”ì¡´ì¼ì“°ã… ã… ã… ã… ã… ã… ', num_repeats=2))
print(emoticon_normalize('ì•œã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ì´ì˜í™”ì¡´ì¼ì“°ã… ã… ã… ã… ã… ã… ã… ã… ', num_repeats=2))
ì•„ã…‹ã…‹ì˜í™”ì¡´ì¼ì“°ã… ã… 
ì•„ã…‹ã…‹ì˜í™”ì¡´ì¼ì“°ã… ã… 
ì•„ã…‹ã…‹ì˜í™”ì¡´ì¼ì“°ã… ã… 
ì•„ã…‹ã…‹ì˜í™”ì¡´ì¼ì“°ã… ã… 

# ì˜ë¯¸ì—†ê²Œ ë°˜ë³µë˜ëŠ” ê²ƒì€ ë¹„ë‹¨ ì´ëª¨í‹°ì½˜ì— í•œì •ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

print(repeat_normalize('ì™€í•˜í•˜í•˜í•˜í•˜í•˜í•˜í•˜í•˜í•«', num_repeats=2))
print(repeat_normalize('ì™€í•˜í•˜í•˜í•˜í•˜í•˜í•«', num_repeats=2))
print(repeat_normalize('ì™€í•˜í•˜í•˜í•˜í•«', num_repeats=2))
ì™€í•˜í•˜í•«
ì™€í•˜í•˜í•«
ì™€í•˜í•˜í•«
```

### 5. Customized KoNLPy

í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•  ë•Œ, ì´ëŸ° ìƒí™©ì— ë´‰ì°©í•œë‹¤ë©´ ì–´ë–»ê²Œ?

```python
í˜•íƒœì†Œ ë¶„ì„ ì…ë ¥ : 'ì€ê²½ì´ëŠ” ì‚¬ë¬´ì‹¤ë¡œ ê°”ìŠµë‹ˆë‹¤.'
í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ : ['ì€', 'ê²½ì´', 'ëŠ”', 'ì‚¬ë¬´ì‹¤', 'ë¡œ', 'ê°”ìŠµë‹ˆë‹¤', '.']
```

í˜•íƒœì†Œ ë¶„ì„ê¸°ì— ì‚¬ìš©ì ì‚¬ì „ì„ ì¶”ê°€í•´ì¤„ ìˆ˜ ìˆë‹¤.

'ì€ê²½ì´'ëŠ” í•˜ë‚˜ì˜ ë‹¨ì–´ì´ê¸° ë•Œë¬¸ì— ë¶„ë¦¬í•˜ì§€ ë§ë¼ê³  í˜•íƒœì†Œ ë¶„ì„ê¸°ì— ì•Œë ¤ì£¼ëŠ” ê²ƒ.

**Customized Konlpy** ëŠ” ì‚¬ìš©ì ì‚¬ì „ ì¶”ê°€ê°€ ë§¤ìš° ì‰¬ìš´ íŒ¨í‚¤ì§€

```python
pip install customized_konlpy

from ckonlpy.tag import Twitter
twitter = Twitter()
twitter.morphs('ì€ê²½ì´ëŠ” ì‚¬ë¬´ì‹¤ë¡œ ê°”ìŠµë‹ˆë‹¤.')
['ì€', 'ê²½ì´', 'ëŠ”', 'ì‚¬ë¬´ì‹¤', 'ë¡œ', 'ê°”ìŠµë‹ˆë‹¤', '.']
```

í˜•íƒœì†Œ ë¶„ì„ê¸° Twitterì— add_dictionary('ë‹¨ì–´', 'í’ˆì‚¬')ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì‚¬ì „ ì¶”ê°€ë¥¼ í•´ì¤„ ìˆ˜ ìˆë‹¤.

```python
twitter.add_dictionary('ì€ê²½ì´', 'Noun')

twitter.morphs('ì€ê²½ì´ëŠ” ì‚¬ë¬´ì‹¤ë¡œ ê°”ìŠµë‹ˆë‹¤.')
['ì€ê²½ì´', 'ëŠ”', 'ì‚¬ë¬´ì‹¤', 'ë¡œ', 'ê°”ìŠµë‹ˆë‹¤', '.']
```

---

# ì–¸ì–´ ëª¨ë¸ (Language Model)

ë‹¨ì–´ ì‹œí€€ìŠ¤(ë¬¸ì¥)ì— í™•ë¥ ì„ í• ë‹¹í•˜ëŠ” ëª¨ë¸

ì–¸ì–´ ëª¨ë¸ì´ í•˜ëŠ” ì¼!           **ì´ ë¬¸ì¥ì€ ì ì ˆí•´! ì´ ë¬¸ì¥ì€ ë§ì´ ì•ˆ ë¼!**  

í†µê³„ì— ê¸°ë°˜í•œ ì „í†µì ì¸ ì–¸ì–´ ëª¨ë¸(Statistical Languagel Model, **SLM**)

ìš°ë¦¬ê°€ ì‹¤ì œ ì‚¬ìš©í•˜ëŠ” ìì—°ì–´ë¥¼ ê·¼ì‚¬í•˜ê¸°ì—ëŠ” ë§ì€ í•œê³„

ì¸ê³µ ì‹ ê²½ë§ì´ ê·¸ëŸ¬í•œ í•œê³„ë¥¼ ë§ì´ í•´ê²°

í†µê³„ ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ì€ ë§ì´ ì‚¬ìš© ìš©ë„ê°€ ì¤„ì—ˆë‹¤.

í†µê³„ ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ì—ì„œ ë°°ìš°ëŠ” **n-gram**ì€ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ í™œë°œí•˜ê²Œ í™œìš©ë˜ê³  ìˆë‹¤.

## ì–¸ì–´ ëª¨ë¸ (Language Model) ì´ë€?

ì–¸ì–´ë¼ëŠ” í˜„ìƒì„ ëª¨ë¸ë§í•˜ê³ ì 
ë‹¨ì–´ ì‹œí€€ìŠ¤(ë˜ëŠ” ë¬¸ì¥)ì— **í™•ë¥ ì„ í• ë‹¹(assign)**í•˜ëŠ” ëª¨ë¸

ì–¸ì–´ ëª¨ë¸ì„ ë§Œë“œëŠ” ë°©ë²•

- í†µê³„ë¥¼ ì´ìš©í•œ ë°©ë²•
- ì¸ê³µ ì‹ ê²½ë§ì„ ì´ìš©í•œ ë°©ë²•

ìµœê·¼ ìì—°ì–´ ì²˜ë¦¬ì˜ ì‹ ê¸°ìˆ ì¸ **GPT**ë‚˜ **BERT** (ì¸ê³µ ì‹ ê²½ë§ ì–¸ì–´ ëª¨ë¸ì˜ ê°œë…ì„ ì‚¬ìš©) ì¸ê¸° !

### 1. ì–¸ì–´ ëª¨ë¸ (Language Model)

ë‹¨ì–´ ì‹œí€€ìŠ¤ì— **í™•ë¥ ì„ í• ë‹¹(assign)**í•˜ëŠ” ì¼ì„ í•˜ëŠ” ëª¨ë¸

â†’ ê°€ì¥ ìì—°ìŠ¤ëŸ¬ìš´ ë‹¨ì–´ ì‹œí€€ìŠ¤ë¥¼ ì°¾ì•„ë‚´ëŠ” ëª¨ë¸

ë‹¨ì–´ ì‹œí€€ìŠ¤ì— í™•ë¥ ì„ í• ë‹¹í•˜ê¸° ìœ„í•´ ê°€ì¥ ë³´í¸ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë°©ë²•

â†’  ì–¸ì–´ ëª¨ë¸ì´ ì´ì „ ë‹¨ì–´ë“¤ì´ ì£¼ì–´ì¡Œì„ ë•Œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•˜ëŠ” ê²ƒ.

ë‹¤ë¥¸ ìœ í˜•ì˜ ì–¸ì–´ ëª¨ë¸

ì£¼ì–´ì§„ ì–‘ìª½ì˜ ë‹¨ì–´ë“¤ë¡œë¶€í„° ê°€ìš´ë° ë¹„ì–´ìˆëŠ” ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì–¸ì–´ ëª¨ë¸

**ì–¸ì–´ ëª¨ë¸ë§(Language Modeling)**

 ì£¼ì–´ì§„ ë‹¨ì–´ë“¤ë¡œë¶€í„° ì•„ì§ ëª¨ë¥´ëŠ” ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì‘ì—…

 ì–¸ì–´ ëª¨ë¸ì´ ì´ì „ ë‹¨ì–´ë“¤ë¡œë¶€í„° ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì¼

---

### 2. ë‹¨ì–´ ì‹œí€€ìŠ¤ì˜ í™•ë¥  í• ë‹¹

ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ë‹¨ì–´ ì‹œí€€ìŠ¤ì— í™•ë¥ ì„ í• ë‹¹í•˜ëŠ” ì¼ì´ ì™œ í•„ìš”í• ê¹Œ?

**a. ê¸°ê³„ ë²ˆì—­(Machine Translation):**

P(ë‚˜ëŠ”Â ë²„ìŠ¤ë¥¼Â íƒ”ë‹¤)Â >Â P(ë‚˜ëŠ”Â ë²„ìŠ¤ë¥¼Â íƒœìš´ë‹¤)

: ì–¸ì–´ ëª¨ë¸ì€ ë‘ ë¬¸ì¥ì„ ë¹„êµí•˜ì—¬ ì¢Œì¸¡ì˜ ë¬¸ì¥ì˜ í™•ë¥ ì´ ë” ë†’ë‹¤ê³  íŒë‹¨.

**b. ì˜¤íƒ€ êµì •(Spell Correction)**

ì„ ìƒë‹˜ì´ êµì‹¤ë¡œ ë¶€ë¦¬ë‚˜ì¼€P(ë‹¬ë ¤ê°”ë‹¤)Â >Â P(ì˜ë ¤ê°”ë‹¤)

: ì–¸ì–´ ëª¨ë¸ì€ ë‘ ë¬¸ì¥ì„ ë¹„êµí•˜ì—¬ ì¢Œì¸¡ì˜ ë¬¸ì¥ì˜ í™•ë¥ ì´ ë” ë†’ë‹¤ê³  íŒë‹¨.

**c. ìŒì„± ì¸ì‹(Speech Recognition)**

P(ë‚˜ëŠ”Â ë©”ë¡±ì„Â ë¨¹ëŠ”ë‹¤)Â <Â P(ë‚˜ëŠ”Â ë©”ë¡ ì„Â ë¨¹ëŠ”ë‹¤)

: ì–¸ì–´ ëª¨ë¸ì€ ë‘ ë¬¸ì¥ì„ ë¹„êµí•˜ì—¬ ìš°ì¸¡ì˜ ë¬¸ì¥ì˜ í™•ë¥ ì´ ë” ë†’ë‹¤ê³  íŒë‹¨.

ì–¸ì–´ ëª¨ë¸ì€ ìœ„ì™€ ê°™ì´ **í™•ë¥ **ì„ í†µí•´ ë³´ë‹¤ **ì ì ˆí•œ ë¬¸ì¥ì„ íŒë‹¨**

---

### 3. ì£¼ì–´ì§„ ì´ì „ ë‹¨ì–´ë“¤ë¡œë¶€í„° ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡í•˜ê¸°

**A. ë‹¨ì–´ ì‹œí€€ìŠ¤ì˜ í™•ë¥ **

í•˜ë‚˜ì˜ ë‹¨ì–´ë¥¼Â w, ë‹¨ì–´ ì‹œí€€ìŠ¤ì„ ëŒ€ë¬¸ìÂ Wë¼ê³  í•œë‹¤ë©´,Â 

nê°œì˜ ë‹¨ì–´ê°€ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ ì‹œí€€ìŠ¤Â Wì˜ í™•ë¥ 

$$P(W)=P(w1,w2,w3,w4,w5,...,wn)$$

**B. ë‹¤ìŒ ë‹¨ì–´ ë“±ì¥ í™•ë¥ **

n-1ê°œì˜ ë‹¨ì–´ê°€ ë‚˜ì—´ëœ ìƒíƒœì—ì„œÂ në²ˆì§¸ ë‹¨ì–´ì˜ í™•ë¥ 

$$P(wn|w1,...,wnâˆ’1)$$

ì „ì²´ ë‹¨ì–´ ì‹œí€€ìŠ¤ Wì˜ í™•ë¥ 

$$P(W)=P(w1,w2,w3,w4,w5,...wn)=âˆP(wn|w1,...,wnâˆ’1)$$

---

### 4. ì–¸ì–´ ëª¨ë¸ì˜ ê°„ë‹¨í•œ ì§ê´€

ì•ì— ì–´ë–¤ ë‹¨ì–´ë“¤ì´ ë‚˜ì™”ëŠ”ì§€ ê³ ë ¤í•˜ì—¬ 

í›„ë³´ê°€ ë  ìˆ˜ ìˆëŠ” ì—¬ëŸ¬ ë‹¨ì–´ë“¤ì— ëŒ€í•´ì„œ ë“±ì¥ **í™•ë¥ ì„ ì¶”ì •**í•˜ê³ 

**ê°€ì¥ ë†’ì€ í™•ë¥ **ì„ ê°€ì§„ ë‹¨ì–´ë¥¼ ì„ íƒ

---

### 5. ê²€ìƒ‰ ì—”ì§„ì—ì„œì˜ ì–¸ì–´ ëª¨ë¸ì˜ ì˜ˆ

<img src = "/image/Untitled 5.png" width = "600px">

---

## í†µê³„ì  ì–¸ì–´ ëª¨ë¸ (Statistical Language Model, SLM)

### 1. ì¡°ê±´ë¶€ í™•ë¥ 

$$p(B|A)=P(A,B)/P(A)$$

$$P(A,B)=P(A)P(B|A)$$

**ì¡°ê±´ë¶€ í™•ë¥ ì˜ ì—°ì‡„ ë²•ì¹™(chain rule)**

$$P(x1,x2,x3...xn)=P(x1)P(x2|x1)P(x3|x1,x2)...P(xn|x1...xnâˆ’1)$$

---

### 2. ë¬¸ì¥ì— ëŒ€í•œ í™•ë¥ 

ë¬¸ì¥ '**An adorable little boy is spreading smiles**'ì˜ í™•ë¥ 

$$P(An adorable little boy is spreading smiles)=
P(An)Ã—P(adorable|An)Ã—P(little|An adorable)Ã—P(boy|An adorable little)Ã—P(is|An adorable little boy)P(An)Ã—P(adorable|An)Ã—P(little|An adorable)Ã—P(boy|An adorable little)Ã—P(is|An adorable little boy) Ã—P(spreading|An adorable little boy is)Ã—P(smiles|An adorable little boy is spreading)$$

---

### 3. ì¹´ìš´íŠ¸ ê¸°ë°˜ì˜ ì ‘ê·¼

SLMì€ ì´ì „ ë‹¨ì–´ë¡œë¶€í„° ë‹¤ìŒ ë‹¨ì–´ì— ëŒ€í•œ í™•ë¥ ì€ ì–´ë–»ê²Œ êµ¬í• ê¹Œ?

â†’ **ì¹´ìš´íŠ¸ì— ê¸°ë°˜í•˜ì—¬ í™•ë¥ ì„ ê³„ì‚°.**

$$P\text{(is|An adorable little boy}) = \frac{\text{count(An adorable little boy is})}{\text{count(An adorable little boy })}$$

ex) ê¸°ê³„ê°€ í•™ìŠµí•œ ì½”í¼ìŠ¤ ë°ì´í„°ì—ì„œ An adorable little boyê°€ 100ë²ˆ ë“±ì¥

ê·¸ ë‹¤ìŒì— isê°€ ë“±ì¥í•œ ê²½ìš°ëŠ” 30ë²ˆ

 â†’ ì´ ê²½ìš° $P(\text{is|An adorable little boy})$ëŠ” 30%

---

### 4. ì¹´ìš´íŠ¸ ê¸°ë°˜ ì ‘ê·¼ì˜ í•œê³„ - í¬ì†Œ ë¬¸ì œ (Sparsity Problem)

ê¸°ê³„ì—ê²Œ ë§ì€ ì½”í¼ìŠ¤ë¥¼ í›ˆë ¨ì‹œì¼œì„œ ì–¸ì–´ ëª¨ë¸ì„ í†µí•´ í˜„ì‹¤ì—ì„œì˜ í™•ë¥  ë¶„í¬ë¥¼ ê·¼ì‚¬í•˜ëŠ” ê²ƒì´ ì–¸ì–´ ëª¨ë¸ì˜ ëª©í‘œ

ê·¸ëŸ°ë° ì¹´ìš´íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì ‘ê·¼í•˜ë ¤ê³  í•œë‹¤ë©´ ê°–ê³  ìˆëŠ” ì½”í¼ìŠ¤(corpus). 

ì¦‰, ë‹¤ì‹œ ë§í•´ ê¸°ê³„ê°€ í›ˆë ¨í•˜ëŠ” ë°ì´í„°ëŠ” ì •ë§ **ë°©ëŒ€í•œ ì–‘**ì´ í•„ìš”.

ìœ„ì™€ ê°™ì´ $P\text{(is|An adorable little boy})$ë¥¼ êµ¬í•˜ëŠ” ê²½ìš°

ê¸°ê³„ê°€ í›ˆë ¨í•œ ì½”í¼ìŠ¤ì— An adorable little boy isë¼ëŠ” ë‹¨ì–´ ì‹œí€€ìŠ¤ê°€ ì—†ì—ˆë‹¤ë©´ 

ì´ ë‹¨ì–´ ì‹œí€€ìŠ¤ì— ëŒ€í•œ í™•ë¥ ì€ 0.

ë˜ëŠ” An adorable little boyë¼ëŠ” ë‹¨ì–´ ì‹œí€€ìŠ¤ê°€ ì—†ì—ˆë‹¤ë©´ 

ë¶„ëª¨ê°€ 0ì´ ë˜ì–´ í™•ë¥ ì€ ì •ì˜ë˜ì§€ ì•ŠëŠ”ë‹¤.

**í¬ì†Œ ë¬¸ì œ(sparsity problem)**

ì¶©ë¶„í•œ ë°ì´í„°ë¥¼ ê´€ì¸¡í•˜ì§€ ëª»í•˜ì—¬ ì–¸ì–´ë¥¼ ì •í™•íˆ ëª¨ë¸ë§í•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œ

ìœ„ ë¬¸ì œë¥¼ ì™„í™”í•˜ëŠ” ë°©ë²•

n-gram, ìŠ¤ë¬´ë”©, ë°±ì˜¤í”„

í¬ì†Œ ë¬¸ì œì— ëŒ€í•œ ê·¼ë³¸ì ì¸ í•´ê²°ì±…ì€ X

ì´ëŸ¬í•œ í•œê³„ë¡œ ì¸í•´ ì–¸ì–´ ëª¨ë¸ì˜ íŠ¸ë Œë“œëŠ” ì¸ê³µ ì‹ ê²½ë§ ì–¸ì–´ ëª¨ë¸ë¡œ ë„˜ì–´ê°”ë‹¤.

---

## N-gram ì–¸ì–´ ëª¨ë¸ (N-gram Language Model)

n-gram ì–¸ì–´ ëª¨ë¸ì€ 

ì´ì „ì— ë“±ì¥í•œ ëª¨ë“  ë‹¨ì–´ë¥¼ ê³ ë ¤í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì¼ë¶€ ë‹¨ì–´ë§Œ ê³ ë ¤í•˜ëŠ” ì ‘ê·¼ ë°©ë²•ì„ ì‚¬ìš©

nì˜ ì˜ë¯¸?

ì¼ë¶€ ë‹¨ì–´ë¥¼ ëª‡ ê°œ ë³´ëŠëƒë¥¼ ê²°ì •

### 1. Corpus ì—ì„œ ì¹´ìš´íŠ¸ í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ì˜ ê°ì†Œ

SLMì˜ í•œê³„

 í›ˆë ¨ ì½”í¼ìŠ¤ì— í™•ë¥ ì„ ê³„ì‚°í•˜ê³  ì‹¶ì€ ë¬¸ì¥ì´ë‚˜ ë‹¨ì–´ê°€ ì—†ì„ ìˆ˜ ìˆë‹¤ëŠ” ì 

 í™•ë¥ ì„ ê³„ì‚°í•˜ê³  ì‹¶ì€ ë¬¸ì¥ì´ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì½”í¼ìŠ¤ì—ì„œ ê·¸ ë¬¸ì¥ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´  

 ë†’ë‹¤ëŠ” ì 

$$P(\text{is|An adorable little boy}) \approx\ P(\text{is|little boy})$$

ì´ì œëŠ” ë‹¨ì–´ì˜ í™•ë¥ ì„ êµ¬í•˜ê³ ì ê¸°ì¤€ ë‹¨ì–´ì˜ ì• ë‹¨ì–´ë¥¼ ì „ë¶€ í¬í•¨í•´ì„œ ì¹´ìš´íŠ¸í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì• ë‹¨ì–´ ì¤‘ **ì„ì˜ì˜ ê°œìˆ˜ë§Œ í¬í•¨**í•´ì„œ ì¹´ìš´íŠ¸í•˜ì—¬ ê·¼ì‚¬

---

### 2. N-gram

ê°–ê³  ìˆëŠ” ì½”í¼ìŠ¤ì—ì„œ **nê°œì˜ ë‹¨ì–´ ë­‰ì¹˜ ë‹¨ìœ„**ë¡œ ëŠì–´ì„œ ì´ë¥¼ **í•˜ë‚˜ì˜ í† í°**ìœ¼ë¡œ ê°„ì£¼

**uni**grams : an, adorable, little, boy, is, spreading, smiles

**bi**grams : an adorable, adorable little, little boy, boy is, is spreading, spreading smiles

**tri**grams : an adorable little, adorable little boy, little boy is, boy is spreading, is spreading smiles

**4-**grams : an adorable little boy, adorable little boy is, little boy is spreading, boy is spreading smiles

n-gramì„ í†µí•œ ì–¸ì–´ ëª¨ë¸ì—ì„œëŠ” 

ë‹¤ìŒì— ë‚˜ì˜¬ ë‹¨ì–´ì˜ ì˜ˆì¸¡ì€ **ì˜¤ì§ n-1ê°œì˜ ë‹¨ì–´ì—ë§Œ ì˜ì¡´**

ex) 4-gram ì„ ì´ìš©í•œ ì–¸ì–´ ëª¨ë¸

<img src = "/image/Untitled 6.png" width = "600px">

$$P(w\text{|boy is spreading}) = \frac{\text{count(boy is spreading}\ w)}{\text{count(boy is spreading)}}$$

---

### 3. N-gram Language Modelì˜ í•œê³„

ì „ì²´ ë¬¸ì¥ì„ ê³ ë ¤í•œ ì–¸ì–´ ëª¨ë¸ë³´ë‹¤ëŠ” ì •í™•ë„ê°€ ë–¨ì–´ì§ˆ ìˆ˜ë°–ì— ì—†ë‹¤.

**1) í¬ì†Œ ë¬¸ì œ (Sparsity Problem)** 

n-gram ì–¸ì–´ ëª¨ë¸ë„ ì—¬ì „íˆ n-gramì— ëŒ€í•œ í¬ì†Œ ë¬¸ì œê°€ ì¡´ì¬

**2) nì„ ì„ íƒí•˜ëŠ” ê²ƒì€ trade-off ë¬¸ì œ**

nì„ í¬ê²Œ ì„ íƒí•˜ë©´ 

í›ˆë ¨ ì½”í¼ìŠ¤ì—ì„œ í•´ë‹¹ n-gramì„ ì¹´ìš´íŠ¸í•  ìˆ˜ ìˆëŠ” í™•ë¥ ì€ ì ì–´ì§ â†’ í¬ì†Œ ë¬¸ì œëŠ” ì ì  ì‹¬ê°

nì„ ì‘ê²Œ ì„ íƒí•˜ë©´ 

í›ˆë ¨ ì½”í¼ìŠ¤ì—ì„œ ì¹´ìš´íŠ¸ëŠ” ì˜ ë˜ê² ì§€ë§Œ ê·¼ì‚¬ì˜ ì •í™•ë„ëŠ” í˜„ì‹¤ì˜ í™•ë¥ ë¶„í¬ì™€ ë©€ì–´ì§

ì •í™•ë„ë¥¼ ë†’ì´ë ¤ë©´ **nì€ ìµœëŒ€ 5ë¥¼ ë„˜ê²Œ ì¡ì•„ì„œëŠ” ì•ˆ ëœë‹¤ê³  ê¶Œì¥**

---

### 4. ì ìš© ë¶„ì•¼(Domain)ì— ë§ëŠ” ì½”í¼ìŠ¤ì˜ ìˆ˜ì§‘

ì–´ë–¤ ë¶„ì•¼ì¸ì§€, ì–´ë–¤ ì–´í”Œë¦¬ì¼€ì´ì…˜ì¸ì§€ì— ë”°ë¼ì„œ íŠ¹ì • ë‹¨ì–´ë“¤ì˜ í™•ë¥  ë¶„í¬ëŠ” ë‹¤ë¥´ë‹¤.

ì–¸ì–´ ëª¨ë¸ì— ì‚¬ìš©í•˜ëŠ” ì½”í¼ìŠ¤ë¥¼ í•´ë‹¹ ë„ë©”ì¸ì˜ ì½”í¼ìŠ¤ë¥¼ ì‚¬ìš©í•œë‹¤ë©´ 

ë‹¹ì—°íˆ ì–¸ì–´ ëª¨ë¸ì´ ì œëŒ€ë¡œ ëœ ì–¸ì–´ ìƒì„±ì„ í•  ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§„ë‹¤.

---

### 5. ì¸ê³µ ì‹ ê²½ë§ì„ ì´ìš©í•œ ì–¸ì–´ ëª¨ë¸(Neural Network Based Language Model)

n-gram ì–¸ì–´ ëª¨ë¸ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ 

ë¶„ëª¨, ë¶„ìì— ìˆ«ìë¥¼ ë”í•´ ì¹´ìš´íŠ¸ í–ˆì„ ë•Œ í™•ë¥ ì´ 0ì„ ë°©ì§€í•˜ëŠ” ë“±ì˜ ì—¬ëŸ¬ ì¼ë°˜í™” ë°©ë²• ì¡´ì¬

ê·¸ëŸ¼ì—ë„ ë³¸ì§ˆì ìœ¼ë¡œ n-gram ì–¸ì–´ ëª¨ë¸ì— ëŒ€í•œ ì·¨ì•½ì ì„ ì™„ì „íˆ í•´ê²°í•˜ì§€ëŠ” ëª»í•¨

ì¸ê³µ ì‹ ê²½ë§ì„ ì´ìš©í•œ ì–¸ì–´ ëª¨ë¸ì´ ë§ì´ ì‚¬ìš©ë˜ê³  ìˆë‹¤.

---

## í•œêµ­ì–´ì—ì„œì˜ ì–¸ì–´ ëª¨ë¸ (Language Model for Korean Sentences)

í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ëŠ” ì˜ì–´ë³´ë‹¤ í›¨ì”¬ ì–´ë µë‹¤.

### 1. í•œêµ­ì–´ëŠ” ì–´ìˆœì´ ì¤‘ìš”í•˜ì§€ ì•Šë‹¤.

ex)

â‘  ë‚˜ëŠ” ìš´ë™ì„ í•©ë‹ˆë‹¤ ì²´ìœ¡ê´€ì—ì„œ.

â‘¡ ë‚˜ëŠ” ì²´ìœ¡ê´€ì—ì„œ ìš´ë™ì„ í•©ë‹ˆë‹¤.

â‘¢ ì²´ìœ¡ê´€ì—ì„œ ìš´ë™ì„ í•©ë‹ˆë‹¤.

â‘£ ë‚˜ëŠ” ìš´ë™ì„ ì²´ìœ¡ê´€ì—ì„œ í•©ë‹ˆë‹¤.

ë‹¨ì–´ ìˆœì„œë¥¼ ë’¤ì£½ë°•ì£½ìœ¼ë¡œ ë°”ê¾¸ì–´ë„ í•œêµ­ì–´ëŠ” ì˜ë¯¸ê°€ ì „ë‹¬ ë˜ê¸° ë•Œë¬¸ì— 

í™•ë¥ ì— ê¸°ë°˜í•œ ì–¸ì–´ ëª¨ë¸ì´ ì œëŒ€ë¡œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ê¸°ê°€ ì–´ë µë‹¤.

---

### 2. í•œêµ­ì–´ëŠ” êµì°©ì–´ì´ë‹¤.

ëŒ€í‘œì ì¸ ì˜ˆë¡œ êµì°©ì–´ì¸ í•œêµ­ì–´ì—ëŠ” ì¡°ì‚¬ê°€ ìˆë‹¤.

**ë„ì–´ì“°ê¸° ë‹¨ìœ„ì¸ ì–´ì ˆ ë‹¨ìœ„**ë¡œ í† í°í™”ë¥¼ í•  ê²½ìš°

ë¬¸ì¥ì—ì„œ ë°œìƒ ê°€ëŠ¥í•œ ë‹¨ì–´ì˜ ìˆ˜ê°€ êµ‰ì¥íˆ ëŠ˜ì–´ë‚œë‹¤.

'ê·¸ë…€'ë¼ëŠ” ë‹¨ì–´ í•˜ë‚˜ë§Œ í•´ë„ 

ê·¸ë…€ê°€, ê·¸ë…€ë¥¼, ê·¸ë…€ì˜, ê·¸ë…€ì™€, ê·¸ë…€ë¡œ, ê·¸ë…€ê»˜ì„œ, ê·¸ë…€ì²˜ëŸ¼ ë“±ê³¼ ê°™ì´ ë‹¤ì–‘í•œ ê²½ìš°ê°€ ì¡´ì¬

í•œêµ­ì–´ì—ì„œëŠ” **í† í°í™”**ë¥¼ í†µí•´ ì ‘ì‚¬ë‚˜ ì¡°ì‚¬ ë“±ì„ ë¶„ë¦¬í•˜ëŠ” ê²ƒì€ ì¤‘ìš”í•œ ì‘ì—…

---

### í•œêµ­ì–´ëŠ” ë„ì–´ì“°ê¸°ê°€ ì œëŒ€ë¡œ ì§€ì¼œì§€ì§€ ì•ŠëŠ”ë‹¤.

í•œêµ­ì–´ëŠ” ë„ì–´ì“°ê¸°ë¥¼ ì œëŒ€ë¡œ í•˜ì§€ ì•Šì•„ë„ ì˜ë¯¸ê°€ ì „ë‹¬ë˜ë©°, ë„ì–´ì“°ê¸° ê·œì¹™ ë˜í•œ ìƒëŒ€ì ìœ¼ë¡œ ê¹Œë‹¤ë¡œìš´ ì–¸ì–´

í† í°ì´ ì œëŒ€ë¡œ ë¶„ë¦¬ ë˜ì§€ ì•Šì€ ì±„ í›ˆë ¨ ë°ì´í„°ë¡œ ì‚¬ìš©ëœë‹¤ë©´ ì–¸ì–´ ëª¨ë¸ì€ ì œëŒ€ë¡œ ë™ì‘í•˜ì§€ ì•ŠëŠ”ë‹¤.

---

## í„í”Œë ‰ì„œí‹° (Perplexity)

ë‘ ê°œì˜ ëª¨ë¸ A, Bê°€ ìˆì„ ë•Œ ì„±ëŠ¥ì€ ì–´ë–»ê²Œ ë¹„êµí•  ìˆ˜ ìˆì„ê¹Œ?

**ì™¸ë¶€ í‰ê°€(extrinsic evaluation)**

ë‘ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ê³ ì, ì¼ì¼ì´ ëª¨ë¸ë“¤ì— ëŒ€í•´ì„œ ì‹¤ì œ ì‘ì—…ì„ ì‹œì¼œë³´ê³  ì •í™•ë„ë¥¼ ë¹„êµí•˜ëŠ” ì‘ì—…ì€ ê³µìˆ˜ê°€ ë„ˆë¬´ ë§ì´ ë“œëŠ” ì‘ì—…

**ë‚´ë¶€ í‰ê°€(Intrinsic evaluation)**

ì¡°ê¸ˆì€ ë¶€ì •í™•í•  ìˆ˜ëŠ” ìˆì–´ë„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ì„œ ë¹ ë¥´ê²Œ ì‹ìœ¼ë¡œ ê³„ì‚°ë˜ëŠ” ë” ê°„ë‹¨í•œ í‰ê°€ ë°©ë²•

ëª¨ë¸ ë‚´ì—ì„œ ìì‹ ì˜ ì„±ëŠ¥ì„ ìˆ˜ì¹˜í™”í•˜ì—¬ ê²°ê³¼ë¥¼ ë‚´ë†“ëŠ” í‰ê°€

**perplexity** 

### 1. ì–¸ì–´ ëª¨ë¸ì˜ í‰ê°€ ë°©ë²• (Evaluation metric) : Perplexity ì¤„ì—¬ì„œ PPL

ì–¸ì–´ ëª¨ë¸ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë‚´ë¶€ í‰ê°€ ì§€í‘œ

ë‹¨ì–´ì˜ ìˆ˜ë¡œ ì •ê·œí™”(normalization) ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ í™•ë¥ ì˜ ì—­ìˆ˜

$$PPL(W)=P(w_{1}, w_{2}, w_{3}, ... , w_{N})^{-\frac{1}{N}}=\sqrt[N]{\frac{1}{P(w_{1}, w_{2}, w_{3}, ... , w_{N})}}$$

**PPLì„ ìµœì†Œí™”í•œë‹¤ëŠ” ê²ƒì€ ë¬¸ì¥ì˜ í™•ë¥ ì„ ìµœëŒ€í™”**

ë¬¸ì¥ì˜ í™•ë¥ ì— ì²´ì¸ë£°(chain rule)ì„ ì ìš©í•˜ë©´

$$PPL(W)=\sqrt[N]{\frac{1}{P(w_{1}, w_{2}, w_{3}, ... , w_{N})}}=\sqrt[N]{\frac{1}{\prod_{i=1}^{N}P(w_{i}| w_{1}, w_{2}, ... , w_{i-1})}}$$

### 2. ë¶„ê¸° ê³„ìˆ˜(Branching factor)

PPLì€ ì„ íƒí•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥í•œ ê²½ìš°ì˜ ìˆ˜ë¥¼ ì˜ë¯¸í•˜ëŠ” ë¶„ê¸°ê³„ìˆ˜(branching factor)ì´ë‹¤.

PPLì€ ì´ ì–¸ì–´ ëª¨ë¸ì´ íŠ¹ì • ì‹œì ì—ì„œ í‰ê· ì ìœ¼ë¡œ **ëª‡ ê°œì˜ ì„ íƒì§€**ë¥¼ ê°€ì§€ê³  ê³ ë¯¼í•˜ê³  ìˆëŠ”ì§€ë¥¼ ì˜ë¯¸

ex) ì–¸ì–´ ëª¨ë¸ì— ì–´ë–¤ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì„ ì£¼ê³  ì¸¡ì •í–ˆë”ë‹ˆ **PPLì´ 10**

 â†’ í•´ë‹¹ ì–¸ì–´ ëª¨ë¸ì€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ì„œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë“  ì‹œì (time-step)ë§ˆë‹¤ **í‰ê· ì ìœ¼ë¡œ 10ê°œì˜ ë‹¨ì–´**ë¥¼ ê°€ì§€ê³  ì–´ë–¤ ê²ƒì´ ì •ë‹µì¸ì§€ ê³ ë¯¼í•˜ê³  ìˆë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.

$$PPL(W)=P(w_{1}, w_{2}, w_{3}, ... , w_{N})^{-\frac{1}{N}}=(\frac{1}{10}^{N})^{-\frac{1}{N}}=\frac{1}{10}^{-1}=10$$

ì£¼ì˜í•  ì !

PPLì˜ ê°’ì´ ë‚®ë‹¤ëŠ” ê²ƒì€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì—ì„œ ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì¸ë‹¤ëŠ” ê²ƒì´ì§€, 

ì‚¬ëŒì´ ì§ì ‘ ëŠë¼ê¸°ì— ì¢‹ì€ ì–¸ì–´ ëª¨ë¸ì´ë¼ëŠ” ê²ƒì„ ë°˜ë“œì‹œ ì˜ë¯¸í•˜ì§„ ì•ŠëŠ”ë‹¤ëŠ” ì 

ì •ëŸ‰ì ìœ¼ë¡œ ì–‘ì´ ë§ê³ , ë˜í•œ ë„ë©”ì¸ì— ì•Œë§ì€ ë™ì¼í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì•¼ ì‹ ë¢°ë„ê°€ ë†’ë‹¤

### 3. ê¸°ì¡´ ì–¸ì–´ ëª¨ë¸ vs ì¸ê³µ ì‹ ê²½ë§ì„ ì´ìš©í•œ ì–¸ì–´ ëª¨ë¸

<img src = "/image/Untitled 7.png" width = "600px">

---

# ì¹´ìš´íŠ¸ ê¸°ë°˜ì˜ ë‹¨ì–´ í‘œí˜„ (Count based word Representation)

ë¨¸ì‹  ëŸ¬ë‹ ë“±ì˜ ì•Œê³ ë¦¬ì¦˜ì´ ì ìš©ëœ ë³¸ê²©ì ì¸ ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•´ì„œëŠ” 

**ë¬¸ìë¥¼ ìˆ«ìë¡œ ìˆ˜ì¹˜í™” í•  í•„ìš”ê°€ ìˆë‹¤.**

## ë‹¤ì–‘í•œ ë‹¨ì–´ì˜ í‘œí˜„ ë°©ë²•

### 1. ë‹¨ì–´ì˜ í‘œí˜„ ë°©ë²•

**êµ­ì†Œ í‘œí˜„(Local Representation)** ë°©ë²•

  í•´ë‹¹ ë‹¨ì–´ ê·¸ ìì²´ë§Œ ë³´ê³ , íŠ¹ì •ê°’ì„ ë§µí•‘í•˜ì—¬ ë‹¨ì–´ë¥¼ í‘œí˜„í•˜ëŠ” ë°©ë²•

**ë¶„ì‚° í‘œí˜„(Distributed Representation)** ë°©ë²•

  ê·¸ ë‹¨ì–´ë¥¼ í‘œí˜„í•˜ê³ ì ì£¼ë³€ì„ ì°¸ê³ í•˜ì—¬ ë‹¨ì–´ë¥¼ í‘œí˜„í•˜ëŠ” ë°©ë²•

ex) puppy, cute, lovely  â† ì´ ì„¸ ë‹¨ì–´ë¥¼ 

êµ­ì†Œ í‘œí˜„ ì˜ˆ) 1ë²ˆ 2ë²ˆ 3ë²ˆ ë“±ê³¼ ê°™ì€ ìˆ«ìë¥¼ mapping í•˜ì—¬ ë¶€ì—¬

ë¶„ì‚° í‘œí˜„ ì˜ˆ) puppy ë‹¨ì–´ ê·¼ì²˜ì— cute, lovely ìì£¼ ë“±ì¥ 

                   â†’ puppy ëŠ” cute, lovely í•œ ëŠë‚Œìœ¼ë¡œ ì •ì˜ 

ë‹¨ì–´ì˜ ì˜ë¯¸, ë‰˜ì•™ìŠ¤ë¥¼ í‘œí˜„  : êµ­ì†Œ í‘œí˜„ O  /  ë¶„ì‚° í‘œí˜„ X

êµ­ì†Œ í‘œí˜„ ë°©ë²•(Local Representation)ì„ ì´ì‚° í‘œí˜„(Discrete Representation)ë¼ê³ ,

ë¶„ì‚° í‘œí˜„(Distributed Representation)ì„ ì—°ì† í‘œí˜„(Continuous Representation)ë¼ê³ ë„í•œë‹¤

---

### 2. ë‹¨ì–´ í‘œí˜„ì˜ ì¹´í…Œê³ ë¦¬í™”

<img src = "/image/Untitled 8.png" width = "600px">

**Bag of Words** :    êµ­ì†Œ í‘œí˜„ì—(Local Representation)ì— ì†í•˜ë©°, ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ë¥¼ ì¹´ìš´íŠ¸(Count)í•˜ì—¬ ë‹¨ì–´ë¥¼ ìˆ˜ì¹˜í™”í•˜ëŠ” ë‹¨ì–´ í‘œí˜„ ë°©ë²•

**DTM**(ë˜ëŠ” **TDM**):   BoW ì˜ í™•ì¥

**TF-IDF** :    ë¹ˆë„ìˆ˜ ê¸°ë°˜ ë‹¨ì–´ í‘œí˜„ì— ë‹¨ì–´ì˜ ì¤‘ìš”ë„ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ì¤Œ 

**LSA**:    ë‹¨ì–´ì˜ ë‰˜ì•™ìŠ¤ë¥¼ ë°˜ì˜í•˜ëŠ” ì—°ì† í‘œí˜„(Continuous Representation)ì˜ ì¼ì¢…

**Word2Vec**(ì›Œë“œíˆ¬ë²¡í„°) :    ì—°ì† í‘œí˜„(Continuous Representation)ì— ì†í•˜ë©´ì„œ, ì˜ˆì¸¡(prediction)ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ì–´ì˜ ë‰˜ì•™ìŠ¤ë¥¼ í‘œí˜„

**FastText**(íŒ¨ìŠ¤íŠ¸í…ìŠ¤íŠ¸) :      Word2Vec ì˜ í™•ì¥ 

**GloVe**(ê¸€ë¡œë¸Œ):     ì˜ˆì¸¡ê³¼ ì¹´ìš´íŠ¸ë¼ëŠ” ë‘ ê°€ì§€ ë°©ë²•ì´ ëª¨ë‘ ì‚¬ìš©

---

## Bag of Words(BoW)

ë‹¨ì–´ì˜ **ë“±ì¥ ìˆœì„œë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”** ë¹ˆë„ìˆ˜ ê¸°ë°˜ì˜ ë‹¨ì–´ í‘œí˜„ ë°©ë²•

ë‹¨ì–´ë“¤ì˜ **ì¶œí˜„ ë¹ˆë„(frequency)**ì—ë§Œ ì§‘ì¤‘

BoWë¥¼ ë§Œë“œëŠ” ê³¼ì •

(1) ìš°ì„ , ê° ë‹¨ì–´ì— ê³ ìœ í•œ ì •ìˆ˜ ì¸ë±ìŠ¤ë¥¼ ë¶€ì—¬

(2) ê° ì¸ë±ìŠ¤ì˜ ìœ„ì¹˜ì— ë‹¨ì–´ í† í°ì˜ ë“±ì¥ íšŸìˆ˜ë¥¼ ê¸°ë¡í•œ ë²¡í„°ë¥¼ ë§Œë“ ë‹¤.

ex) 

ë¬¸ì„œ1:  ì •ë¶€ê°€ ë°œí‘œí•˜ëŠ” ë¬¼ê°€ìƒìŠ¹ë¥ ê³¼ ì†Œë¹„ìê°€ ëŠë¼ëŠ” ë¬¼ê°€ìƒìŠ¹ë¥ ì€ ë‹¤ë¥´ë‹¤.

ì…ë ¥ëœ ë¬¸ì„œì— ëŒ€í•´ì„œ ë‹¨ì–´ ì§‘í•©(vocaburary)ì„ ë§Œë“¤ì–´ ì¸ë±ìŠ¤ë¥¼ í• ë‹¹í•˜ê³ , BoWë¥¼ ë§Œë“œëŠ” ì½”ë“œ

```python
from konlpy.tag import Okt
import re  
okt=Okt()  

token=re.sub("(\.)","","ì •ë¶€ê°€ ë°œí‘œí•˜ëŠ” ë¬¼ê°€ìƒìŠ¹ë¥ ê³¼ ì†Œë¹„ìê°€ ëŠë¼ëŠ” ë¬¼ê°€ìƒìŠ¹ë¥ ì€ ë‹¤ë¥´ë‹¤.")  
# ì •ê·œ í‘œí˜„ì‹ì„ í†µí•´ ì˜¨ì ì„ ì œê±°í•˜ëŠ” ì •ì œ ì‘ì—…ì…ë‹ˆë‹¤.  
token=okt.morphs(token)  
# OKT í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ í†µí•´ í† í°í™” ì‘ì—…ì„ ìˆ˜í–‰í•œ ë’¤ì—, tokenì—ë‹¤ê°€ ë„£ìŠµë‹ˆë‹¤.  

word2index={}  
bow=[]  
for voca in token:  
         if voca not in word2index.keys():  
             word2index[voca]=len(word2index)  
# tokenì„ ì½ìœ¼ë©´ì„œ, word2indexì— ì—†ëŠ” (not in) ë‹¨ì–´ëŠ” ìƒˆë¡œ ì¶”ê°€í•˜ê³ , ì´ë¯¸ ìˆëŠ” ë‹¨ì–´ëŠ” ë„˜ê¹ë‹ˆë‹¤.   
             bow.insert(len(word2index)-1,1)
# BoW ì „ì²´ì— ì „ë¶€ ê¸°ë³¸ê°’ 1ì„ ë„£ì–´ì¤ë‹ˆë‹¤. ë‹¨ì–´ì˜ ê°œìˆ˜ëŠ” ìµœì†Œ 1ê°œ ì´ìƒì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.  
         else:
            index=word2index.get(voca)
# ì¬ë“±ì¥í•˜ëŠ” ë‹¨ì–´ì˜ ì¸ë±ìŠ¤ë¥¼ ë°›ì•„ì˜µë‹ˆë‹¤.
            bow[index]=bow[index]+1
# ì¬ë“±ì¥í•œ ë‹¨ì–´ëŠ” í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ì˜ ìœ„ì¹˜ì— 1ì„ ë”í•´ì¤ë‹ˆë‹¤. (ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ ì„¸ëŠ” ê²ƒì…ë‹ˆë‹¤.)  

print(word2index)  
('ì •ë¶€': 0, 'ê°€': 1, 'ë°œí‘œ': 2, 'í•˜ëŠ”': 3, 'ë¬¼ê°€ìƒìŠ¹ë¥ ': 4, 'ê³¼': 5, 'ì†Œë¹„ì': 6, 'ëŠë¼ëŠ”': 7, 'ì€': 8, 'ë‹¤ë¥´ë‹¤': 9)  

bow  
[1, 2, 1, 1, 2, 1, 1, 1, 1, 1]
```

---

### 2. Bag of Words ì˜ ë‹¤ë¥¸ ì˜ˆì œë“¤

BoWì— ìˆì–´ì„œ **ì¤‘ìš”**í•œ ê²ƒì€ **ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„**

**ë‹¨ì–´ì˜ ìˆœì„œ**. ì¦‰, ì¸ë±ìŠ¤ì˜ ìˆœì„œëŠ” ì „í˜€ **ìƒê´€ì—†ë‹¤!**

ex) 

  ë¬¸ì„œ 2: ì†Œë¹„ìëŠ” ì£¼ë¡œ ì†Œë¹„í•˜ëŠ” ìƒí’ˆì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¼ê°€ìƒìŠ¹ë¥ ì„ ëŠë‚€ë‹¤.

```python
('ì†Œë¹„ì': 0, 'ëŠ”': 1, 'ì£¼ë¡œ': 2, 'ì†Œë¹„': 3, 'í•˜ëŠ”': 4, 'ìƒí’ˆ': 5, 'ì„': 6, 'ê¸°ì¤€': 7, 'ìœ¼ë¡œ': 8, 'ë¬¼ê°€ìƒìŠ¹ë¥ ': 9, 'ëŠë‚€ë‹¤': 10)  
[1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]
```

ë¬¸ì„œ 1ê³¼ ë¬¸ì„œ 2ë¥¼ í•©ì³ 

  ë¬¸ì„œ3: ì •ë¶€ê°€ ë°œí‘œí•˜ëŠ” ë¬¼ê°€ìƒìŠ¹ë¥ ê³¼ ì†Œë¹„ìê°€ ëŠë¼ëŠ” ë¬¼ê°€ìƒìŠ¹ë¥ ì€ ë‹¤ë¥´ë‹¤. ì†Œë¹„ìëŠ” ì£¼ë¡œ ì†Œë¹„í•˜ëŠ” ìƒí’ˆì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¼ê°€ìƒìŠ¹ë¥ ì„ ëŠë‚€ë‹¤.

```python
('ì •ë¶€': 0, 'ê°€': 1, 'ë°œí‘œ': 2, 'í•˜ëŠ”': 3, 'ë¬¼ê°€ìƒìŠ¹ë¥ ': 4, 'ê³¼': 5, 'ì†Œë¹„ì': 6, 'ëŠë¼ëŠ”': 7, 'ì€': 8, 'ë‹¤ë¥´ë‹¤': 9, 'ëŠ”': 10, 'ì£¼ë¡œ': 11, 'ì†Œë¹„': 12, 'ìƒí’ˆ': 13, 'ì„': 14, 'ê¸°ì¤€': 15, 'ìœ¼ë¡œ': 16, 'ëŠë‚€ë‹¤': 17)  
[1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1]
```

BoWëŠ” ì¢…ì¢… ì—¬ëŸ¬ ë¬¸ì„œì˜ ë‹¨ì–´ ì§‘í•©ì„ í•©ì¹œ ë’¤ì—, í•´ë‹¹ ë‹¨ì–´ ì§‘í•©ì— ëŒ€í•œ ê° ë¬¸ì„œì˜ BoWë¥¼ êµ¬í•˜ê¸°ë„ í•œë‹¤. 

ë¬¸ì„œ3ì— ëŒ€í•œ ë‹¨ì–´ ì§‘í•©ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì„œ1, ë¬¸ì„œ2ì˜ BoWë¥¼ ë§Œë“ ë‹¤ê³  í•œë‹¤ë©´

```python
ë¬¸ì„œ3 ë‹¨ì–´ ì§‘í•©ì— ëŒ€í•œ ë¬¸ì„œ1 BoW : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]  
ë¬¸ì„œ3 ë‹¨ì–´ ì§‘í•©ì— ëŒ€í•œ ë¬¸ì„œ2 BoW : [0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1]
```

BoWëŠ” ê° ë‹¨ì–´ê°€ ë“±ì¥í•œ íšŸìˆ˜ë¥¼ ìˆ˜ì¹˜í™” í•˜ëŠ” í…ìŠ¤íŠ¸ í‘œí˜„ ë°©ë²•ì´ê¸° ë•Œë¬¸ì—, 
ì£¼ë¡œ ì–´ë–¤ ë‹¨ì–´ê°€ ì–¼ë§ˆë‚˜ ë“±ì¥ í–ˆëŠ”ì§€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 
ë¬¸ì„œê°€ ì–´ë–¤ ì„±ê²©ì˜ ë¬¸ì„œ ì¸ì§€ë¥¼ íŒë‹¨í•˜ëŠ” ì‘ì—…ì— ì“°ì¸ë‹¤.
ì¦‰, ë¶„ë¥˜ ë¬¸ì œë‚˜ ì—¬ëŸ¬ ë¬¸ì„œ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ëŠ” ë¬¸ì œì— ì£¼ë¡œ ì“°ì¸ë‹¤. 

ex)
'ë¯¸ë¶„', 'ë°©ì •ì‹', 'ë¶€ë“±ì‹'ê³¼ ê°™ì€ ë‹¨ì–´ê°€ ìì£¼ ë“±ì¥í•œë‹¤ë©´ 
ìˆ˜í•™ ê´€ë ¨ ë¬¸ì„œë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆë‹¤.
'ë‹¬ë¦¬ê¸°', 'ì²´ë ¥', 'ê·¼ë ¥'ê³¼ ê°™ì€ ë‹¨ì–´ê°€ ìì£¼ ë“±ì¥í•˜ë©´ 
í•´ë‹¹ ë¬¸ì„œë¥¼ ì²´ìœ¡ ê´€ë ¨ ë¬¸ì„œë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆë‹¤. 

---

### 3. CounVectorizer í´ë˜ìŠ¤ë¡œ BoW ë§Œë“¤ê¸°

ì‚¬ì´í‚· ëŸ°ì—ì„œ ë‹¨ì–´ì˜ ë¹ˆë„ë¥¼ Countí•˜ì—¬ Vectorë¡œ ë§Œë“œëŠ” **CountVectorizer** í´ë˜ìŠ¤ ì§€ì›

```python
from sklearn.feature_extraction.text import CountVectorizer
corpus = ['you know I want your love. because I love you.']
vector = CountVectorizer()
print(vector.fit_transform(corpus).toarray()) # ì½”í¼ìŠ¤ë¡œë¶€í„° ê° ë‹¨ì–´ì˜ ë¹ˆë„ ìˆ˜ë¥¼ ê¸°ë¡í•œë‹¤.
print(vector.vocabulary_) # ê° ë‹¨ì–´ì˜ ì¸ë±ìŠ¤ê°€ ì–´ë–»ê²Œ ë¶€ì—¬ë˜ì—ˆëŠ”ì§€ë¥¼ ë³´ì—¬ì¤€ë‹¤.

[[1 1 2 1 2 1]]
{'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}
```

'I' ê°€ ì‚¬ë¼ì§„ ì´ìœ ?

CountVectorizerê°€ ê¸°ë³¸ì ìœ¼ë¡œ ê¸¸ì´ê°€ 2ì´ìƒì¸ ë¬¸ìì— ëŒ€í•´ì„œë§Œ í† í°ìœ¼ë¡œ ì¸ì‹í•˜ê¸° ë•Œë¬¸

ì£¼ì˜! 
ë‹¨ì§€ ë„ì–´ì“°ê¸°ë§Œì„ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ìë¥´ëŠ” ë‚®ì€ ìˆ˜ì¤€ì˜ í† í°í™”ë¥¼ ì§„í–‰í•˜ê³  BoWë¥¼ ë§Œë“ ë‹¤

í•œêµ­ì–´ì— CountVectorizerë¥¼ ì ìš©í•˜ë©´, ì¡°ì‚¬ ë“±ì˜ ì´ìœ ë¡œ ì œëŒ€ë¡œ BoWê°€ ë§Œë“¤ì–´ì§€ì§€ ì•ŠìŒì„ ì˜ë¯¸

---

### 4. ë¶ˆìš©ì–´ë¥¼ ì œê±°í•œ BoW ë§Œë“¤ê¸°

BoWë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì€ ê·¸ ë¬¸ì„œì—ì„œ ê° ë‹¨ì–´ê°€ ì–¼ë§ˆë‚˜ ìì£¼ ë“±ì¥í–ˆëŠ”ì§€ë¥¼ ë³´ê² ë‹¤ëŠ” ê²ƒ

BoWë¥¼ ë§Œë“¤ ë•Œ ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ëŠ” ì¼ â†’ ìì—°ì–´ ì²˜ë¦¬ì˜ ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ì„œ ì„ íƒí•  ìˆ˜ ìˆëŠ” ì „ì²˜ë¦¬ ê¸°ë²•

**1) ì‚¬ìš©ìê°€ ì§ì ‘ ì •ì˜í•œ ë¶ˆìš©ì–´ ì‚¬ìš©**

```python
from sklearn.feature_extraction.text import CountVectorizer

text=["Family is not an important thing. It's everything."]
vect = CountVectorizer(stop_words=["the", "a", "an", "is", "not"])
print(vect.fit_transform(text).toarray()) 
print(vect.vocabulary_)
[[1 1 1 1 1]]
{'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}
```

**2) CountVectorizer ì—ì„œ ì œê³µí•˜ëŠ” ìì²´ ë¶ˆìš©ì–´ ì‚¬ìš©**

```python
from sklearn.feature_extraction.text import CountVectorizer

text=["Family is not an important thing. It's everything."]
vect = CountVectorizer(stop_words="english")
print(vect.fit_transform(text).toarray())
print(vect.vocabulary_)
[[1 1 1]]
{'family': 0, 'important': 1, 'thing': 2}
```

**3) NLTKì—ì„œ ì§€ì›í•˜ëŠ” ë¶ˆìš©ì–´ ì‚¬ìš©**

```python
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords

text=["Family is not an important thing. It's everything."]
sw = stopwords.words("english")
vect = CountVectorizer(stop_words =sw)
print(vect.fit_transform(text).toarray()) 
print(vect.vocabulary_)
[[1 1 1 1]]
{'family': 1, 'important': 2, 'thing': 3, 'everything': 0}
```

---

## ë¬¸ì„œ ë‹¨ì–´ í–‰ë ¬ (Document - Term Matrix, DTM)

ê° ë¬¸ì„œì— ëŒ€í•œ BoW í‘œí˜„ ë°©ë²•ì„ ê·¸ëŒ€ë¡œ ê°–ê³  ì™€ì„œ,

**ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œë“¤ì˜ BoWë“¤ì„ ê²°í•©**í•œ í‘œí˜„ ë°©ë²•

### 1. ë¬¸ì„œ ë‹¨ì–´ í–‰ë ¬ (Document - Term Matrix, DTM) ì˜ í‘œê¸°ë²•

ë¬¸ì„œ ë‹¨ì–´ í–‰ë ¬(Document-Term Matrix, DTM)ì´ë€?

ë‹¤ìˆ˜ì˜ ë¬¸ì„œì—ì„œ ë“±ì¥í•˜ëŠ” ê° ë‹¨ì–´ë“¤ì˜ ë¹ˆë„ë¥¼ í–‰ë ¬ë¡œ í‘œí˜„í•œ ê²ƒ 

ex) 

ë¬¸ì„œ1 : ë¨¹ê³  ì‹¶ì€ ì‚¬ê³¼

ë¬¸ì„œ2 : ë¨¹ê³  ì‹¶ì€ ë°”ë‚˜ë‚˜

ë¬¸ì„œ3 : ê¸¸ê³  ë…¸ë€ ë°”ë‚˜ë‚˜ ë°”ë‚˜ë‚˜

ë¬¸ì„œ4 : ì €ëŠ” ê³¼ì¼ì´ ì¢‹ì•„ìš”

<img src = "/image/Untitled 9.png" width = "600px">

---

### 2. ë¬¸ì„œ ë‹¨ì–´ í–‰ë ¬ì˜ í•œê³„

**1) í¬ì†Œ í‘œí˜„ (Sparse representation)**

**ì›-í•« ë²¡í„°ì˜ ë‹¨ì **ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ 

ê° ë¬¸ì„œ ë²¡í„°ì˜ ì°¨ì›ì€ ì›-í•« ë²¡í„°ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì „ì²´ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°ë¥¼ ê°€ì§„ë‹¤.

ë˜í•œ ë§ì€ ë¬¸ì„œ ë²¡í„°ê°€ ëŒ€ë¶€ë¶„ì˜ ê°’ì´ 0ì„ ê°€ì§ˆ ìˆ˜ë„ ìˆë‹¤.

**í¬ì†Œ ë²¡í„°(sparse vector)**  :  ëŒ€ë¶€ë¶„ì˜ ê°’ì´ 0ì¸ í‘œí˜„ 

â†’ ì „ì²˜ë¦¬ë¥¼ í†µí•´ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ì¼ì€ BoW í‘œí˜„ì„ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì—ì„œ ì¤‘ìš”!

ex) êµ¬ë‘ì , ë¹ˆë„ìˆ˜ ë‚®ì€ ë‹¨ì–´, ë¶ˆìš©ì–´ë¥¼ ì œê±° / ì–´ê°„ì´ë‚˜ í‘œì œì–´ ì¶”ì¶œë¡œ ë‹¨ì–´ë¥¼ ì •ê·œí™”

**2) ë‹¨ìˆœ ë¹ˆë„ ìˆ˜ ê¸°ë°˜ ì ‘ê·¼**

ë¶ˆìš©ì–´ì¸ theëŠ” ì–´ë–¤ ë¬¸ì„œì´ë“  ìì£¼ ë“±ì¥í•  ìˆ˜ ë°–ì— ì—†ë‹¤.

ë™ì¼í•˜ê²Œ theê°€ ë¹ˆë„ìˆ˜ê°€ ë†’ë‹¤ê³  í•´ì„œ ì´ ë¬¸ì„œë“¤ì´ ìœ ì‚¬í•œ ë¬¸ì„œë¼ê³  íŒë‹¨í•´ì„œëŠ” ì•ˆ ëœë‹¤!

ê° ë¬¸ì„œì—ëŠ” ì¤‘ìš”í•œ ë‹¨ì–´ì™€ ë¶ˆí•„ìš”í•œ ë‹¨ì–´ë“¤ì´ í˜¼ì¬ ë˜ì–´ìˆë‹¤. 

DTMì— ë¶ˆìš©ì–´ì™€ ì¤‘ìš”í•œ ë‹¨ì–´ì— ëŒ€í•´ **ê°€ì¤‘ì¹˜ë¥¼ ì¤„ ìˆ˜ ìˆëŠ” ë°©ë²•**ì€ ì—†ì„ê¹Œ? 

ì´ë¥¼ ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ TF-IDFì…ë‹ˆë‹¤.

---

## TF-IDF (Term Frequency-Inverse Document Freqency)

TF-IDFë¥¼ ì‚¬ìš©í•˜ë©´, 

ê¸°ì¡´ì˜ DTMì„ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ ë³´ë‹¤ ë” ë§ì€ ì •ë³´ë¥¼ ê³ ë ¤í•˜ì—¬ ë¬¸ì„œë“¤ì„ ë¹„êµí•  ìˆ˜ ìˆë‹¤.

ì£¼ì˜! TF-IDFê°€ DTMë³´ë‹¤ í•­ìƒ ì„±ëŠ¥ì´ ë›°ì–´ë‚˜ì§„ ì•Šë‹¤!

### 1. TF - IDF (ë‹¨ì–´ ë¹ˆë„- ì—­ ë¬¸ì„œ ë¹ˆë„, Term Freqency - Inverse Document Frequency)

TF-IDF  :    **TFì™€ IDFë¥¼ ê³±í•œ ê°’**

ìš°ì„  DTMì„ ë§Œë“  í›„, TF-IDF ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬

ì£¼ë¡œ ë¬¸ì„œì˜ ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ëŠ” ì‘ì—…, ê²€ìƒ‰ ì‹œìŠ¤í…œì—ì„œ ê²€ìƒ‰ ê²°ê³¼ì˜ ì¤‘ìš”ë„ë¥¼ ì •í•˜ëŠ” ì‘ì—…, 

ë¬¸ì„œ ë‚´ì—ì„œ íŠ¹ì • ë‹¨ì–´ì˜ ì¤‘ìš”ë„ë¥¼ êµ¬í•˜ëŠ” ì‘ì—… ë“±ì— ì“°ì¼ ìˆ˜ ìˆë‹¤.

**1) tf(d,t) : íŠ¹ì • ë¬¸ì„œ dì—ì„œì˜ íŠ¹ì • ë‹¨ì–´ tì˜ ë“±ì¥ íšŸìˆ˜**

**2) df(t) : íŠ¹ì • ë‹¨ì–´ tê°€ ë“±ì¥í•œ ë¬¸ì„œì˜ ìˆ˜**

**3) idf(d, t) : df(t)ì— ë°˜ë¹„ë¡€í•˜ëŠ” ìˆ˜**

$$idf(d, t) = log(\frac{n}{1+df(t)})$$

**log**ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ì„ ë•Œ

â†’ ì´ ë¬¸ì„œì˜ ìˆ˜ nì´ ì»¤ì§ˆ ìˆ˜ë¡, IDFì˜ ê°’ì€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì»¤ì§„ë‹¤.

â†’ í¬ê·€ ë‹¨ì–´ë“¤ì— ì—„ì²­ë‚œ ê°€ì¤‘ì¹˜ê°€ ë¶€ì—¬ë  ìˆ˜ ìˆë‹¤.

**ë¶„ëª¨ì— 1ì„ ë”í•´ì£¼ëŠ” ì´ìœ **: íŠ¹ì • ë‹¨ì–´ê°€ ì „ì²´ ë¬¸ì„œì—ì„œ ë“±ì¥í•˜ì§€ ì•Šì„ ê²½ìš°ì— ë¶„ëª¨ê°€ 0ì´ ë˜ëŠ” ìƒí™©ì„ ë°©ì§€í•˜ê¸° ìœ„í•¨

**ëª¨ë“  ë¬¸ì„œ**ì—ì„œ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” **ì¤‘ìš”ë„ê°€ ë‚®**ë‹¤ê³  íŒë‹¨
**íŠ¹ì • ë¬¸ì„œ**ì—ì„œë§Œ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” **ì¤‘ìš”ë„ê°€ ë†’ë‹¤**ê³  íŒë‹¨

TF-IDF ê°’ì´ ë‚®ìœ¼ë©´ ì¤‘ìš”ë„ê°€ ë‚®ì€ ê²ƒ

ë¶ˆìš©ì–´ì˜ TF-IDFì˜ ê°’ì€ ë‹¤ë¥¸ ë‹¨ì–´ì˜ TF-IDFì— ë¹„í•´ì„œ ë‚®ë‹¤.

### 2. íŒŒì´ì¬ìœ¼ë¡œ TF-IDF ì§ì ‘ êµ¬í˜„í•˜ê¸°

```python
import pandas as pd # ë°ì´í„°í”„ë ˆì„ ì‚¬ìš©ì„ ìœ„í•´
from math import log # IDF ê³„ì‚°ì„ ìœ„í•´

docs = [
  'ë¨¹ê³  ì‹¶ì€ ì‚¬ê³¼',
  'ë¨¹ê³  ì‹¶ì€ ë°”ë‚˜ë‚˜',
  'ê¸¸ê³  ë…¸ë€ ë°”ë‚˜ë‚˜ ë°”ë‚˜ë‚˜',
  'ì €ëŠ” ê³¼ì¼ì´ ì¢‹ì•„ìš”'
] 
vocab = list(set(w for doc in docs for w in doc.split()))
vocab.sort()

N = len(docs) # ì´ ë¬¸ì„œì˜ ìˆ˜

def tf(t, d):
    return d.count(t)

def idf(t):
    df = 0
    for doc in docs:
        df += t in doc
    return log(N/(df + 1))

def tfidf(t, d):
    return tf(t,d)* idf(t)
```

```python
result = []
for i in range(N): # ê° ë¬¸ì„œì— ëŒ€í•´ì„œ ì•„ë˜ ëª…ë ¹ì„ ìˆ˜í–‰
    result.append([])
    d = docs[i]
    for j in range(len(vocab)):
        t = vocab[j]        
        result[-1].append(tf(t, d))

tf_ = pd.DataFrame(result, columns = vocab)
tf_
```

<img src = "/image/Untitled 10.png" width = "600px">

```python
result = []
for j in range(len(vocab)):
    t = vocab[j]
    result.append(idf(t))

idf_ = pd.DataFrame(result, index = vocab, columns = ["IDF"])
idf_
```

<img src = "/image/Untitled 11.png" width = "600px">

```python
result = []
for i in range(N):
    result.append([])
    d = docs[i]
    for j in range(len(vocab)):
        t = vocab[j]

        result[-1].append(tfidf(t,d))

tfidf_ = pd.DataFrame(result, columns = vocab)
tfidf_
```

<img src = "/image/Untitled 12.png" width = "600px">

ì—¬ì „íˆ ë¬¸ì œì ì´ ì¡´ì¬

logí•­ì˜ ë¶„ìì™€ ë¶„ëª¨ì˜ ê°’ì´ ê°™ì•„ì§ˆ ìˆ˜ ìˆë‹¤. 

â†’ logì˜ ì§„ìˆ˜ê°’ì´ 1ì´ ë˜ë©´ì„œ idf(d,t)ì˜ ê°’ì´ 0ì´ ë¨ì„ ì˜ë¯¸

ê·¸ë˜ì„œ ì‹¤ì œ êµ¬í˜„ì²´ëŠ” logí•­ì— 1ì„ ë”í•´ IDFê°€ ìµœì†Œ 1ì´ìƒì˜ ê°’ì„ ê°€ì§€ë„ë¡ í•œë‹¤.

$$idf(d, t) = log(n/(df(t)+1)) + 1$$

---

### 3. ì‚¬ì´í‚·ëŸ°ì„ ì´ìš©í•œ DTMê³¼ TF-IDF ì‹¤ìŠµ

```python
from sklearn.feature_extraction.text import CountVectorizer
corpus = [
    'you know I want your love',
    'I like you',
    'what should I do ',    
]
vector = CountVectorizer()
print(vector.fit_transform(corpus).toarray()) # ì½”í¼ìŠ¤ë¡œë¶€í„° ê° ë‹¨ì–´ì˜ ë¹ˆë„ ìˆ˜ë¥¼ ê¸°ë¡í•œë‹¤.
print(vector.vocabulary_) # ê° ë‹¨ì–´ì˜ ì¸ë±ìŠ¤ê°€ ì–´ë–»ê²Œ ë¶€ì—¬ë˜ì—ˆëŠ”ì§€ë¥¼ ë³´ì—¬ì¤€ë‹¤.

[[0 1 0 1 0 1 0 1 1]
 [0 0 1 0 0 0 0 1 0]
 [1 0 0 0 1 0 1 0 0]]
{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}
```

ì‚¬ì´í‚·ëŸ°ì€ TF-IDFë¥¼ ìë™ ê³„ì‚°í•´ì£¼ëŠ” **TfidfVectorizer** ì œê³µí•œë‹¤.

ì‚¬ì´í‚·ëŸ°ì˜ TF-IDFëŠ” ìœ„ì—ì„œ ë°°ì› ë˜ ë³´í¸ì ì¸ TF-IDF ì‹ì—ì„œ ì¢€ ë” ì¡°ì •ëœ ë‹¤ë¥¸ ì‹ì„ ì‚¬ìš©

(ë¡œê·¸í•­ì˜ ë¶„ìì— 1ì„ ë”í•´ì£¼ë©°, ë¡œê·¸í•­ì— 1ì„ ë”í•´ì£¼ê³ , TF-IDFì— L2 ì •ê·œí™”ë¼ëŠ” ë°©ë²•ìœ¼ë¡œ ê°’ì„ ì¡°ì •í•˜ëŠ” ë“±ì˜ ì°¨ì´)

```python
from sklearn.feature_extraction.text import TfidfVectorizer
corpus = [
    'you know I want your love',
    'I like you',
    'what should I do ',    
]
tfidfv = TfidfVectorizer().fit(corpus)
print(tfidfv.transform(corpus).toarray())
print(tfidfv.vocabulary_)

[[0.         0.46735098 0.         0.46735098 0.         0.46735098 0.         0.35543247 0.46735098]
 [0.         0.         0.79596054 0.         0.         0.         0.         0.60534851 0.        ]
 [0.57735027 0.         0.         0.         0.57735027 0.         0.57735027 0.         0.        ]]
{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}
```

---

# ë¬¸ì„œ ìœ ì‚¬ë„ (Document Similarity)

ìì—°ì–´ ì²˜ë¦¬ì˜ ì£¼ìš” ì£¼ì œ ì¤‘ í•˜ë‚˜

ê° ë¬¸ì„œì˜ ë‹¨ì–´ë“¤ì„ ì–´ë–¤ ë°©ë²•ìœ¼ë¡œ ìˆ˜ì¹˜í™”í•˜ì—¬ í‘œí˜„í–ˆëŠ”ì§€**(DTM, Word2Vec ë“±)**, 

ë¬¸ì„œ ê°„ì˜ ë‹¨ì–´ë“¤ì˜ ì°¨ì´ë¥¼ ì–´ë–¤ ë°©ë²•**(ìœ í´ë¦¬ë“œ ê±°ë¦¬, ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë“±)**ìœ¼ë¡œ ê³„ì‚°í–ˆëŠ”ì§€ì— ë‹¬ë ¤ìˆë‹¤.

## ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (Cosine Similarity)

### 1. ì½”ì‚¬ì¸ ìœ ì‚¬ë„

ë‘ ë²¡í„° ê°„ì˜ **ì½”ì‚¬ì¸ ê°ë„**ë¥¼ ì´ìš©

<img src = "/image/Untitled 13.png" width = "600px">

**-1 ì´ìƒ 1 ì´í•˜ì˜ ê°’**ì„ ê°€ì§€ë©° ê°’ì´ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬ë„ê°€ ë†’ë‹¤ê³  íŒë‹¨

$$similarity=cos(Î˜)=\frac{Aâ‹…B}{||A||\ ||B||}=\frac{\sum_{i=1}^{n}{A_{i}Ã—B_{i}}}{\sqrt{\sum_{i=1}^{n}(A_{i})^2}Ã—\sqrt{\sum_{i=1}^{n}(B_{i})^2}}$$

ex) 

ë¬¸ì„œ1 : ì €ëŠ” ì‚¬ê³¼ ì¢‹ì•„ìš”

ë¬¸ì„œ2 : ì €ëŠ” ë°”ë‚˜ë‚˜ ì¢‹ì•„ìš”

ë¬¸ì„œ3 : ì €ëŠ” ë°”ë‚˜ë‚˜ ì¢‹ì•„ìš” ì €ëŠ” ë°”ë‚˜ë‚˜ ì¢‹ì•„ìš”

ì„¸ ë¬¸ì„œì— ëŒ€í•œ DTM 

<img src = "/image/Untitled 14.png" width = "600px">

```python
from numpy import dot
from numpy.linalg import norm
import numpy as np
def cos_sim(A, B):
       return dot(A, B)/(norm(A)*norm(B))

doc1=np.array([0,1,1,1])
doc2=np.array([1,0,1,1])
doc3=np.array([2,0,2,2])

print(cos_sim(doc1, doc2)) #ë¬¸ì„œ1ê³¼ ë¬¸ì„œ2ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„
print(cos_sim(doc1, doc3)) #ë¬¸ì„œ1ê³¼ ë¬¸ì„œ3ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„
print(cos_sim(doc2, doc3)) #ë¬¸ì„œ2ê³¼ ë¬¸ì„œ3ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„

0.67
0.67
1.00
```

ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” ë¬¸ì„œì˜ ê¸¸ì´ê°€ ë‹¤ë¥¸ ìƒí™©ì—ì„œ ë¹„êµì  ê³µì •í•œ ë¹„êµë¥¼ í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤€ë‹¤.

ë²¡í„°ì˜ í¬ê¸°ê°€ ì•„ë‹ˆë¼ ë²¡í„°ì˜ ë°©í–¥(íŒ¨í„´)ì— ì´ˆì ì„ ë‘ê¸° ë•Œë¬¸!

---

### 2. ìœ ì‚¬ë„ë¥¼ ì´ìš©í•œ ì¶”ì²œ ì‹œìŠ¤í…œ êµ¬í˜„í•˜ê¸°

[https://www.kaggle.com/rounakbanik/the-movies-dataset](https://www.kaggle.com/rounakbanik/the-movies-dataset)

movies_metadata.csv     ì´ 24ê°œì˜ ì—´ì„ ê°€ì§„ 45,466ê°œì˜ ìƒ˜í”Œë¡œ êµ¬ì„±

```python
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
data = pd.read_csv('í˜„ì¬ movies_metadata.csvì˜ íŒŒì¼ ê²½ë¡œ', low_memory=False)
data.head(2)
```

<img src = "/image/Untitled 15.png" width = "600px">

```python
data = data.head(20000)
```

tf-idf í•  ë•Œ ë°ì´í„°ì— Null ê°’ ë“¤ì–´ìˆìœ¼ë©´ ì—ëŸ¬ ë°œìƒ â†’ Null ëŒ€ì‹  ë„£ê³ ì í•˜ëŠ” ê°’ìœ¼ë¡œ ëŒ€ì²´

```python
data['overview'].isnull().sum()
135

# overviewì—ì„œ Null ê°’ì„ ê°€ì§„ ê²½ìš°ì—ëŠ” Null ê°’ì„ ì œê±°
data['overview'] = data['overview'].fillna('')

tfidf = TfidfVectorizer(stop_words='english')
# overviewì— ëŒ€í•´ì„œ tf-idf ìˆ˜í–‰
tfidf_matrix = tfidf.fit_transform(data['overview'])
print(tfidf_matrix.shape)

(20000, 47487)
# 20,000ê°œì˜ ì˜í™”ë¥¼ í‘œí˜„í•˜ê¸°ìœ„í•´ ì´ 47,487ê°œì˜ ë‹¨ì–´ê°€ ì‚¬ìš©ë˜ì—ˆë‹¤.
```

ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°”ë¡œ ë¬¸ì„œì˜ ìœ ì‚¬ë„ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.

```python
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)
# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ êµ¬í•œë‹¤.

indices = pd.Series(data['title'].drop_duplicates().index, index=data['title'].drop_duplicates())
print(indices.head())

title
Toy Story                      0
Jumanji                        1
Grumpier Old Men               2
Waiting to Exhale              3
Father of the Bride Part II    4
dtype: int64

idx = indices['Father of the Bride Part II']
print(idx)
4

def get_recommendations(title, cosine_sim=cosine_sim):
    
    idx = indices[title]

    sim_scores = list(enumerate(cosine_sim[idx]))

    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    sim_scores = sim_scores[1:11]

    movie_indices = [i[0] for i in sim_scores]

    return data['title'].iloc[movie_indices]

get_recommendations('The Dark Knight Rises')
12481                            The Dark Knight
150                               Batman Forever
1328                              Batman Returns
15511                 Batman: Under the Red Hood
585                                       Batman
9230          Batman Beyond: Return of the Joker
18035                           Batman: Year One
19792    Batman: The Dark Knight Returns, Part 1
3095                Batman: Mask of the Phantasm
10122                              Batman Begins
Name: title, dtype: object
```

---

## ì—¬ëŸ¬ê°€ì§€ ìœ ì‚¬ë„ ê¸°ë²•

ë¬¸ì„œì˜ ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œëŠ” ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì™¸ì—ë„ ì—¬ëŸ¬ê°€ì§€ ë°©ë²•ë“¤ì´ ìˆë‹¤.

### 1. ìœ í´ë¦¬ë“œ ê±°ë¦¬ (Euclidean distance)

ìì¹´ë“œ ìœ ì‚¬ë„ë‚˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë§Œí¼, ìœ ìš©í•œ ë°©ë²•ì€ ì•„ë‹ˆë‹¤.

ë‹¤ì°¨ì› ê³µê°„ì—ì„œ ë‘ê°œì˜ ì  pì™€ qê°€ ê°ê° p=(p1,p2,p3,...,pn)ê³¼ q=(q1,q2,q3,...,qn)ì˜ ì¢Œí‘œë¥¼ ê°€ì§ˆ ë•Œ ë‘ ì  ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ê³„ì‚°í•˜ëŠ” ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³µì‹

$$\sqrt{(q_{1}-p_{1})^{2}+(q_{2}-p_{2})^{2}+\ ...\ +(q_{n}-p_{n})^{2}}=\sqrt{\sum_{i=1}^{n}(q_{i}-p_{i})^{2}}$$

ex)

<img src = "/image/Untitled 16.png" width = "600px">

<img src = "/image/Untitled 17.png" width = "600px">

ì´ë•Œ ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì„œQì— ëŒ€í•´ì„œ ë¬¸ì„œ1, ë¬¸ì„œ2, ë¬¸ì„œ3 ì¤‘ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ëŠ”ë‹¤ë©´

```python
import numpy as np
def dist(x,y):   
    return np.sqrt(np.sum((x-y)**2))

doc1 = np.array((2,3,0,1))
doc2 = np.array((1,2,3,1))
doc3 = np.array((2,1,2,2))
docQ = np.array((1,1,0,1))

print(dist(doc1,docQ))
print(dist(doc2,docQ))
print(dist(doc3,docQ))
2.23606797749979
3.1622776601683795
2.449489742783178
```

ìœ í´ë¦¬ë“œ ê±°ë¦¬ì˜ ê°’ì´ ê°€ì¥ ì‘ë‹¤ëŠ” ê²ƒì€, 

â†’ ë¬¸ì„œ ê°„ì˜ ê±°ë¦¬ê°€ ê°€ì¥ ê°€ê¹ë‹¤ëŠ” ê²ƒ.

 ì¦‰, ë¬¸ì„œ1ì´ ë¬¸ì„œQì™€ ê°€ì¥ ìœ ì‚¬í•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.

---

### 2. ìì¹´ë“œ ìœ ì‚¬ë„ (Jaccard similarity)

ìì¹´ë“œ ìœ ì‚¬ë„(jaccard similarity)ì˜ ì•„ì´ë””ì–´

â†’    í•©ì§‘í•©ì—ì„œ êµì§‘í•©ì˜ ë¹„ìœ¨ì„ êµ¬í•œë‹¤ë©´ ë‘ ì§‘í•© Aì™€ Bì˜ ìœ ì‚¬ë„ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤!

$$J(A,B)=\frac{|Aâˆ©B|}{|AâˆªB|}=\frac{|Aâˆ©B|}{|A|+|B|-|Aâˆ©B|}$$

0ê³¼ 1ì‚¬ì´ì˜ ê°’ì„ ê°–ëŠ”ë‹¤!

ë§Œì•½ ë‘ ì§‘í•©ì´ ë™ì¼í•˜ë©´ 1ì˜ ê°’ì„ ê°€ì§€ê³ , ë‘ ì§‘í•©ì˜ ê³µí†µ ì›ì†Œê°€ ì—†ë‹¤ë©´ 0ì˜ ê°’ì„ ê°–ëŠ”ë‹¤.

$$J(doc_{1},doc_{2})=\frac{doc_{1}âˆ©doc_{2}}{doc_{1}âˆªdoc_{2}}$$

```python
# ë‹¤ìŒê³¼ ê°™ì€ ë‘ ê°œì˜ ë¬¸ì„œê°€ ìˆìŠµë‹ˆë‹¤.
# ë‘ ë¬¸ì„œ ëª¨ë‘ì—ì„œ ë“±ì¥í•œ ë‹¨ì–´ëŠ” appleê³¼ banana 2ê°œ.
doc1 = "apple banana everyone like likey watch card holder"
doc2 = "apple banana coupon passport love you"

# í† í°í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
tokenized_doc1 = doc1.split()
tokenized_doc2 = doc2.split()

# í† í°í™” ê²°ê³¼ ì¶œë ¥
print(tokenized_doc1)
print(tokenized_doc2)

['apple', 'banana', 'everyone', 'like', 'likey', 'watch', 'card', 'holder']
['apple', 'banana', 'coupon', 'passport', 'love', 'you']

union = set(tokenized_doc1).union(set(tokenized_doc2))
print(union)
{'card', 'holder', 'passport', 'banana', 'apple', 'love', 'you', 'likey', 'coupon', 'like', 'watch', 'everyone'}

intersection = set(tokenized_doc1).intersection(set(tokenized_doc2))
print(intersection)
{'banana', 'apple'}

print(len(intersection)/len(union)) # 2ë¥¼ 12ë¡œ ë‚˜ëˆ”.
0.16666666666666666

# ìì¹´ë“œ ìœ ì‚¬ë„ì´ì, 
# ë‘ ë¬¸ì„œì˜ ì´ ë‹¨ì–´ ì§‘í•©ì—ì„œ ë‘ ë¬¸ì„œì—ì„œ ê³µí†µì ìœ¼ë¡œ ë“±ì¥í•œ ë‹¨ì–´ì˜ ë¹„ìœ¨ì´ë‹¤.
```

---

# í† í”½ ëª¨ë¸ë§ (Topic Modeling)

ê¸°ê³„ í•™ìŠµ ë° ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ í† í”½ì´ë¼ëŠ” ë¬¸ì„œ ì§‘í•©ì˜ ì¶”ìƒì ì¸ ì£¼ì œë¥¼ ë°œê²¬í•˜ê¸° ìœ„í•œ í†µê³„ì  ëª¨ë¸ ì¤‘ í•˜ë‚˜ë¡œ, í…ìŠ¤íŠ¸ ë³¸ë¬¸ì˜ ìˆ¨ê²¨ì§„ ì˜ë¯¸ êµ¬ì¡°ë¥¼ ë°œê²¬í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” í…ìŠ¤íŠ¸ ë§ˆì´ë‹ ê¸°ë²•

## ì ì¬ ì˜ë¯¸ ë¶„ì„ (Latent Semantic Analysis, LSA)

LSAëŠ” í† í”½ ëª¨ë¸ë§ ë¶„ì•¼ì— ì•„ì´ë””ì–´ë¥¼ ì œê³µí•œ ì•Œê³ ë¦¬ì¦˜ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤!

ë’¤ì— ë‚˜ì˜¤ëŠ” **LDA**ëŠ” **LSAì˜ ë‹¨ì **ì„ ê°œì„ í•´ í† í”½ ëª¨ë¸ë§ì— ë³´ë‹¤ ì í•©í•œ ì•Œê³ ë¦¬ì¦˜!!

BoWì— ê¸°ë°˜í•œ DTMì´ë‚˜ TF-IDFëŠ” **ë‹¨ì–´ì˜ ë¹ˆë„ ìˆ˜**ë¥¼ ì´ìš©!
â†’ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ê³ ë ¤í•˜ì§€ ëª»í•œë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤!

DTMì˜ ì ì¬ëœ(Latent) ì˜ë¯¸ë¥¼ ì´ëŒì–´ë‚´ëŠ” ë°©ë²• â†’ LSA ë˜ëŠ” LSI (ì ì¬ ì˜ë¯¸ ë¶„ì„)

LSA ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ **íŠ¹ì´ê°’ ë¶„í•´**ë¥¼ ì´í•´í•´ì•¼ í•œë‹¤!

### 1. íŠ¹ì´ê°’ ë¶„í•´ (Singular Value Decomposition, SVD)

ì‹¤ìˆ˜ ë²¡í„° ê³µê°„ì— í•œì •

Aê°€ m Ã— n í–‰ë ¬ì¼ ë•Œ, ë‹¤ìŒê³¼ ê°™ì´ 3ê°œì˜ í–‰ë ¬ì˜ ê³±ìœ¼ë¡œ ë¶„í•´(decomposition)

$$A=UÎ£V^\text{T}$$

$U: m Ã— m\ \text{ì§êµí–‰ë ¬}\ (AA^\text{T}=U(Î£Î£^\text{T})U^\text{T})$

$V: n Ã— n\ \text{ì§êµí–‰ë ¬}\ (A^\text{T}A=V(Î£^\text{T}Î£)V^\text{T})$

$Î£: m Ã— n\ \text{ì§ì‚¬ê° ëŒ€ê°í–‰ë ¬}$

SVDë¡œ ë‚˜ì˜¨ **ëŒ€ê° í–‰ë ¬ì˜ ëŒ€ê° ì›ì†Œì˜ ê°’**ì´  í–‰ë ¬ Aì˜ **íŠ¹ì´ê°’(singular value)**

SVDë¥¼ í†µí•´ ë‚˜ì˜¨ ëŒ€ê° í–‰ë ¬ Î£ì˜ ì¶”ê°€ì ì¸ ì„±ì§ˆ!

    â†’ íŠ¹ì´ê°’ë“¤ì´ **ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬**ë˜ì–´ ìˆë‹¤!

---

### 2. ì ˆë‹¨ëœ SVD (Truncated SVD)

LSAì˜ ê²½ìš° **í’€ SVD**ì˜ 3ê°œì˜ í–‰ë ¬ì—ì„œ **ì¼ë¶€ ë²¡í„°ë“¤ì„ ì‚­ì œ** ì‹œí‚¨ **ì ˆë‹¨ëœ SVD(truncated SVD)**ë¥¼ ì‚¬ìš©

<img src = "/image/Untitled 18.png" width = "600px">

ì ˆë‹¨ëœ SVDëŠ” ëŒ€ê° í–‰ë ¬ Î£ì˜ ëŒ€ê° ì›ì†Œì˜ ê°’ ì¤‘ì—ì„œ **ìƒìœ„ ê°’ tê°œ**ë§Œ ë‚¨ê²Œ ëœë‹¤.

Uí–‰ë ¬ê³¼ Ví–‰ë ¬ì˜ **tì—´**ê¹Œì§€ë§Œ ë‚¨ê¸´ë‹¤. 

ğŸ’¡ì—¬ê¸°ì„œ tëŠ” ìš°ë¦¬ê°€ ì°¾ê³ ì í•˜ëŠ” **í† í”½ì˜ ìˆ˜**ë¥¼ ë°˜ì˜í•œ **í•˜ì´í¼íŒŒë¼ë¯¸í„°!**

**të¥¼ í¬ê²Œ ì¡ìœ¼ë©´** ê¸°ì¡´ì˜ í–‰ë ¬ Aë¡œë¶€í„° ë‹¤ì–‘í•œ ì˜ë¯¸ë¥¼ ê°€ì ¸ê°ˆ ìˆ˜ ìˆë‹¤!

ê·¸ëŸ¬ë‚˜ ! **të¥¼ ì‘ê²Œ ì¡ì•„ì•¼ë§Œ** ë…¸ì´ì¦ˆë¥¼ ì œê±°í•  ìˆ˜ ìˆë‹¤! 

ì´ë ‡ê²Œ ì¼ë¶€ ë²¡í„°ë“¤ì„ ì‚­ì œí•˜ëŠ” ê²ƒ â†’ '**ë°ì´í„°ì˜ ì°¨ì›ì„ ì¤„ì¸ë‹¤!**'

í’€ SVD ë³´ë‹¤ ì§ê´€ì ìœ¼ë¡œ ê³„ì‚° ë¹„ìš©ì´ ë‚®ì•„ì§„ë‹¤!
ìƒëŒ€ì ìœ¼ë¡œ ì¤‘ìš”í•˜ì§€ ì•Šì€ ì •ë³´ë¥¼ ì‚­ì œí•˜ëŠ” íš¨ê³¼ê°€ ìˆë‹¤! 
    ì˜ìƒ ì²˜ë¦¬ ë¶„ì•¼: ë…¸ì´ì¦ˆë¥¼ ì œê±°í•œë‹¤ëŠ” ì˜ë¯¸
    ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼: ì„¤ëª…ë ¥ ë‚®ì€ ì •ë³´ë¥¼ ì‚­ì œ, ì„¤ëª…ë ¥ ë†’ì€ ì •ë³´ë¥¼ ë‚¨ê¸´ë‹¤ëŠ” ì˜ë¯¸

ê¸°ì¡´ì˜ í–‰ë ¬ì—ì„œëŠ” ë“œëŸ¬ë‚˜ì§€ ì•Šì•˜ë˜ ì‹¬ì¸µì ì¸ ì˜ë¯¸ë¥¼ í™•ì¸í•  ìˆ˜ ìˆê²Œ ëœë‹¤!

---

### 3. ì ì¬ ì˜ë¯¸ ë¶„ì„ (Latent Semantic Analysis , LSA)

<img src = "/image/Untitled 19.png" width = "600px">

ìœ„ì˜ DTMì„ numpyë¡œ êµ¬í˜„

```python
import numpy as np
A=np.array([[0,0,0,1,0,1,1,0,0],[0,0,0,1,1,0,1,0,0],[0,1,1,0,2,0,0,0,0],[1,0,0,0,0,0,0,1,1]])
np.shape(A)
(4, 9) # 4 x 9 í¬ê¸°ì˜ DTM
```

**í’€ SVD(full SVD)**

4 Ã— 4ì˜ í¬ê¸°ë¥¼ ê°€ì§€ëŠ” ì§êµ í–‰ë ¬ U ìƒì„±

```python
U, s, VT = np.linalg.svd(A, full_matrices = True)
print(U.round(2))
np.shape(U)

[[-0.24  0.75  0.   -0.62]
 [-0.51  0.44 -0.    0.74]
 [-0.83 -0.49 -0.   -0.27]
 [-0.   -0.    1.    0.  ]]
(4, 4)
```

ëŒ€ê° í–‰ë ¬ S ìƒì„±

Numpyì˜ linalg.svd()ëŠ” íŠ¹ì´ê°’ ë¶„í•´ì˜ ê²°ê³¼ë¡œ ëŒ€ê° í–‰ë ¬ì´ ì•„ë‹ˆë¼ íŠ¹ì´ê°’ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜

```python
print(s.round(2))
np.shape(s)
[2.69 2.05 1.73 0.77]
(4,)
```

ì´ë¥¼ ë‹¤ì‹œ ëŒ€ê° í–‰ë ¬ì˜ í˜•íƒœë¡œ

```python
S = np.zeros((4, 9)) # ëŒ€ê° í–‰ë ¬ì˜ í¬ê¸°ì¸ 4 x 9ì˜ ì„ì˜ì˜ í–‰ë ¬ ìƒì„±
S[:4, :4] = np.diag(s) # íŠ¹ì´ê°’ì„ ëŒ€ê°í–‰ë ¬ì— ì‚½ì…
print(S.round(2))
np.shape(S)
[[2.69 0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   2.05 0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   1.73 0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.77 0.   0.   0.   0.   0.  ]]   # íŠ¹ì´ê°’ë“¤ ë‚´ë¦¼ì°¨ìˆœ í™•ì¸ ê°€ëŠ¥!
(4, 9)
```

9 Ã— 9ì˜ í¬ê¸°ë¥¼ ê°€ì§€ëŠ” ì§êµ í–‰ë ¬ VT(Vì˜ ì „ì¹˜ í–‰ë ¬) ìƒì„±

```python
print(VT.round(2))
np.shape(VT)
[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]
 [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]
 [ 0.58 -0.    0.    0.   -0.    0.   -0.    0.58  0.58]
 [ 0.   -0.35 -0.35  0.16  0.25 -0.8   0.16 -0.   -0.  ]
 [-0.   -0.78 -0.01 -0.2   0.4   0.4  -0.2   0.    0.  ]
 [-0.29  0.31 -0.78 -0.24  0.23  0.23  0.01  0.14  0.14]
 [-0.29 -0.1   0.26 -0.59 -0.08 -0.08  0.66  0.14  0.14]
 [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19  0.75 -0.25]
 [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19 -0.25  0.75]]
(9, 9)
```

U Ã— S Ã— VTë¥¼ í•˜ë©´ ê¸°ì¡´ì˜ í–‰ë ¬ Aì™€ ë™ì¼í•œì§€ í™•ì¸

```python
np.allclose(A, np.dot(np.dot(U,S), VT).round(2))
True
```

ì´ì œ ì ˆë‹¨ëœ SVD(Truncated SVD)ë¥¼ ìˆ˜í–‰

t = 2  ì„¤ì •

ëŒ€ê° í–‰ë ¬ S ë‚´ì˜ íŠ¹ì´ê°’ ì¤‘ì—ì„œ ìƒìœ„ 2ê°œë§Œ ë‚¨ê¸°ê³  ì œê±°

```python
S=S[:2,:2]
print(S.round(2))
[[2.69 0.  ]
 [0.   2.05]]
```

ì§êµ í–‰ë ¬ U ë„ 2ê°œì˜ ì—´ë§Œ ë‚¨ê¸°ê³  ì œê±°

```python
U=U[:,:2]
print(U.round(2))
[[-0.24  0.75]
 [-0.51  0.44]
 [-0.83 -0.49]
 [-0.   -0.  ]]
```

VTì— ëŒ€í•´ì„œ 2ê°œì˜ í–‰ë§Œ ë‚¨ê¸°ê³  ì œê±°

(ì´ëŠ” Vê´€ì ì—ì„œëŠ” 2ê°œì˜ ì—´ë§Œ ë‚¨ê¸°ê³  ì œê±°í•œ ê²ƒ)

```python
VT=VT[:2,:]
print(VT.round(2))
[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]
 [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]]
```

ì¶•ì†Œëœ í–‰ë ¬ U, S, VT ìœ¼ë¡œ ë‹¤ì‹œ U Ã— S Ã— VT  í•˜ë©´ ê¸°ì¡´ì˜ Aì™€ ë‹¤ë¥¸ ê²°ê³¼ê°€ ë‚˜ì˜¤ê²Œ ëœë‹¤!

```python
A_prime=np.dot(np.dot(U,S), VT)
print(A)
print(A_prime.round(2))
[[0 0 0 1 0 1 1 0 0]
 [0 0 0 1 1 0 1 0 0]
 [0 1 1 0 2 0 0 0 0]
 [1 0 0 0 0 0 0 1 1]]
[[ 0.   -0.17 -0.17  1.08  0.12  0.62  1.08 -0.   -0.  ]
 [ 0.    0.2   0.2   0.91  0.86  0.45  0.91  0.    0.  ]
 [ 0.    0.93  0.93  0.03  2.05 -0.17  0.03  0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.    0.    0.    0.  ]]

# ëŒ€ì²´ì ìœ¼ë¡œ ê¸°ì¡´ì— 0ì¸ ê°’ë“¤ì€ 0ì— ê°€ê¹Œìš´ ê°’ì´ ë‚˜ì˜¤ê³ , 
# 1ì¸ ê°’ë“¤ì€ 1ì— ê°€ê¹Œìš´ ê°’ì´ ë‚˜ì˜¤ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤!
# ê°’ì´ ì œëŒ€ë¡œ ë³µêµ¬ë˜ì§€ ì•Šì€ êµ¬ê°„ë„ ì¡´ì¬!
```

ì¶•ì†Œëœ UëŠ” 4 Ã— 2ì˜ í¬ê¸° 
â†’ ë¬¸ì„œì˜ ê°œìˆ˜ Ã— í† í”½ì˜ ìˆ˜ tì˜ í¬ê¸°
â†’ 4ê°œì˜ ë¬¸ì„œ ê°ê°ì„ 2ê°œì˜ ê°’ìœ¼ë¡œ í‘œí˜„
â†’ ì¦‰, Uì˜ ê° í–‰ì€ ì ì¬ ì˜ë¯¸ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•œ ìˆ˜ì¹˜í™” ëœ ê°ê°ì˜ **ë¬¸ì„œ ë²¡í„°**

ì¶•ì†Œëœ VTëŠ” 2 Ã— 9ì˜ í¬ê¸°
â†’ í† í”½ì˜ ìˆ˜ t Ã— ë‹¨ì–´ì˜ ê°œìˆ˜ì˜ í¬ê¸°
â†’ ì¦‰, VTì˜ ê° ì—´ì€ ì ì¬ ì˜ë¯¸ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ ìˆ˜ì¹˜í™” ëœ ê°ê°ì˜ **ë‹¨ì–´ ë²¡í„°

ì´ ë¬¸ì„œ ë²¡í„°ë“¤ê³¼ ë‹¨ì–´ ë²¡í„°ë“¤ì„ í†µí•´ 
ë‹¤ë¥¸ ë¬¸ì„œì˜ ìœ ì‚¬ë„, ë‹¤ë¥¸ ë‹¨ì–´ì˜ ìœ ì‚¬ë„, ë‹¨ì–´(ì¿¼ë¦¬)ë¡œë¶€í„° ë¬¸ì„œì˜ ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ëŠ” ê²ƒë“¤ì´ ê°€ëŠ¥**

---

### 4. ì‹¤ìŠµì„ í†µí•œ ì´í•´

ì‚¬ì´í‚·ëŸ°ì—ì„œ Twenty Newsgroupsì´ë¼ëŠ” 20ê°œì˜ ë‹¤ë¥¸ ì£¼ì œë¥¼ ê°€ì§„ ë‰´ìŠ¤ ê·¸ë£¹ ë°ì´í„°ë¥¼ ì œê³µ

LSAë¥¼ ì‚¬ìš©í•´ ë¬¸ì„œì˜ ìˆ˜ë¥¼ ì›í•˜ëŠ” í† í”½ì˜ ìˆ˜ë¡œ ì••ì¶•í•´,

ê° í† í”½ë‹¹ ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ì–´ 5ê°œë¥¼ ì¶œë ¥í•˜ëŠ” ì‹¤ìŠµìœ¼ë¡œ í† í”½ ëª¨ë¸ë§ì„ ìˆ˜í–‰

**1) ë‰´ìŠ¤ê·¸ë£¹ ë°ì´í„°ì— ëŒ€í•œ ì´í•´**

```python
import pandas as pd
from sklearn.datasets import fetch_20newsgroups
dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))
documents = dataset.data
len(documents)
11314 # í›ˆë ¨ì— ì‚¬ìš©í•  ë‰´ìŠ¤ê·¸ë£¹ ë°ì´í„°ëŠ” ì´ 11,314ê°œ

documents[1]
"\n\n\n\n\n\n\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\nof steam!\n\n\n\n\n\n\n\nJim,\n\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\ndenial about the faith you need to get by.  Oh well, just pretend that it will\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\nalt.atheist.hard, you won't be bummin' so much?\n\n\n\n\n\n\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \n--\nBake Timmons, III"
```

target_nameì—ëŠ” ì´ ë°ì´í„°ê°€ ì–´ë–¤ 20ê°œì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ê°–ê³  ìˆëŠ” ì§€ ì €ì¥ë˜ì–´ ìˆë‹¤.

```python
print(dataset.target_names)
['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']
```

**2) í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬**

ì•„ì´ë””ì–´

- ì•ŒíŒŒë²³ ì œì™¸í•œ êµ¬ë‘ì , ìˆ«ì, íŠ¹ìˆ˜ ë¬¸ì ì œê±°
- ê¸¸ì´ê°€ ì§§ì€ ë‹¨ì–´ ì œê±°
- ëª¨ë“  ì•ŒíŒŒë²³ì„ ì†Œë¬¸ìë¡œ ë°”ê¿” ë‹¨ì–´ì˜ ê°œìˆ˜ ì¤„ì´ê¸°

```python
news_df = pd.DataFrame({'document':documents})
# íŠ¹ìˆ˜ ë¬¸ì ì œê±°
news_df['clean_doc'] = news_df['document'].str.replace("[^a-zA-Z]", " ")
# ê¸¸ì´ê°€ 3ì´í•˜ì¸ ë‹¨ì–´ëŠ” ì œê±° (ê¸¸ì´ê°€ ì§§ì€ ë‹¨ì–´ ì œê±°)
news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))
# ì „ì²´ ë‹¨ì–´ì— ëŒ€í•œ ì†Œë¬¸ì ë³€í™˜
news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())

news_df['clean_doc'][1]
'yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons'
```

í† í°í™”! ê·¸ ë‹¤ìŒ ë¶ˆìš©ì–´ ì œê±°!

```python
from nltk.corpus import stopwords
stop_words = stopwords.words('english') # NLTKë¡œë¶€í„° ë¶ˆìš©ì–´ë¥¼ ë°›ê¸°
tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # í† í°í™”
tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])
# ë¶ˆìš©ì–´ë¥¼ ì œê±°

print(tokenized_doc[1])
['yeah', 'expect', 'people', 'read', 'actually', 'accept', 'hard', 'atheism', 'need', 'little', 'leap', 'faith', 'jimmy', 'logic', 'runs', 'steam', 'sorry', 'pity', 'sorry', 'feelings', 'denial', 'faith', 'need', 'well', 'pretend', 'happily', 'ever', 'anyway', 'maybe', 'start', 'newsgroup', 'atheist', 'hard', 'bummin', 'much', 'forget', 'flintstone', 'chewables', 'bake', 'timmons']
```

**3) TF-IDF í–‰ë ¬ ë§Œë“¤ê¸°**

TfidfVectorizerëŠ” ê¸°ë³¸ì ìœ¼ë¡œ í† í°í™”ê°€ ë˜ì–´ìˆì§€ ì•Šì€ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©

ë‹¤ì‹œ í† í°í™” ì‘ì—…ì„ ì—­ìœ¼ë¡œ ì·¨ì†Œí•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰ 

ì—­í† í°í™” (Detokenization)

```python
# ì—­í† í°í™” (í† í°í™” ì‘ì—…ì„ ì—­ìœ¼ë¡œ ë˜ëŒë¦¼)
detokenized_doc = []
for i in range(len(news_df)):
    t = ' '.join(tokenized_doc[i])
    detokenized_doc.append(t)

news_df['clean_doc'] = detokenized_doc

news_df['clean_doc'][1]
'yeah expect people read actually accept hard atheism need little leap faith jimmy logic runs steam sorry pity sorry feelings denial faith need well pretend happily ever anyway maybe start newsgroup atheist hard bummin much forget flintstone chewables bake timmons'
```

TfidfVectorizerë¥¼ í†µí•´ ë‹¨ì–´ 1,000ê°œì— ëŒ€í•œ TF-IDF í–‰ë ¬ ìƒì„±

```python
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(stop_words='english', 
max_features= 1000, # ìƒìœ„ 1,000ê°œì˜ ë‹¨ì–´ë¥¼ ë³´ì¡´ 
max_df = 0.5, 
smooth_idf=True)

X = vectorizer.fit_transform(news_df['clean_doc'])
X.shape # TF-IDF í–‰ë ¬ì˜ í¬ê¸° í™•ì¸
(11314, 1000)
```

**4) í† í”½ ëª¨ë¸ë§ (Topic Modeling)**

ì‚¬ì´í‚·ëŸ°ì˜ ì ˆë‹¨ëœ SVD(Truncated SVD)ë¥¼ ì‚¬ìš© â†’ ì°¨ì› ì¶•ì†Œ ê°€ëŠ¥!

ê¸°ì¡´ ë°ì´í„°ê°€ 20ê°œì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ê°–ê³  ìˆì—ˆê¸° ë•Œë¬¸ì—, 20ê°œì˜ í† í”½ì„ ê°€ì¡Œë‹¤ê³  ê°€ì •

```python
from sklearn.decomposition import TruncatedSVD
svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)
svd_model.fit(X)
len(svd_model.components_)
20 

np.shape(svd_model.components_) # LSAì—ì„œ VTì— í•´ë‹¹
(20, 1000)
```

ê° 20ê°œì˜ í–‰ì˜ ê° 1,000ê°œì˜ ì—´ ì¤‘ ê°€ì¥ ê°’ì´ í° 5ê°œì˜ ê°’ì„ ì°¾ì•„ì„œ ë‹¨ì–´ë¡œ ì¶œë ¥

```python
terms = vectorizer.get_feature_names() # ë‹¨ì–´ ì§‘í•©. 1,000ê°œì˜ ë‹¨ì–´ê°€ ì €ì¥ë¨.

def get_topics(components, feature_names, n=5):
    for idx, topic in enumerate(components):
        print("Topic %d:" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])
get_topics(svd_model.components_,terms)

Topic 1: [('like', 0.2138), ('know', 0.20031), ('people', 0.19334), ('think', 0.17802), ('good', 0.15105)]
Topic 2: [('thanks', 0.32918), ('windows', 0.29093), ('card', 0.18016), ('drive', 0.1739), ('mail', 0.15131)]
Topic 3: [('game', 0.37159), ('team', 0.32533), ('year', 0.28205), ('games', 0.25416), ('season', 0.18464)]
Topic 4: [('drive', 0.52823), ('scsi', 0.20043), ('disk', 0.15518), ('hard', 0.15511), ('card', 0.14049)]
Topic 5: [('windows', 0.40544), ('file', 0.25619), ('window', 0.1806), ('files', 0.16196), ('program', 0.14009)]
Topic 6: [('government', 0.16085), ('chip', 0.16071), ('mail', 0.15626), ('space', 0.15047), ('information', 0.13582)]
Topic 7: [('like', 0.67121), ('bike', 0.14274), ('know', 0.11189), ('chip', 0.11043), ('sounds', 0.10389)]
Topic 8: [('card', 0.44948), ('sale', 0.21639), ('video', 0.21318), ('offer', 0.14896), ('monitor', 0.1487)]
Topic 9: [('know', 0.44869), ('card', 0.35699), ('chip', 0.17169), ('video', 0.15289), ('government', 0.15069)]
Topic 10: [('good', 0.41575), ('know', 0.23137), ('time', 0.18933), ('bike', 0.11317), ('jesus', 0.09421)]
Topic 11: [('think', 0.7832), ('chip', 0.10776), ('good', 0.10613), ('thanks', 0.08985), ('clipper', 0.07882)]
Topic 12: [('thanks', 0.37279), ('right', 0.21787), ('problem', 0.2172), ('good', 0.21405), ('bike', 0.2116)]
Topic 13: [('good', 0.36691), ('people', 0.33814), ('windows', 0.28286), ('know', 0.25238), ('file', 0.18193)]
Topic 14: [('space', 0.39894), ('think', 0.23279), ('know', 0.17956), ('nasa', 0.15218), ('problem', 0.12924)]
Topic 15: [('space', 0.3092), ('good', 0.30207), ('card', 0.21615), ('people', 0.20208), ('time', 0.15716)]
Topic 16: [('people', 0.46951), ('problem', 0.20879), ('window', 0.16), ('time', 0.13873), ('game', 0.13616)]
Topic 17: [('time', 0.3419), ('bike', 0.26896), ('right', 0.26208), ('windows', 0.19632), ('file', 0.19145)]
Topic 18: [('time', 0.60079), ('problem', 0.15209), ('file', 0.13856), ('think', 0.13025), ('israel', 0.10728)]
Topic 19: [('file', 0.4489), ('need', 0.25951), ('card', 0.1876), ('files', 0.17632), ('problem', 0.1491)]
Topic 20: [('problem', 0.32797), ('file', 0.26268), ('thanks', 0.23414), ('used', 0.19339), ('space', 0.13861)]
```

---

### 5.  LSAì˜ ì¥ë‹¨ì  (Pros and Cons of LSA)

ì‰½ê³  ë¹ ë¥´ê²Œ êµ¬í˜„ì´ ê°€ëŠ¥

ë‹¨ì–´ì˜ ì ì¬ì ì¸ ì˜ë¯¸ë¥¼ ì´ëŒì–´ë‚¼ ìˆ˜ ìˆë‹¤ 
  â†’ ë¬¸ì„œ ìœ ì‚¬ë„ ê³„ì‚° ë“±ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ ë³´ì—¬ì¤€ë‹¤! 

SVDì˜ íŠ¹ì„± ìƒ ì´ë¯¸ ê³„ì‚°ëœ LSAì— ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì¶”ê°€í•´ ê³„ì‚°í•˜ë ¤ë©´ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ê³„ì‚°í•´ì•¼ í•œë‹¤. 
  â†’ì¦‰, ìƒˆë¡œìš´ ì •ë³´ì— ëŒ€í•´ ì—…ë°ì´íŠ¸ê°€ ì–´ë µë‹¤!

**ìµœê·¼ LSA ëŒ€ì‹  Word2Vec ë“± ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë²¡í„°í™” í•  ìˆ˜ ìˆëŠ” ë˜ ë‹¤ë¥¸ ë°©ë²•ë¡ ì¸ ì¸ê³µ ì‹ ê²½ë§ ê¸°ë°˜ì˜ ë°©ë²•ë¡ ì´ ê°ê´‘ë°›ëŠ” ì´ìœ **

---

## ì ì¬ ë””ë¦¬í´ë ˆ í• ë‹¹ (Latent Dirichlet Allocation, LDA)

í† í”½ ëª¨ë¸ë§ì˜ ëŒ€í‘œì ì¸ ì•Œê³ ë¦¬ì¦˜!

ë¬¸ì„œë“¤ì€ í† í”½ë“¤ì˜ í˜¼í•©ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, í† í”½ë“¤ì€ í™•ë¥  ë¶„í¬ì— ê¸°ë°˜í•˜ì—¬ ë‹¨ì–´ë“¤ì„ ìƒì„±í•œë‹¤ê³  ê°€ì •

ë°ì´í„°ê°€ ì£¼ì–´ì§€ë©´, LDAëŠ” ë¬¸ì„œê°€ ìƒì„±ë˜ë˜ ê³¼ì •ì„ ì—­ì¶”ì 

### 1. ì ì¬ ë””ë¦¬í´ë ˆ í• ë‹¹ (Latent Dirichlet Allocation, LDA) ê°œìš”

ex)

3ê°œì˜ ë¬¸ì„œ ì§‘í•©ì„ ì…ë ¥í•˜ë©´ ì–´ë–¤ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ëŠ”ì§€ ê°„ì†Œí™” ëœ ì˜ˆ

ë¬¸ì„œ1 : ì €ëŠ” ì‚¬ê³¼ë‘ ë°”ë‚˜ë‚˜ë¥¼ ë¨¹ì–´ìš”

ë¬¸ì„œ2 : ìš°ë¦¬ëŠ” ê·€ì—¬ìš´ ê°•ì•„ì§€ê°€ ì¢‹ì•„ìš”

ë¬¸ì„œ3 : ì €ì˜ ê¹œì°í•˜ê³  ê·€ì—¬ìš´ ê°•ì•„ì§€ê°€ ë°”ë‚˜ë‚˜ë¥¼ ë¨¹ì–´ìš”

ë¬¸ì„œ ì§‘í•©ì—ì„œ **í† í”½ì´ ëª‡ ê°œê°€ ì¡´ì¬í• ì§€ ê°€ì •**í•˜ëŠ” ê²ƒì€ ì‚¬ìš©ìê°€ í•´ì•¼ í•  ì¼

LDAì— 2ê°œì˜ í† í”½ì„ ì°¾ìœ¼ë¼ê³  ìš”ì²­

ì „ì²˜ë¦¬ ê³¼ì •ì„ ê±°ì¹œ DTMì´ LDAì˜ ì…ë ¥ì´ ë˜ì—ˆë‹¤ê³  ê°€ì •

ì„¸ ë¬¸ì„œë¡œë¶€í„° 2ê°œì˜ í† í”½ì„ ì°¾ì€ ê²°ê³¼

LDAëŠ”Â **ê° ë¬¸ì„œì˜ í† í”½ ë¶„í¬**ì™€Â **ê° í† í”½ ë‚´ì˜ ë‹¨ì–´ ë¶„í¬**ë¥¼ ì¶”ì •

**<ê° ë¬¸ì„œì˜ í† í”½ ë¶„í¬>**

ë¬¸ì„œ1 : í† í”½ A 100%

ë¬¸ì„œ2 : í† í”½ B 100%

ë¬¸ì„œ3 : í† í”½ B 60%, í† í”½ A 40%

**<ê° í† í”½ì˜ ë‹¨ì–´ ë¶„í¬>**

í† í”½A :Â **ì‚¬ê³¼ 20%, ë°”ë‚˜ë‚˜ 40%, ë¨¹ì–´ìš” 40%**, ê·€ì—¬ìš´ 0%, ê°•ì•„ì§€ 0%, ê¹œì°í•˜ê³  0%, ì¢‹ì•„ìš” 0%

í† í”½B : ì‚¬ê³¼ 0%, ë°”ë‚˜ë‚˜ 0%, ë¨¹ì–´ìš” 0%,Â **ê·€ì—¬ìš´ 33%, ê°•ì•„ì§€ 33%, ê¹œì°í•˜ê³  16%, ì¢‹ì•„ìš” 16%**

â†’ ì‚¬ìš©ìëŠ” ìœ„ ê²°ê³¼ë¡œë¶€í„° ë‘ í† í”½ì´ ê°ê° ê³¼ì¼ì— ëŒ€í•œ í† í”½ê³¼ ê°•ì•„ì§€ì— ëŒ€í•œ í† í”½ì´ë¼ê³  íŒë‹¨í•  ìˆ˜ ìˆë‹¤.

---

### 2. LDAì˜ ê°€ì •

DTM ë˜ëŠ” TF-IDF í–‰ë ¬ì„ ì…ë ¥ìœ¼ë¡œ í•œë‹¤! â†’ LDAëŠ” ë‹¨ì–´ì˜ ìˆœì„œëŠ” ì‹ ê²½ ì“°ì§€ ì•ŠëŠ”ë‹¤!

**'ë‚˜ëŠ” ì´ ë¬¸ì„œë¥¼ ì‘ì„±í•˜ê¸° ìœ„í•´ì„œ ì´ëŸ° ì£¼ì œë“¤ì„ ë„£ì„ê±°ê³ , ì´ëŸ° ì£¼ì œë“¤ì„ ìœ„í•´ì„œëŠ” ì´ëŸ° ë‹¨ì–´ë“¤ì„ ë„£ì„ ê±°ì•¼.'**

**1) ë¬¸ì„œì— ì‚¬ìš©í•  ë‹¨ì–´ì˜ ê°œìˆ˜ Nì„ ì •í•œë‹¤**

**2) ë¬¸ì„œì— ì‚¬ìš©í•  í† í”½ì˜ í˜¼í•©ì„ í™•ë¥  ë¶„í¬ì— ê¸°ë°˜í•˜ì—¬ ê²°ì •í•œë‹¤

    -** Ex) ìœ„ ì˜ˆì œì™€ ê°™ì´ í† í”½ì´ 2ê°œë¼ê³  í–ˆì„ ë•Œ ê°•ì•„ì§€ í† í”½ì„ 60%, ê³¼ì¼ í† í”½ì„ 40%ì™€ ê°™ì´ ì„ íƒí•  ìˆ˜ ìˆë‹¤

**3) ë¬¸ì„œì— ì‚¬ìš©í•  ê° ë‹¨ì–´ë¥¼ (ì•„ë˜ì™€ ê°™ì´) ì •í•œë‹¤

3-1) í† í”½ ë¶„í¬ì—ì„œ í† í”½ Të¥¼ í™•ë¥ ì ìœ¼ë¡œ ê³ ë¥¸ë‹¤**

    - Ex) 60% í™•ë¥ ë¡œ ê°•ì•„ì§€ í† í”½ì„ ì„ íƒí•˜ê³ , 40% í™•ë¥ ë¡œ ê³¼ì¼ í† í”½ì„ ì„ íƒí•  ìˆ˜ ìˆë‹¤.

**3-2) ì„ íƒí•œ í† í”½ Tì—ì„œ ë‹¨ì–´ì˜ ì¶œí˜„ í™•ë¥  ë¶„í¬ì— ê¸°ë°˜í•´ ë¬¸ì„œì— ì‚¬ìš©í•  ë‹¨ì–´ë¥¼ ê³ ë¦…ë‹ˆë‹¤.**

    - Ex) ê°•ì•„ì§€ í† í”½ì„ ì„ íƒí–ˆë‹¤ë©´, 33% í™•ë¥ ë¡œ ê°•ì•„ì§€ë€ ë‹¨ì–´ë¥¼ ì„ íƒí•  ìˆ˜ ìˆë‹¤. 3)ì„ ë°˜ë³µí•˜ë©´ì„œ ë¬¸ì„œë¥¼ ì™„ì„±í•œë‹¤.

ì´ëŸ¬í•œ ê³¼ì •ì„ í†µí•´ ë¬¸ì„œê°€ ì‘ì„±ë˜ì—ˆë‹¤ëŠ” ê°€ì • í•˜ì— 

LDAëŠ” í† í”½ì„ ë½‘ì•„ë‚´ê¸° ìœ„í•´ ìœ„ ê³¼ì •ì„ ì—­ìœ¼ë¡œ ì¶”ì í•˜ëŠ” **ì—­ê³µí•™(reverse engineering)** ìˆ˜í–‰

---

### 3. LDA ìˆ˜í–‰í•˜ê¸°

**1) ì‚¬ìš©ìëŠ” í† í”½ì˜ ê°œìˆ˜ kë¥¼ ì„¤ì •**

kë¥¼ ì…ë ¥ë°›ìœ¼ë©´, kê°œì˜ í† í”½ì´ Mê°œì˜ ì „ì²´ ë¬¸ì„œì— ê±¸ì³ ë¶„í¬ë˜ì–´ ìˆë‹¤ê³  ê°€ì •

**2) ëª¨ë“  ë‹¨ì–´ë¥¼ kê°œ ì¤‘ í•˜ë‚˜ì˜ í† í”½ì— í• ë‹¹**

ì´ ì‘ì—…ì´ ëë‚˜ë©´ ê° ë¬¸ì„œëŠ” í† í”½ì„ ê°€ì§€ë©°, í† í”½ì€ ë‹¨ì–´ ë¶„í¬ë¥¼ ê°€ì§€ëŠ” ìƒíƒœ

ë¬¼ë¡  ëœë¤ìœ¼ë¡œ í• ë‹¹ â†’ ê²°ê³¼ëŠ” ì „ë¶€ í‹€ë¦° ìƒíƒœ

**3) ì´ì œ ëª¨ë“  ë¬¸ì„œì˜ ëª¨ë“  ë‹¨ì–´ì— ëŒ€í•´ì„œ ì•„ë˜ì˜ ì‚¬í•­ì„ ë°˜ë³µ ì§„í–‰ (iterative)**

**3-1) ì–´ë–¤ ë¬¸ì„œì˜ ê° ë‹¨ì–´ wëŠ” ìì‹ ì€ ì˜ëª»ëœ í† í”½ì— í• ë‹¹ë˜ì–´ ìˆì§€ë§Œ, ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì€ ì „ë¶€ ì˜¬ë°”ë¥¸ í† í”½ì— í• ë‹¹ë˜ì–´ ìˆëŠ” ìƒíƒœë¼ê³  ê°€ì •. ì´ì— ë”°ë¼ ë‹¨ì–´ wëŠ” ì•„ë˜ì˜ ë‘ ê°€ì§€ ê¸°ì¤€ì— ë”°ë¼ì„œ í† í”½ì´ ì¬í• ë‹¹ëœë‹¤**

- p(topic t | document d) : ë¬¸ì„œ dì˜ ë‹¨ì–´ë“¤ ì¤‘ í† í”½ tì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ë“¤ì˜ ë¹„ìœ¨
- p(word w | topic t) : ê° í† í”½ë“¤ tì—ì„œ í•´ë‹¹ ë‹¨ì–´ wì˜ ë¶„í¬

ex)

doc1ì˜ ì„¸ë²ˆì§¸ ë‹¨ì–´ appleì˜ í† í”½ì„ ê²°ì •í•˜ê³ ì í•œë‹¤.

![https://wikidocs.net/images/page/30708/lda1.PNG](https://wikidocs.net/images/page/30708/lda1.PNG)

**ì²« ë²ˆì§¸ ê¸°ì¤€ì€ ë¬¸ì„œ doc1ì˜ ë‹¨ì–´ë“¤ì´ ì–´ë–¤ í† í”½ì— í•´ë‹¹í•˜ëŠ”ì§€**

![https://wikidocs.net/images/page/30708/lda3.PNG](https://wikidocs.net/images/page/30708/lda3.PNG)

í† í”½ Aì™€ í† í”½ Bì— 50 ëŒ€ 50ì˜ ë¹„ìœ¨ë¡œ í• ë‹¹ â†’ ì–´ëŠ í† í”½ì—ë„ ì†í•  ê°€ëŠ¥ì„±ì´ ìˆë‹¤!

**ë‘ë²ˆì§¸ ê¸°ì¤€ì€ ë‹¨ì–´ appleì´ ì „ì²´ ë¬¸ì„œì—ì„œ ì–´ë–¤ í† í”½ì— í• ë‹¹ë˜ì–´ ìˆëŠ”ì§€**

![https://wikidocs.net/images/page/30708/lda2.PNG](https://wikidocs.net/images/page/30708/lda2.PNG)

ë‹¨ì–´ appleì€ í† í”½ Bì— í• ë‹¹ë  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤! 

---

### 4. ì ì¬ ë””ë¦¬í´ë ˆ í• ë‹¹ê³¼ ì ì¬ ì˜ë¯¸ ë¶„ì„ì˜ ì°¨ì´

**LSA : DTM ë˜ëŠ” TF-IDFë¥¼ ì°¨ì› ì¶•ì†Œ í•´ ì¶•ì†Œ ì°¨ì›ì—ì„œ ê·¼ì ‘ ë‹¨ì–´ë“¤ì„ í† í”½ìœ¼ë¡œ ë¬¶ëŠ”ë‹¤.**

**LDA : ë‹¨ì–´ê°€ íŠ¹ì • í† í”½ì— ì¡´ì¬í•  í™•ë¥ ê³¼ ë¬¸ì„œì— íŠ¹ì • í† í”½ì´ ì¡´ì¬í•  í™•ë¥ ì„ ê²°í•©í™•ë¥ ë¡œ ì¶”ì •í•˜ì—¬ í† í”½ì„ ì¶”ì¶œí•œë‹¤.**